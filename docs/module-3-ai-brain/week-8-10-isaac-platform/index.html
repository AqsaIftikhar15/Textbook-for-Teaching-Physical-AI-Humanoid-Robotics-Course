<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-module-3-ai-brain/week-8-10-isaac-platform" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">NVIDIA Isaac Platform | Physical AI &amp; Humanoid Robotics Book</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://aqsaiftikhar15.github.io/Textbook-for-Teaching-Physical-AI-Humanoid-Robotics-Course/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://aqsaiftikhar15.github.io/Textbook-for-Teaching-Physical-AI-Humanoid-Robotics-Course/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://aqsaiftikhar15.github.io/Textbook-for-Teaching-Physical-AI-Humanoid-Robotics-Course/docs/module-3-ai-brain/week-8-10-isaac-platform/"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="NVIDIA Isaac Platform | Physical AI &amp; Humanoid Robotics Book"><meta data-rh="true" name="description" content="AI-powered perception and manipulation with Isaac SDK"><meta data-rh="true" property="og:description" content="AI-powered perception and manipulation with Isaac SDK"><link data-rh="true" rel="icon" href="/Textbook-for-Teaching-Physical-AI-Humanoid-Robotics-Course/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://aqsaiftikhar15.github.io/Textbook-for-Teaching-Physical-AI-Humanoid-Robotics-Course/docs/module-3-ai-brain/week-8-10-isaac-platform/"><link data-rh="true" rel="alternate" href="https://aqsaiftikhar15.github.io/Textbook-for-Teaching-Physical-AI-Humanoid-Robotics-Course/docs/module-3-ai-brain/week-8-10-isaac-platform/" hreflang="en"><link data-rh="true" rel="alternate" href="https://aqsaiftikhar15.github.io/Textbook-for-Teaching-Physical-AI-Humanoid-Robotics-Course/docs/module-3-ai-brain/week-8-10-isaac-platform/" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"NVIDIA Isaac Platform","item":"https://aqsaiftikhar15.github.io/Textbook-for-Teaching-Physical-AI-Humanoid-Robotics-Course/docs/module-3-ai-brain/week-8-10-isaac-platform"}]}</script><link rel="stylesheet" href="/Textbook-for-Teaching-Physical-AI-Humanoid-Robotics-Course/assets/css/styles.b1ae60b0.css">
<script src="/Textbook-for-Teaching-Physical-AI-Humanoid-Robotics-Course/assets/js/runtime~main.b6c8dbdc.js" defer="defer"></script>
<script src="/Textbook-for-Teaching-Physical-AI-Humanoid-Robotics-Course/assets/js/main.9e5f1205.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/Textbook-for-Teaching-Physical-AI-Humanoid-Robotics-Course/"><div class="navbar__logo"><img src="/Textbook-for-Teaching-Physical-AI-Humanoid-Robotics-Course/img/logo.svg" alt="Robotics Book Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/Textbook-for-Teaching-Physical-AI-Humanoid-Robotics-Course/img/logo.svg" alt="Robotics Book Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Physical AI &amp; Humanoid Robotics</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/Textbook-for-Teaching-Physical-AI-Humanoid-Robotics-Course/docs/">Modules</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://aqsaiftikhar15.github.io/Textbook-for-Teaching-Physical-AI-Humanoid-Robotics-Course/" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/Textbook-for-Teaching-Physical-AI-Humanoid-Robotics-Course/docs/"><span title="Introduction" class="categoryLinkLabel_W154">Introduction</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/Textbook-for-Teaching-Physical-AI-Humanoid-Robotics-Course/docs/module-1-ros2/week-1-2-intro-physical-ai/"><span title="Module 1: The Robotic Nervous System (ROS 2)" class="categoryLinkLabel_W154">Module 1: The Robotic Nervous System (ROS 2)</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/Textbook-for-Teaching-Physical-AI-Humanoid-Robotics-Course/docs/module-2-digital-twin/week-6-7-gazebo-unity/"><span title="Module 2: The Digital Twin (Gazebo &amp; Unity)" class="categoryLinkLabel_W154">Module 2: The Digital Twin (Gazebo &amp; Unity)</span></a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" href="/Textbook-for-Teaching-Physical-AI-Humanoid-Robotics-Course/docs/module-3-ai-brain/week-8-10-isaac-platform/"><span title="Module 3: The AI-Robot Brain (NVIDIA Isaac)" class="categoryLinkLabel_W154">Module 3: The AI-Robot Brain (NVIDIA Isaac)</span></a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/Textbook-for-Teaching-Physical-AI-Humanoid-Robotics-Course/docs/module-3-ai-brain/week-8-10-isaac-platform/"><span title="NVIDIA Isaac Platform" class="linkLabel_WmDU">NVIDIA Isaac Platform</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/Textbook-for-Teaching-Physical-AI-Humanoid-Robotics-Course/docs/module-3-ai-brain/week-11-12-humanoid-dev/"><span title="Humanoid Robot Development" class="linkLabel_WmDU">Humanoid Robot Development</span></a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/Textbook-for-Teaching-Physical-AI-Humanoid-Robotics-Course/docs/module-3-ai-brain/week-13-conversational-robotics/"><span title="Conversational Robotics Overview" class="linkLabel_WmDU">Conversational Robotics Overview</span></a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="categoryLink_byQd menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/Textbook-for-Teaching-Physical-AI-Humanoid-Robotics-Course/docs/module-4-vla/week-13-vla-concepts/"><span title="Module 4: Vision-Language-Action (VLA)" class="categoryLinkLabel_W154">Module 4: Vision-Language-Action (VLA)</span></a></div></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/Textbook-for-Teaching-Physical-AI-Humanoid-Robotics-Course/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Module 3: The AI-Robot Brain (NVIDIA Isaac)</span></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">NVIDIA Isaac Platform</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Week 8: NVIDIA Isaac Platform Introduction</h1></header>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="introduction">Introduction<a href="#introduction" class="hash-link" aria-label="Direct link to Introduction" title="Direct link to Introduction" translate="no">​</a></h2>
<p>The NVIDIA Isaac platform represents a comprehensive ecosystem for developing AI-powered robotic applications, specifically designed to leverage GPU acceleration for perception, navigation, and manipulation tasks. This week introduces the core components of the Isaac platform, including Isaac ROS, Isaac Sim, and Isaac Lab, and explores how these tools enable the development of sophisticated humanoid robots capable of intelligent behavior in complex environments.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="nvidia-isaac-platform-architecture">NVIDIA Isaac Platform Architecture<a href="#nvidia-isaac-platform-architecture" class="hash-link" aria-label="Direct link to NVIDIA Isaac Platform Architecture" title="Direct link to NVIDIA Isaac Platform Architecture" translate="no">​</a></h2>
<p>The Isaac platform provides a unified framework that bridges simulation, perception, and real-world deployment through a suite of interconnected tools and libraries.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="isaac-platform-components">Isaac Platform Components<a href="#isaac-platform-components" class="hash-link" aria-label="Direct link to Isaac Platform Components" title="Direct link to Isaac Platform Components" translate="no">​</a></h3>
<p>The Isaac ecosystem consists of several key components:</p>
<ul>
<li class=""><strong>Isaac ROS</strong>: GPU-accelerated ROS 2 packages for perception and navigation</li>
<li class=""><strong>Isaac Sim</strong>: High-fidelity physics simulation and synthetic data generation</li>
<li class=""><strong>Isaac Lab</strong>: Reinforcement learning and imitation learning framework</li>
<li class=""><strong>Isaac Apps</strong>: Reference applications and demonstrations</li>
<li class=""><strong>Isaac Core</strong>: Foundational libraries and tools</li>
</ul>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">Pseudocode: Isaac Platform Architecture</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">class IsaacPlatform:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def __init__(self):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.isaac_ros = IsaacROS()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.isaac_sim = IsaacSim()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.isaac_lab = IsaacLab()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.isaac_apps = IsaacApps()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.isaac_core = IsaacCore()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.hardware_abstraction = HardwareAbstractionLayer()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.gpu_manager = GPUResourceManager()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.data_pipeline = DataPipeline()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def initialize_platform(self):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Initialize core components</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.isaac_core.initialize()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Configure GPU resources</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.gpu_manager.initialize_gpus()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.gpu_manager.allocate_memory_pools()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Set up data pipeline</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.data_pipeline.setup_message_passing()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.data_pipeline.configure_compression()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Initialize simulation environment</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.isaac_sim.initialize()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Initialize ROS integration</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.isaac_ros.initialize()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def create_robot_application(self, robot_description):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Create robot-specific application</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        robot_app = RobotApplication(robot_description)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Configure perception pipeline</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        perception_pipeline = self.isaac_ros.create_perception_pipeline()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        perception_pipeline.add_gpu_nodes([</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            &#x27;isaac_ros_detectnet&#x27;,      # Object detection</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            &#x27;isaac_ros_pointcloud&#x27;,     # 3D point cloud processing</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            &#x27;isaac_ros_image_proc&#x27;      # Image processing</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        ])</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Configure navigation pipeline</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        navigation_pipeline = self.isaac_ros.create_navigation_pipeline()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        navigation_pipeline.add_gpu_nodes([</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            &#x27;isaac_ros_costmap_2d&#x27;,     # 2D costmap generation</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            &#x27;isaac_ros_planner&#x27;,        # Path planning</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            &#x27;isaac_ros_controller&#x27;      # Motion control</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        ])</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        robot_app.set_perception_pipeline(perception_pipeline)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        robot_app.set_navigation_pipeline(navigation_pipeline)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        return robot_app</span><br></span></code></pre></div></div>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="gpu-acceleration-framework">GPU Acceleration Framework<a href="#gpu-acceleration-framework" class="hash-link" aria-label="Direct link to GPU Acceleration Framework" title="Direct link to GPU Acceleration Framework" translate="no">​</a></h3>
<p>Isaac leverages NVIDIA&#x27;s GPU ecosystem for accelerated computation:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">Pseudocode: GPU Acceleration System</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">class GPUAcceleration:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def __init__(self):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.gpu_devices = self.detect_gpu_devices()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.memory_manager = GPUMemoryManager()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.compute_contexts = {}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.tensor_cores = TensorCoreManager()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def setup_compute_context(self, device_id):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Create CUDA context for specific GPU</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        context = cuda.Context(device_id)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.compute_contexts[device_id] = context</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Allocate memory pools</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.memory_manager.allocate_pinned_memory(device_id, size_gb=4)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.memory_manager.allocate_unified_memory(device_id, size_gb=8)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Initialize Tensor Core operations for mixed precision</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.tensor_cores.enable_mixed_precision(device_id)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        return context</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def accelerate_perception_task(self, task_type, input_data):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Select appropriate GPU based on task requirements</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        gpu_id = self.select_optimal_gpu(task_type)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        context = self.compute_contexts[gpu_id]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        with context:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            # Transfer data to GPU memory</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            gpu_input = self.memory_manager.copy_to_gpu(input_data, gpu_id)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            if task_type == &#x27;object_detection&#x27;:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                # Use TensorRT for optimized inference</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                result = self.tensorrt_inference(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                    model=&#x27;detectnet_model&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                    input=gpu_input,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                    precision=&#x27;fp16&#x27;  # Mixed precision</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            elif task_type == &#x27;pointcloud_processing&#x27;:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                # Use CUDA kernels for point cloud operations</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                result = self.cuda_pointcloud_process(gpu_input)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            elif task_type == &#x27;image_processing&#x27;:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                # Use GPU-accelerated image processing</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                result = self.gpu_image_process(gpu_input)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            # Transfer result back to CPU memory</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            cpu_result = self.memory_manager.copy_to_cpu(result, gpu_id)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        return cpu_result</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def optimize_memory_usage(self):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Implement memory optimization strategies</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.memory_manager.enable_memory_pooling()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.memory_manager.setup_memory_caching()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.memory_manager.configure_streaming()</span><br></span></code></pre></div></div>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="isaac-ros-gpu-accelerated-ros-2-packages">Isaac ROS: GPU-Accelerated ROS 2 Packages<a href="#isaac-ros-gpu-accelerated-ros-2-packages" class="hash-link" aria-label="Direct link to Isaac ROS: GPU-Accelerated ROS 2 Packages" title="Direct link to Isaac ROS: GPU-Accelerated ROS 2 Packages" translate="no">​</a></h2>
<p>Isaac ROS provides GPU-accelerated implementations of common ROS 2 packages, significantly improving performance for perception and navigation tasks.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="isaac-ros-package-ecosystem">Isaac ROS Package Ecosystem<a href="#isaac-ros-package-ecosystem" class="hash-link" aria-label="Direct link to Isaac ROS Package Ecosystem" title="Direct link to Isaac ROS Package Ecosystem" translate="no">​</a></h3>
<p>Key Isaac ROS packages include:</p>
<ul>
<li class=""><strong>isaac_ros_detectnet</strong>: Object detection using NVIDIA&#x27;s DetectNet</li>
<li class=""><strong>isaac_ros_pointcloud</strong>: GPU-accelerated point cloud processing</li>
<li class=""><strong>isaac_ros_image_proc</strong>: High-performance image processing</li>
<li class=""><strong>isaac_ros_visual_slam</strong>: Visual SLAM with GPU acceleration</li>
<li class=""><strong>isaac_ros_pose_estimation</strong>: Real-time pose estimation</li>
</ul>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">Pseudocode: Isaac ROS Node Implementation</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">class IsaacROSNode:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def __init__(self, node_name):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.node = rclpy.create_node(node_name)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.gpu_pipeline = GPUPipeline()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.tensorrt_engine = None</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.input_queue = queue.Queue(maxsize=10)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.output_queue = queue.Queue(maxsize=10)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def initialize_gpu_pipeline(self, model_path, input_shape):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Load TensorRT engine for optimized inference</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.tensorrt_engine = self.load_tensorrt_engine(model_path)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Configure GPU memory allocation</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.gpu_pipeline.allocate_tensors(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            input_shape=input_shape,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            output_shape=self.get_output_shape(model_path)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Set up CUDA streams for asynchronous processing</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.gpu_pipeline.setup_cuda_streams(num_streams=2)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def process_gpu_inference(self, input_data):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Asynchronous GPU inference</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        with self.gpu_pipeline.get_stream() as stream:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            # Copy input to GPU memory</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            gpu_input = self.gpu_pipeline.copy_input(input_data, stream)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            # Execute TensorRT inference</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            gpu_output = self.tensorrt_engine.execute_async(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                bindings=[gpu_input, self.gpu_pipeline.get_output_buffer()],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                stream_handle=stream.cuda_stream</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            # Copy result back to CPU</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            output_result = self.gpu_pipeline.copy_output(gpu_output, stream)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        return output_result</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def create_isaac_ros_publisher(self, msg_type, topic_name):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Create publisher with GPU-optimized message handling</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        publisher = self.node.create_publisher(msg_type, topic_name, 10)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Optimize for GPU-accelerated data</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        publisher.set_qos_profile(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            rclpy.qos.QoSProfile(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                reliability=rclpy.qos.ReliabilityPolicy.RELIABLE,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                durability=rclpy.qos.DurabilityPolicy.VOLATILE,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                history=rclpy.qos.HistoryPolicy.KEEP_LAST,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                depth=1  # Minimize memory usage for high-frequency data</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        return publisher</span><br></span></code></pre></div></div>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="gpu-accelerated-perception-pipeline">GPU-Accelerated Perception Pipeline<a href="#gpu-accelerated-perception-pipeline" class="hash-link" aria-label="Direct link to GPU-Accelerated Perception Pipeline" title="Direct link to GPU-Accelerated Perception Pipeline" translate="no">​</a></h3>
<p>Building an optimized perception pipeline using Isaac ROS:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">Pseudocode: Perception Pipeline</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">class PerceptionPipeline:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def __init__(self):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.image_subscriber = None</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.detection_publisher = None</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.pointcloud_subscriber = None</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.gpu_pipeline = GPUPipeline()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.synchronization = MessageSynchronizer()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def setup_gpu_perception_nodes(self):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Initialize Isaac ROS nodes with GPU acceleration</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.image_subscriber = self.node.create_subscription(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            sensor_msgs.msg.Image,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            &#x27;/camera/image_raw&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            self.image_callback,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            10</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Object detection node</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.detection_node = self.create_gpu_detection_node()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Depth processing node</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.depth_node = self.create_gpu_depth_node()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Point cloud fusion node</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.fusion_node = self.create_gpu_fusion_node()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def image_callback(self, image_msg):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Process image using GPU acceleration</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        image_tensor = self.convert_image_to_tensor(image_msg)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Run object detection on GPU</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        detections = self.detection_node.run_inference(image_tensor)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Process depth information</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        depth_tensor = self.get_depth_tensor(image_msg.header.stamp)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        depth_processed = self.depth_node.process(depth_tensor)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Fuse detection and depth data</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        fused_result = self.fusion_node.fuse_data(detections, depth_processed)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Publish results</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.publish_detection_results(fused_result, image_msg.header)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def create_gpu_detection_node(self):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Create GPU-optimized detection pipeline</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        detection_node = IsaacDetectionNode()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Load optimized model</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        detection_node.load_model(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            model_path=&#x27;detectnet_model.plan&#x27;,  # TensorRT optimized</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            input_shape=(3, 512, 512),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            output_shape=(1, 100, 7)  # batch, detections, (class, conf, x, y, w, h, angle)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Configure GPU memory</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        detection_node.configure_gpu_memory(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            input_buffer_size=512*512*3*4,  # 4 bytes per float</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            output_buffer_size=100*7*4,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            max_batch_size=8</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        return detection_node</span><br></span></code></pre></div></div>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="isaac-sim-physics-simulation-and-synthetic-data-generation">Isaac Sim: Physics Simulation and Synthetic Data Generation<a href="#isaac-sim-physics-simulation-and-synthetic-data-generation" class="hash-link" aria-label="Direct link to Isaac Sim: Physics Simulation and Synthetic Data Generation" title="Direct link to Isaac Sim: Physics Simulation and Synthetic Data Generation" translate="no">​</a></h2>
<p>Isaac Sim provides high-fidelity physics simulation and synthetic data generation capabilities essential for training AI models without real-world data collection.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="simulation-environment-architecture">Simulation Environment Architecture<a href="#simulation-environment-architecture" class="hash-link" aria-label="Direct link to Simulation Environment Architecture" title="Direct link to Simulation Environment Architecture" translate="no">​</a></h3>
<p>Isaac Sim builds upon Omniverse technology to provide realistic simulation:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">Pseudocode: Isaac Sim Environment</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">class IsaacSimEnvironment:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def __init__(self):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.sim_app = omni.kit.app.get_app()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.physics_context = PhysicsContext()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.renderer = KitRenderer()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.synthetic_data_generator = SyntheticDataGenerator()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.domain_randomization = DomainRandomization()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def create_simulation_world(self, world_config):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Create physics scene</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.physics_context.create_simulation()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Set up physics properties</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.physics_context.set_gravity(world_config.gravity)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.physics_context.set_timestep(world_config.timestep)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.physics_context.set_solver_iterations(world_config.solver_iterations)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Create ground plane</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        ground_plane = self.create_ground_plane(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            size=world_config.ground_size,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            friction=world_config.ground_friction,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            restitution=world_config.ground_restitution</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Create lighting environment</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.setup_environment_lighting(world_config.lighting_config)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Create dynamic objects</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        for obj_config in world_config.objects:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            self.create_dynamic_object(obj_config)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def setup_synthetic_data_generation(self):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Configure synthetic data generation pipeline</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.synthetic_data_generator.set_camera_config(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            resolution=(1920, 1080),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            fov=60.0,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            sensor_noise={&#x27;mean&#x27;: 0.0, &#x27;std&#x27;: 0.001}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Set up annotation generation</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.synthetic_data_generator.enable_annotations([</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            &#x27;bounding_box_2d_tight&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            &#x27;instance_segmentation&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            &#x27;depth&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            &#x27;normals&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            &#x27;motion_vectors&#x27;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        ])</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Configure domain randomization</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.domain_randomization.set_randomization_ranges({</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            &#x27;lighting&#x27;: {&#x27;intensity_range&#x27;: (0.5, 2.0)},</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            &#x27;textures&#x27;: {&#x27;roughness_range&#x27;: (0.1, 0.9)},</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            &#x27;materials&#x27;: {&#x27;color_range&#x27;: (&#x27;red&#x27;, &#x27;blue&#x27;, &#x27;green&#x27;)}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        })</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def generate_synthetic_dataset(self, num_samples, scenario_configs):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        dataset = []</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        for i in range(num_samples):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            # Apply domain randomization</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            self.domain_randomization.randomize_scene()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            # Execute scenario</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            scenario_config = random.choice(scenario_configs)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            self.execute_scenario(scenario_config)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            # Capture synthetic data</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            frame_data = self.synthetic_data_generator.capture_frame()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            annotations = self.synthetic_data_generator.generate_annotations()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            sample = {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                &#x27;image&#x27;: frame_data[&#x27;rgb&#x27;],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                &#x27;depth&#x27;: frame_data[&#x27;depth&#x27;],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                &#x27;annotations&#x27;: annotations,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                &#x27;scenario&#x27;: scenario_config,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                &#x27;randomization_params&#x27;: self.domain_randomization.get_current_params()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            dataset.append(sample)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        return dataset</span><br></span></code></pre></div></div>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="physics-simulation-capabilities">Physics Simulation Capabilities<a href="#physics-simulation-capabilities" class="hash-link" aria-label="Direct link to Physics Simulation Capabilities" title="Direct link to Physics Simulation Capabilities" translate="no">​</a></h3>
<p>Advanced physics simulation for humanoid robot development:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">Pseudocode: Advanced Physics Simulation</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">class AdvancedPhysicsSimulation:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def __init__(self):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.material_properties = MaterialProperties()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.contact_properties = ContactProperties()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.deformable_bodies = DeformableBodySystem()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.fluid_simulation = FluidSimulation()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def setup_material_properties(self):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Configure realistic material properties</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.material_properties.set_defaults({</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            &#x27;robot_metal&#x27;: {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                &#x27;density&#x27;: 7800,  # kg/m^3</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                &#x27;youngs_modulus&#x27;: 200e9,  # Pa</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                &#x27;poissons_ratio&#x27;: 0.3,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                &#x27;static_friction&#x27;: 0.5,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                &#x27;dynamic_friction&#x27;: 0.4,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                &#x27;restitution&#x27;: 0.2</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            },</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            &#x27;rubber_foot&#x27;: {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                &#x27;density&#x27;: 1200,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                &#x27;youngs_modulus&#x27;: 1e6,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                &#x27;poissons_ratio&#x27;: 0.49,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                &#x27;static_friction&#x27;: 0.8,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                &#x27;dynamic_friction&#x27;: 0.7,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                &#x27;restitution&#x27;: 0.1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        })</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def simulate_robot_environment_interaction(self, robot, environment):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Simulate complex interactions between robot and environment</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        for link in robot.links:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            for contact_point in self.detect_contacts(link, environment):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                # Calculate contact forces</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                contact_force = self.calculate_contact_force(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                    contact_point,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                    link.material,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                    environment.material</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                # Apply forces to robot dynamics</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                link.apply_force(contact_force, contact_point.position)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                # Handle friction and sliding</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                if self.is_sliding(contact_point):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                    friction_force = self.calculate_friction_force(contact_point)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                    link.apply_force(friction_force, contact_point.position)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def simulate_deformable_contacts(self, robot, soft_objects):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Handle interactions with deformable objects</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        for soft_obj in soft_objects:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            deformation = self.calculate_deformation(robot, soft_obj)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            soft_obj.apply_deformation(deformation)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            # Update collision geometry based on deformation</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            soft_obj.update_collision_mesh(deformation)</span><br></span></code></pre></div></div>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="isaac-lab-reinforcement-learning-framework">Isaac Lab: Reinforcement Learning Framework<a href="#isaac-lab-reinforcement-learning-framework" class="hash-link" aria-label="Direct link to Isaac Lab: Reinforcement Learning Framework" title="Direct link to Isaac Lab: Reinforcement Learning Framework" translate="no">​</a></h2>
<p>Isaac Lab provides a comprehensive framework for training robotic policies using reinforcement learning and imitation learning.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="reinforcement-learning-environment">Reinforcement Learning Environment<a href="#reinforcement-learning-environment" class="hash-link" aria-label="Direct link to Reinforcement Learning Environment" title="Direct link to Reinforcement Learning Environment" translate="no">​</a></h3>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">Pseudocode: Isaac Lab RL Environment</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">class IsaacLabEnvironment:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def __init__(self, task_config):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.task_config = task_config</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.simulation = IsaacSimEnvironment()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.observation_space = self.define_observation_space()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.action_space = self.define_action_space()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.reward_function = self.define_reward_function()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.termination_conditions = self.define_termination_conditions()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def define_observation_space(self):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Define observation space for RL agent</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        observation_spec = {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            &#x27;joint_positions&#x27;: {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                &#x27;shape&#x27;: (self.robot.num_joints,),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                &#x27;dtype&#x27;: np.float32,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                &#x27;low&#x27;: -np.pi,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                &#x27;high&#x27;: np.pi</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            },</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            &#x27;joint_velocities&#x27;: {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                &#x27;shape&#x27;: (self.robot.num_joints,),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                &#x27;dtype&#x27;: np.float32,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                &#x27;low&#x27;: -10.0,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                &#x27;high&#x27;: 10.0</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            },</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            &#x27;base_pose&#x27;: {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                &#x27;shape&#x27;: (7,),  # [x, y, z, qw, qx, qy, qz]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                &#x27;dtype&#x27;: np.float32,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                &#x27;low&#x27;: [-np.inf, -np.inf, 0, -1, -1, -1, -1],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                &#x27;high&#x27;: [np.inf, np.inf, np.inf, 1, 1, 1, 1]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            },</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            &#x27;sensor_data&#x27;: {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                &#x27;shape&#x27;: (self.sensor_size,),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                &#x27;dtype&#x27;: np.float32,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                &#x27;low&#x27;: -np.inf,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                &#x27;high&#x27;: np.inf</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        return observation_spec</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def define_action_space(self):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Define action space for humanoid robot</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        action_spec = {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            &#x27;joint_commands&#x27;: {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                &#x27;shape&#x27;: (self.robot.num_joints,),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                &#x27;dtype&#x27;: np.float32,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                &#x27;low&#x27;: -np.pi,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                &#x27;high&#x27;: np.pi</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            },</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            &#x27;gains&#x27;: {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                &#x27;shape&#x27;: (self.robot.num_joints * 2,),  # [position_gains, velocity_gains]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                &#x27;dtype&#x27;: np.float32,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                &#x27;low&#x27;: 0.0,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                &#x27;high&#x27;: 1000.0</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        return action_spec</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def define_reward_function(self):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Define reward function for humanoid locomotion</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        def reward_function(state, action, next_state, info):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            reward = 0.0</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            # Progress reward - move forward</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            progress = next_state[&#x27;base_pose&#x27;][0] - state[&#x27;base_pose&#x27;][0]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            reward += progress * 10.0</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            # Velocity reward - maintain target speed</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            current_velocity = self.calculate_base_velocity(next_state)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            target_velocity = self.task_config.target_velocity</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            velocity_reward = -abs(current_velocity - target_velocity) * 5.0</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            reward += velocity_reward</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            # Balance reward - maintain upright posture</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            base_orientation = next_state[&#x27;base_pose&#x27;][3:7]  # quaternion</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            upright_reward = self.calculate_upright_reward(base_orientation) * 2.0</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            reward += upright_reward</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            # Smoothness reward - penalize jerky movements</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            action_smoothness = -np.sum(np.abs(action)) * 0.1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            reward += action_smoothness</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            return reward</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        return reward_function</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def reset(self):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Reset environment to initial state</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.simulation.reset()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Apply randomization if enabled</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        if self.task_config.randomize:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            self.randomize_environment()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Get initial observation</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        initial_state = self.get_current_state()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        return self.format_observation(initial_state)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def step(self, action):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Execute one step of the environment</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        previous_state = self.get_current_state()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Apply action to robot</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.apply_action_to_robot(action)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Step simulation</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.simulation.step()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Get new state</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        current_state = self.get_current_state()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Calculate reward</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        reward = self.reward_function(previous_state, action, current_state, {})</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Check termination conditions</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        terminated = self.check_termination(current_state)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        truncated = self.check_truncation(current_state)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Get observation</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        observation = self.format_observation(current_state)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Get info dictionary</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        info = self.get_info_dict(current_state, reward)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        return observation, reward, terminated, truncated, info</span><br></span></code></pre></div></div>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="learning-outcomes">Learning Outcomes<a href="#learning-outcomes" class="hash-link" aria-label="Direct link to Learning Outcomes" title="Direct link to Learning Outcomes" translate="no">​</a></h2>
<p>By the end of this week, students should be able to:</p>
<ol>
<li class="">
<p><strong>Understand Isaac platform architecture</strong> - Explain the components and capabilities of the NVIDIA Isaac ecosystem for AI-powered robotics.</p>
</li>
<li class="">
<p><strong>Implement GPU-accelerated perception</strong> - Create perception pipelines using Isaac ROS packages that leverage GPU acceleration for real-time performance.</p>
</li>
<li class="">
<p><strong>Configure Isaac Sim environments</strong> - Set up high-fidelity simulation environments with appropriate physics properties and synthetic data generation capabilities.</p>
</li>
<li class="">
<p><strong>Design reinforcement learning environments</strong> - Create RL environments in Isaac Lab with appropriate observation spaces, action spaces, and reward functions for humanoid robot tasks.</p>
</li>
<li class="">
<p><strong>Integrate Isaac components</strong> - Connect simulation, perception, and control systems using the Isaac platform for comprehensive humanoid robot development.</p>
</li>
</ol>
<hr>
<h1>Week 9: AI Perception and Control</h1>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="introduction-1">Introduction<a href="#introduction-1" class="hash-link" aria-label="Direct link to Introduction" title="Direct link to Introduction" translate="no">​</a></h2>
<p>AI perception and control form the cognitive core of humanoid robots, enabling them to understand their environment, make intelligent decisions, and execute complex behaviors. This week explores the integration of artificial intelligence techniques with robotic perception and control systems, focusing on computer vision, sensor fusion, and intelligent control strategies that enable humanoid robots to operate autonomously in complex environments.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="computer-vision-for-robotics">Computer Vision for Robotics<a href="#computer-vision-for-robotics" class="hash-link" aria-label="Direct link to Computer Vision for Robotics" title="Direct link to Computer Vision for Robotics" translate="no">​</a></h2>
<p>Computer vision provides humanoid robots with the ability to interpret visual information from cameras and other optical sensors, enabling navigation, object recognition, and interaction with the environment.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="deep-learning-based-object-detection">Deep Learning-Based Object Detection<a href="#deep-learning-based-object-detection" class="hash-link" aria-label="Direct link to Deep Learning-Based Object Detection" title="Direct link to Deep Learning-Based Object Detection" translate="no">​</a></h3>
<p>Modern object detection systems use deep neural networks to identify and locate objects in images:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">Pseudocode: Object Detection System</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">class ObjectDetectionSystem:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def __init__(self):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.detection_model = self.load_pretrained_model(&#x27;yolov8&#x27;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.feature_extractor = FeatureExtractor()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.tracker = ObjectTracker()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.classifier = ObjectClassifier()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def detect_objects(self, image):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Preprocess image for neural network</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        processed_image = self.preprocess_image(image)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Run object detection</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        detections = self.detection_model.inference(processed_image)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Filter detections by confidence threshold</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        confident_detections = [</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            det for det in detections</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            if det.confidence &gt; self.confidence_threshold</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        ]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Extract features for tracking</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        for detection in confident_detections:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            detection.features = self.feature_extractor.extract(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                image, detection.bbox</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Update object tracker</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        tracked_objects = self.tracker.update(confident_detections)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        return tracked_objects</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def preprocess_image(self, image):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Resize and normalize image</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        resized = cv2.resize(image, (640, 640))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        normalized = resized.astype(np.float32) / 255.0</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        normalized = np.transpose(normalized, (2, 0, 1))  # HWC to CHW</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        return normalized</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def load_pretrained_model(self, model_name):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Load pre-trained model with GPU acceleration</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        if model_name == &#x27;yolov8&#x27;:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            model = YOLOv8Model()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            model.load_weights(&#x27;yolov8_weights.pt&#x27;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            model.to_gpu()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            return model</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        elif model_name == &#x27;detectnet&#x27;:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            return DetectNetModel()</span><br></span></code></pre></div></div>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="semantic-segmentation">Semantic Segmentation<a href="#semantic-segmentation" class="hash-link" aria-label="Direct link to Semantic Segmentation" title="Direct link to Semantic Segmentation" translate="no">​</a></h3>
<p>Semantic segmentation provides pixel-level understanding of the environment:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">Pseudocode: Semantic Segmentation System</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">class SemanticSegmentation:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def __init__(self):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.segmentation_model = SegmentationModel(&#x27;deeplabv3&#x27;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.color_map = self.create_color_map()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.post_processor = SegmentationPostProcessor()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def segment_image(self, image):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Run segmentation inference</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        raw_output = self.segmentation_model.inference(image)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Apply softmax to get class probabilities</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        probabilities = self.softmax(raw_output)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Get predicted class for each pixel</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        predicted_classes = np.argmax(probabilities, axis=0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Apply post-processing to refine boundaries</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        refined_mask = self.post_processor.refine_boundaries(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            predicted_classes, image</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Convert to colored segmentation map</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        colored_map = self.colorize_segmentation(refined_mask)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        return {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            &#x27;segmentation_map&#x27;: refined_mask,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            &#x27;colored_map&#x27;: colored_map,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            &#x27;class_probabilities&#x27;: probabilities</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def colorize_segmentation(self, segmentation_map):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Map class indices to RGB colors</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        height, width = segmentation_map.shape</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        colored_map = np.zeros((height, width, 3), dtype=np.uint8)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        for class_idx in np.unique(segmentation_map):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            mask = segmentation_map == class_idx</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            colored_map[mask] = self.color_map[class_idx]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        return colored_map</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def extract_traversable_regions(self, segmentation_result):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Identify walkable areas from segmentation</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        walkable_classes = [&#x27;floor&#x27;, &#x27;grass&#x27;, &#x27;carpet&#x27;, &#x27;road&#x27;]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        walkable_mask = np.zeros_like(segmentation_result[&#x27;segmentation_map&#x27;])</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        for class_name in walkable_classes:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            class_idx = self.get_class_index(class_name)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            walkable_mask[segmentation_result[&#x27;segmentation_map&#x27;] == class_idx] = 1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        return walkable_mask</span><br></span></code></pre></div></div>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="3d-perception-and-depth-estimation">3D Perception and Depth Estimation<a href="#3d-perception-and-depth-estimation" class="hash-link" aria-label="Direct link to 3D Perception and Depth Estimation" title="Direct link to 3D Perception and Depth Estimation" translate="no">​</a></h3>
<p>Understanding 3D structure is crucial for humanoid robot navigation and manipulation:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">Pseudocode: 3D Perception System</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">class ThreeDPerception:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def __init__(self):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.depth_estimator = DepthEstimator(&#x27;monodepth2&#x27;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.pointcloud_generator = PointCloudGenerator()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.surface_analyzer = SurfaceAnalyzer()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.obstacle_detector = ObstacleDetector()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def process_3d_data(self, stereo_images=None, depth_image=None, rgb_image=None):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        if depth_image is not None:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            # Use provided depth image</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            depth_map = depth_image</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        elif stereo_images is not None:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            # Estimate depth from stereo images</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            depth_map = self.depth_estimator.from_stereo(stereo_images)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        elif rgb_image is not None:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            # Estimate depth from single RGB image</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            depth_map = self.depth_estimator.from_monocular(rgb_image)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Generate point cloud</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        pointcloud = self.pointcloud_generator.from_depth(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            depth_map, self.camera_intrinsics</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Analyze surfaces for navigation</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        surfaces = self.surface_analyzer.analyze(pointcloud)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Detect obstacles</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        obstacles = self.obstacle_detector.detect(pointcloud, surfaces)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        return {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            &#x27;depth_map&#x27;: depth_map,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            &#x27;pointcloud&#x27;: pointcloud,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            &#x27;surfaces&#x27;: surfaces,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            &#x27;obstacles&#x27;: obstacles</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def generate_occupancy_grid(self, pointcloud, resolution=0.1):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Create 2D occupancy grid from 3D point cloud</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        min_x, min_y = np.min(pointcloud[:, :2], axis=0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        max_x, max_y = np.max(pointcloud[:, :2], axis=0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        grid_width = int((max_x - min_x) / resolution)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        grid_height = int((max_y - min_y) / resolution)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        occupancy_grid = np.zeros((grid_width, grid_height), dtype=np.float32)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        for point in pointcloud:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            x_idx = int((point[0] - min_x) / resolution)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            y_idx = int((point[1] - min_y) / resolution)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            if 0 &lt;= x_idx &lt; grid_width and 0 &lt;= y_idx &lt; grid_height:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                # Mark as occupied if point is below robot height threshold</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                if point[2] &lt; self.robot_height_threshold:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                    occupancy_grid[x_idx, y_idx] = 1.0</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        return occupancy_grid</span><br></span></code></pre></div></div>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="sensor-fusion-and-state-estimation">Sensor Fusion and State Estimation<a href="#sensor-fusion-and-state-estimation" class="hash-link" aria-label="Direct link to Sensor Fusion and State Estimation" title="Direct link to Sensor Fusion and State Estimation" translate="no">​</a></h2>
<p>Integrating data from multiple sensors provides a more complete and accurate understanding of the robot&#x27;s state and environment.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="multi-sensor-data-integration">Multi-Sensor Data Integration<a href="#multi-sensor-data-integration" class="hash-link" aria-label="Direct link to Multi-Sensor Data Integration" title="Direct link to Multi-Sensor Data Integration" translate="no">​</a></h3>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">Pseudocode: Sensor Fusion System</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">class SensorFusion:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def __init__(self):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.kalman_filter = ExtendedKalmanFilter()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.particle_filter = ParticleFilter()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.imu_processor = IMUProcessor()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.odometry_processor = OdometryProcessor()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.vision_processor = VisionProcessor()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def initialize_state_estimator(self, initial_pose, initial_covariance):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Initialize Kalman filter state</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.kalman_filter.initialize(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            state=initial_pose,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            covariance=initial_covariance</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Initialize particle filter for multimodal distributions</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.particle_filter.initialize(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            num_particles=1000,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            initial_distribution=initial_pose</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def fuse_sensor_data(self, imu_data, odometry_data, vision_data, dt):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Process IMU data (high frequency, relative measurements)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        imu_prediction = self.imu_processor.process(imu_data, dt)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Process odometry data (medium frequency, relative measurements)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        odometry_correction = self.odometry_processor.process(odometry_data)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Process vision data (low frequency, absolute measurements)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        vision_correction = self.vision_processor.process(vision_data)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Fuse all sensor data using Kalman filter</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        prediction = self.kalman_filter.predict(imu_prediction, dt)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        corrected_state = self.kalman_filter.update(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            prediction,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            [odometry_correction, vision_correction]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Update particle filter for non-linear estimation</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.particle_filter.update(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            odometry_correction,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            vision_correction,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            imu_prediction</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        return {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            &#x27;estimated_pose&#x27;: corrected_state,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            &#x27;uncertainty&#x27;: self.kalman_filter.covariance,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            &#x27;particles&#x27;: self.particle_filter.particles</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def handle_sensor_failures(self, sensor_data):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Implement sensor failure detection and graceful degradation</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        for sensor_name, data in sensor_data.items():</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            if not self.is_sensor_valid(data):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                # Switch to alternative sensor or prediction-only mode</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                self.enable_sensor_backup(sensor_name)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                self.log_sensor_failure(sensor_name)</span><br></span></code></pre></div></div>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="visual-inertial-odometry-vio">Visual-Inertial Odometry (VIO)<a href="#visual-inertial-odometry-vio" class="hash-link" aria-label="Direct link to Visual-Inertial Odometry (VIO)" title="Direct link to Visual-Inertial Odometry (VIO)" translate="no">​</a></h3>
<p>Combining visual and inertial measurements for robust pose estimation:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">Pseudocode: Visual-Inertial Odometry</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">class VisualInertialOdometry:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def __init__(self):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.feature_detector = FeatureDetector(&#x27;orb&#x27;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.feature_tracker = FeatureTracker()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.imu_integrator = IMUIntegrator()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.optimization_backend = OptimizationBackend()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.keyframe_manager = KeyframeManager()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def estimate_pose(self, current_image, imu_measurements, previous_pose):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Track features from previous frame</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        tracked_features = self.feature_tracker.track(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            previous_image=self.previous_image,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            current_image=current_image</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Integrate IMU measurements for prediction</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        imu_prediction = self.imu_integrator.integrate(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            imu_measurements, self.dt</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Estimate pose from feature correspondences</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        visual_pose_estimate = self.estimate_pose_from_features(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            tracked_features, self.camera_intrinsics</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Fuse visual and inertial estimates</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        fused_pose = self.fuse_visual_inertial(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            visual_estimate=visual_pose_estimate,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            inertial_estimate=imu_prediction,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            previous_pose=previous_pose</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Optimize pose graph for consistency</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        optimized_pose = self.optimize_pose_graph(fused_pose)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Manage keyframes for map building</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        if self.should_add_keyframe(optimized_pose):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            self.keyframe_manager.add_keyframe(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                image=current_image,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                pose=optimized_pose</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.previous_image = current_image</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        return optimized_pose</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def estimate_pose_from_features(self, features, intrinsics):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Use PnP (Perspective-n-Point) algorithm</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        object_points = self.get_3d_points(features)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        image_points = self.get_2d_points(features)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        success, rvec, tvec = cv2.solvePnP(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            object_points, image_points, intrinsics, None</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        if success:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            # Convert rotation vector to rotation matrix</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            rotation_matrix, _ = cv2.Rodrigues(rvec)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            # Create transformation matrix</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            transform = np.eye(4)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            transform[:3, :3] = rotation_matrix</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            transform[:3, 3] = tvec.flatten()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            return transform</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        else:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            return None</span><br></span></code></pre></div></div>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="intelligent-control-systems">Intelligent Control Systems<a href="#intelligent-control-systems" class="hash-link" aria-label="Direct link to Intelligent Control Systems" title="Direct link to Intelligent Control Systems" translate="no">​</a></h2>
<p>AI-based control systems enable humanoid robots to execute complex behaviors and adapt to changing conditions.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="model-predictive-control-mpc-with-learning">Model Predictive Control (MPC) with Learning<a href="#model-predictive-control-mpc-with-learning" class="hash-link" aria-label="Direct link to Model Predictive Control (MPC) with Learning" title="Direct link to Model Predictive Control (MPC) with Learning" translate="no">​</a></h3>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">Pseudocode: Learning-Based MPC</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">class LearningBasedMPC:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def __init__(self, robot_model):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.robot_model = robot_model</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.dynamics_predictor = DynamicsPredictor()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.trajectory_optimizer = TrajectoryOptimizer()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.learning_module = LearningModule()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.safety_checker = SafetyChecker()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def compute_control_command(self, current_state, reference_trajectory, horizon=10):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Use learned dynamics model to predict future states</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        predicted_trajectories = []</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        for control_sequence in self.generate_control_candidates(horizon):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            predicted_states = []</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            state = current_state.copy()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            for control in control_sequence:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                # Predict next state using learned dynamics</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                next_state = self.dynamics_predictor.predict(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                    state, control, self.dt</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                # Check safety constraints</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                if not self.safety_checker.is_safe(next_state):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                    break</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                predicted_states.append(next_state)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                state = next_state</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            if len(predicted_states) == len(control_sequence):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                cost = self.evaluate_trajectory_cost(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                    predicted_states, reference_trajectory</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                predicted_trajectories.append({</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                    &#x27;states&#x27;: predicted_states,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                    &#x27;controls&#x27;: control_sequence,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                    &#x27;cost&#x27;: cost</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                })</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Select optimal trajectory</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        optimal_trajectory = min(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            predicted_trajectories,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            key=lambda x: x[&#x27;cost&#x27;]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Apply learning to improve future predictions</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.learning_module.update(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            current_state,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            optimal_trajectory[&#x27;controls&#x27;][0]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        return optimal_trajectory[&#x27;controls&#x27;][0]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def evaluate_trajectory_cost(self, predicted_states, reference_trajectory):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        total_cost = 0.0</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        for i, (pred_state, ref_state) in enumerate(zip(predicted_states, reference_trajectory)):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            # Tracking cost</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            tracking_error = np.linalg.norm(pred_state - ref_state)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            total_cost += tracking_error ** 2</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            # Control effort cost</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            if i &lt; len(predicted_states) - 1:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                control_diff = predicted_states[i+1][&#x27;control&#x27;] - pred_state[&#x27;control&#x27;]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                total_cost += 0.1 * np.sum(control_diff ** 2)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            # Safety cost</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            if not self.safety_checker.is_safe(pred_state):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                total_cost += 1000.0  # High penalty for unsafe states</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        return total_cost</span><br></span></code></pre></div></div>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="adaptive-control-with-neural-networks">Adaptive Control with Neural Networks<a href="#adaptive-control-with-neural-networks" class="hash-link" aria-label="Direct link to Adaptive Control with Neural Networks" title="Direct link to Adaptive Control with Neural Networks" translate="no">​</a></h3>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">Pseudocode: Neural Adaptive Controller</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">class NeuralAdaptiveController:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def __init__(self, robot_dof):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.robot_dof = robot_dof</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.adaptive_network = self.build_adaptive_network()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.feedback_controller = PIDController()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.reference_model = ReferenceModel()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.parameter_estimator = ParameterEstimator()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def build_adaptive_network(self):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Build neural network for adaptive control</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        network = Sequential([</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            Dense(128, activation=&#x27;relu&#x27;, input_shape=(self.robot_dof * 4,)),  # [pos, vel, ref_pos, ref_vel]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            Dense(64, activation=&#x27;relu&#x27;),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            Dense(32, activation=&#x27;relu&#x27;),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            Dense(self.robot_dof, activation=&#x27;linear&#x27;)  # Adaptive control terms</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        ])</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        return network</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def compute_adaptive_control(self, current_state, desired_state, dt):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Extract relevant state information</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        state_error = current_state[&#x27;position&#x27;] - desired_state[&#x27;position&#x27;]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        velocity_error = current_state[&#x27;velocity&#x27;] - desired_state[&#x27;velocity&#x27;]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Prepare network input</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        network_input = np.concatenate([</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            current_state[&#x27;position&#x27;],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            current_state[&#x27;velocity&#x27;],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            desired_state[&#x27;position&#x27;],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            desired_state[&#x27;velocity&#x27;]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        ])</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Compute adaptive control terms</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        adaptive_terms = self.adaptive_network.predict(network_input)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Compute feedback control</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        feedback_control = self.feedback_controller.compute(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            state_error, velocity_error, dt</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Combine adaptive and feedback control</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        total_control = feedback_control + adaptive_terms</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Update network based on tracking performance</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.update_adaptive_network(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            network_input, total_control, state_error</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        return total_control</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def update_adaptive_network(self, state_input, control_output, tracking_error):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Define target adaptive terms to minimize tracking error</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        target_adaptive = self.compute_target_adaptive(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            tracking_error, control_output</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Train network to predict required adaptive terms</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.adaptive_network.fit(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            x=state_input.reshape(1, -1),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            y=target_adaptive.reshape(1, -1),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            epochs=1,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            verbose=0</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        )</span><br></span></code></pre></div></div>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="learning-and-adaptation">Learning and Adaptation<a href="#learning-and-adaptation" class="hash-link" aria-label="Direct link to Learning and Adaptation" title="Direct link to Learning and Adaptation" translate="no">​</a></h2>
<p>Enabling robots to learn from experience and adapt to new situations is crucial for autonomous operation.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="online-learning-systems">Online Learning Systems<a href="#online-learning-systems" class="hash-link" aria-label="Direct link to Online Learning Systems" title="Direct link to Online Learning Systems" translate="no">​</a></h3>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">Pseudocode: Online Learning System</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">class OnlineLearningSystem:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def __init__(self):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.behavior_models = {}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.experience_buffer = ExperienceBuffer(max_size=10000)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.learning_algorithm = IncrementalLearner()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.performance_monitor = PerformanceMonitor()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def update_behavior_model(self, task, observation, action, reward, next_observation):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Store experience in buffer</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        experience = {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            &#x27;task&#x27;: task,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            &#x27;observation&#x27;: observation,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            &#x27;action&#x27;: action,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            &#x27;reward&#x27;: reward,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            &#x27;next_observation&#x27;: next_observation,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            &#x27;timestamp&#x27;: time.time()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.experience_buffer.add(experience)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Update behavior model incrementally</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        if self.should_update_model(task):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            self.learning_algorithm.incremental_update(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                task,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                self.experience_buffer.get_recent_experiences(task, 100)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Monitor performance and trigger retraining if needed</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        performance = self.performance_monitor.evaluate(task)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        if performance &lt; self.performance_threshold:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            self.trigger_retraining(task)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def predict_action(self, task, observation):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Use learned model to predict best action</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        if task in self.behavior_models:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            return self.behavior_models[task].predict(observation)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        else:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            # Use default controller for unknown tasks</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            return self.default_controller(observation)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def transfer_learning(self, source_task, target_task):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Transfer knowledge from source task to target task</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        source_model = self.behavior_models[source_task]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        target_model = self.initialize_model(target_task)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Fine-tune target model using source model weights</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        target_model.transfer_weights(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            source_model,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            learning_rate=0.001</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Adapt to target task using few examples</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        for experience in self.get_target_task_experiences(target_task, 50):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            target_model.update(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                experience[&#x27;observation&#x27;],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                experience[&#x27;action&#x27;]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.behavior_models[target_task] = target_model</span><br></span></code></pre></div></div>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="learning-outcomes-1">Learning Outcomes<a href="#learning-outcomes-1" class="hash-link" aria-label="Direct link to Learning Outcomes" title="Direct link to Learning Outcomes" translate="no">​</a></h2>
<p>By the end of this week, students should be able to:</p>
<ol>
<li class="">
<p><strong>Implement computer vision systems</strong> - Design and implement object detection, semantic segmentation, and 3D perception systems for robotic applications.</p>
</li>
<li class="">
<p><strong>Fuse multi-sensor data</strong> - Integrate data from cameras, IMUs, odometry, and other sensors using Kalman filters and other fusion techniques.</p>
</li>
<li class="">
<p><strong>Design intelligent control systems</strong> - Create adaptive and learning-based controllers that enable robots to improve their performance over time.</p>
</li>
<li class="">
<p><strong>Apply model predictive control</strong> - Implement MPC systems that use learned dynamics models for optimal trajectory planning and control.</p>
</li>
<li class="">
<p><strong>Develop online learning capabilities</strong> - Create systems that enable robots to learn from experience and adapt their behavior in real-time.</p>
</li>
</ol>
<hr>
<h1>Week 10: Reinforcement Learning for Robot Control</h1>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="introduction-2">Introduction<a href="#introduction-2" class="hash-link" aria-label="Direct link to Introduction" title="Direct link to Introduction" translate="no">​</a></h2>
<p>Reinforcement Learning (RL) has emerged as a powerful paradigm for developing adaptive and intelligent robot control systems. This week explores how RL algorithms can be applied to teach humanoid robots complex behaviors, from basic locomotion to sophisticated manipulation tasks. We examine various RL approaches, their implementation in robotic systems, and the challenges of applying these techniques to real-world robot control problems.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="foundations-of-reinforcement-learning-for-robotics">Foundations of Reinforcement Learning for Robotics<a href="#foundations-of-reinforcement-learning-for-robotics" class="hash-link" aria-label="Direct link to Foundations of Reinforcement Learning for Robotics" title="Direct link to Foundations of Reinforcement Learning for Robotics" translate="no">​</a></h2>
<p>Reinforcement learning provides a framework for learning optimal behaviors through interaction with the environment, making it particularly well-suited for robotic applications where explicit programming of all possible scenarios is infeasible.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="rl-framework-for-robotics">RL Framework for Robotics<a href="#rl-framework-for-robotics" class="hash-link" aria-label="Direct link to RL Framework for Robotics" title="Direct link to RL Framework for Robotics" translate="no">​</a></h3>
<p>The RL framework in robotics consists of:</p>
<ul>
<li class=""><strong>Agent</strong>: The robot learning to perform tasks</li>
<li class=""><strong>Environment</strong>: The physical or simulated world where the robot operates</li>
<li class=""><strong>State</strong>: Robot&#x27;s current configuration and environmental information</li>
<li class=""><strong>Action</strong>: Control commands sent to the robot&#x27;s actuators</li>
<li class=""><strong>Reward</strong>: Feedback signal indicating task success or failure</li>
<li class=""><strong>Policy</strong>: Strategy that maps states to actions</li>
</ul>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">Pseudocode: Basic RL Framework for Robotics</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">class RobotRLAgent:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def __init__(self, robot_env, policy_network, learning_rate=0.001):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.env = robot_env</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.policy_network = policy_network</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.learning_rate = learning_rate</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.optimizer = AdamOptimizer(learning_rate)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.replay_buffer = ReplayBuffer(max_size=100000)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.exploration_noise = OrnsteinUhlenbeckProcess()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def train_step(self, state, action, reward, next_state, done):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Store experience in replay buffer</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.replay_buffer.add(state, action, reward, next_state, done)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Sample batch from replay buffer</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        batch = self.replay_buffer.sample(batch_size=32)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Update policy network</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        with tf.GradientTape() as tape:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            # Compute predicted actions</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            predicted_actions = self.policy_network(batch.states)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            # Compute loss (negative expected return)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            loss = self.compute_policy_loss(batch, predicted_actions)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Apply gradients</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        gradients = tape.gradient(loss, self.policy_network.trainable_variables)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.optimizer.apply_gradients(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            zip(gradients, self.policy_network.trainable_variables)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        return loss</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def select_action(self, state, explore=True):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Get action from policy network</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        action = self.policy_network.predict(state)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        if explore:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            # Add exploration noise</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            noise = self.exploration_noise.sample()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            action = action + noise</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Ensure action is within bounds</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        action = np.clip(action, self.env.action_space.low, self.env.action_space.high)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        return action</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def compute_policy_loss(self, batch, predicted_actions):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Compute policy gradient loss</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # This is a simplified version - actual implementation depends on specific algorithm</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        advantages = self.compute_advantages(batch.rewards, batch.values)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        log_probs = self.compute_log_probs(predicted_actions, batch.actions)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        policy_loss = -tf.reduce_mean(advantages * log_probs)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        return policy_loss</span><br></span></code></pre></div></div>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="reward-function-design">Reward Function Design<a href="#reward-function-design" class="hash-link" aria-label="Direct link to Reward Function Design" title="Direct link to Reward Function Design" translate="no">​</a></h3>
<p>Designing effective reward functions is critical for successful RL in robotics:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">Pseudocode: Reward Function Design</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">class RewardFunctionDesigner:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def __init__(self):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.components = {}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.weights = {}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.normalization_factors = {}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def design_locomotion_reward(self):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Design reward function for bipedal locomotion</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        def reward_function(state, action, next_state, info):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            reward = 0.0</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            # Forward progress reward</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            forward_velocity = next_state[&#x27;base_linear_velocity&#x27;][0]  # x-direction</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            target_velocity = 1.0  # m/s</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            velocity_reward = max(0, forward_velocity)  # Only reward forward motion</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            reward += velocity_reward * 2.0</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            # Balance reward - maintain upright posture</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            base_orientation = next_state[&#x27;base_orientation&#x27;]  # quaternion</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            upright_reward = self.calculate_upright_reward(base_orientation)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            reward += upright_reward * 3.0</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            # Energy efficiency reward - penalize excessive joint torques</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            joint_torques = next_state[&#x27;joint_torques&#x27;]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            energy_penalty = -np.mean(np.abs(joint_torques)) * 0.1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            reward += energy_penalty</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            # Smoothness reward - penalize jerky movements</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            action_smoothness = -np.sum(np.abs(action)) * 0.05</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            reward += action_smoothness</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            # Safety reward - penalize dangerous configurations</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            if self.is_unsafe_configuration(next_state):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                reward += -10.0  # Large penalty for unsafe states</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            # Normalize reward</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            reward = np.clip(reward, -10.0, 10.0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            return reward</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        return reward_function</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def design_manipulation_reward(self):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Design reward function for manipulation tasks</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        def reward_function(state, action, next_state, info):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            reward = 0.0</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            # Distance to target reward</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            target_pos = info[&#x27;target_position&#x27;]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            end_effector_pos = next_state[&#x27;end_effector_position&#x27;]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            distance = np.linalg.norm(target_pos - end_effector_pos)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            distance_reward = -distance  # Negative distance (closer is better)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            reward += distance_reward * 5.0</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            # Grasping reward</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            if self.is_grasping_object(next_state):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                reward += 10.0</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                # Bonus for stable grasp</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                grasp_stability = self.calculate_grasp_stability(next_state)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                reward += grasp_stability * 2.0</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            # Avoid collisions</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            if self.is_in_collision(next_state):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                reward += -5.0</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            # Joint limit penalty</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            joint_angles = next_state[&#x27;joint_positions&#x27;]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            joint_limit_penalty = self.calculate_joint_limit_penalty(joint_angles)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            reward += joint_limit_penalty * -1.0</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            return reward</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        return reward_function</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def calculate_upright_reward(self, orientation_quat):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Calculate how upright the robot is (1.0 = perfectly upright, -1.0 = upside down)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Convert quaternion to rotation matrix and extract z-axis</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        rotation_matrix = self.quaternion_to_rotation_matrix(orientation_quat)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        world_up = np.array([0, 0, 1])</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        robot_up = rotation_matrix[:, 2]  # z-axis of robot frame</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        dot_product = np.dot(world_up, robot_up)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        return max(-1.0, min(1.0, dot_product))  # Clamp between -1 and 1</span><br></span></code></pre></div></div>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="deep-reinforcement-learning-algorithms">Deep Reinforcement Learning Algorithms<a href="#deep-reinforcement-learning-algorithms" class="hash-link" aria-label="Direct link to Deep Reinforcement Learning Algorithms" title="Direct link to Deep Reinforcement Learning Algorithms" translate="no">​</a></h2>
<p>Deep RL algorithms combine neural networks with reinforcement learning to handle high-dimensional state and action spaces common in robotics.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="deep-deterministic-policy-gradient-ddpg">Deep Deterministic Policy Gradient (DDPG)<a href="#deep-deterministic-policy-gradient-ddpg" class="hash-link" aria-label="Direct link to Deep Deterministic Policy Gradient (DDPG)" title="Direct link to Deep Deterministic Policy Gradient (DDPG)" translate="no">​</a></h3>
<p>DDPG is suitable for continuous control tasks like robot joint control:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">Pseudocode: DDPG Implementation</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">class DDPGAgent:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def __init__(self, state_dim, action_dim, action_bound):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.state_dim = state_dim</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.action_dim = action_dim</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.action_bound = action_bound</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Actor network (policy)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.actor = self.create_actor_network()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.target_actor = self.create_actor_network()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Critic network (value function)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.critic = self.create_critic_network()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.target_critic = self.create_critic_network()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Initialize target networks with same weights as main networks</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.target_actor.set_weights(self.actor.get_weights())</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.target_critic.set_weights(self.critic.get_weights())</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.replay_buffer = ReplayBuffer(max_size=100000)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.noise_process = OrnsteinUhlenbeckProcess(size=action_dim)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Hyperparameters</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.gamma = 0.99  # Discount factor</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.tau = 0.005   # Target network update rate</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.learning_rate = 0.001</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def create_actor_network(self):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Actor network: state -&gt; action</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        model = Sequential([</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            Dense(400, activation=&#x27;relu&#x27;, input_shape=(self.state_dim,)),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            Dense(300, activation=&#x27;relu&#x27;),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            Dense(self.action_dim, activation=&#x27;tanh&#x27;)  # Output in [-1, 1]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        ])</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Scale output to action bounds</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        def scale_action(action):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            return action * self.action_bound</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        model.add(Lambda(scale_action))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        model.compile(optimizer=Adam(learning_rate=self.learning_rate))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        return model</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def create_critic_network(self):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Critic network: (state, action) -&gt; Q-value</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        state_input = Input(shape=(self.state_dim,))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        action_input = Input(shape=(self.action_dim,))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Process state</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        state_hidden = Dense(400, activation=&#x27;relu&#x27;)(state_input)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        state_hidden = Dense(300)(state_hidden)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Process action</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        action_hidden = Dense(300)(action_input)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Combine state and action</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        combined = Add()([state_hidden, action_hidden])</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        combined = Activation(&#x27;relu&#x27;)(combined)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Output Q-value</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        q_value = Dense(1)(combined)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        model = Model(inputs=[state_input, action_input], outputs=q_value)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        model.compile(optimizer=Adam(learning_rate=self.learning_rate), loss=&#x27;mse&#x27;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        return model</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def update_networks(self):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        if len(self.replay_buffer) &lt; 64:  # Batch size</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            return</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Sample batch from replay buffer</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        batch = self.replay_buffer.sample(64)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        states, actions, rewards, next_states, dones = batch</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Update critic network</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        with tf.GradientTape() as tape:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            # Get target Q-values from target networks</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            next_actions = self.target_actor(next_states)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            target_q_values = self.target_critic([next_states, next_actions])</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            target_q_values = rewards + (1 - dones) * self.gamma * target_q_values</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            # Current Q-values</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            current_q_values = self.critic([states, actions])</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            # Critic loss</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            critic_loss = tf.reduce_mean(tf.square(target_q_values - current_q_values))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Apply critic gradients</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        critic_gradients = tape.gradient(critic_loss, self.critic.trainable_variables)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.critic.optimizer.apply_gradients(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            zip(critic_gradients, self.critic.trainable_variables)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Update actor network (policy gradient)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        with tf.GradientTape() as tape:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            # Get actions from current policy</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            current_actions = self.actor(states)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            # Get Q-values for these actions</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            q_values = self.critic([states, current_actions])</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            # Actor loss (maximize Q-values)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            actor_loss = -tf.reduce_mean(q_values)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Apply actor gradients</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        actor_gradients = tape.gradient(actor_loss, self.actor.trainable_variables)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.actor.optimizer.apply_gradients(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            zip(actor_gradients, self.actor.trainable_variables)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Update target networks (soft update)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.update_target_networks()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def update_target_networks(self):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Soft update target networks</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        actor_weights = self.actor.get_weights()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        target_actor_weights = self.target_actor.get_weights()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        for i in range(len(actor_weights)):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            target_actor_weights[i] = (</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                self.tau * actor_weights[i] + (1 - self.tau) * target_actor_weights[i]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.target_actor.set_weights(target_actor_weights)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        critic_weights = self.critic.get_weights()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        target_critic_weights = self.target_critic.get_weights()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        for i in range(len(critic_weights)):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            target_critic_weights[i] = (</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                self.tau * critic_weights[i] + (1 - self.tau) * target_critic_weights[i]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.target_critic.set_weights(target_critic_weights)</span><br></span></code></pre></div></div>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="twin-delayed-ddpg-td3">Twin Delayed DDPG (TD3)<a href="#twin-delayed-ddpg-td3" class="hash-link" aria-label="Direct link to Twin Delayed DDPG (TD3)" title="Direct link to Twin Delayed DDPG (TD3)" translate="no">​</a></h3>
<p>TD3 addresses overestimation bias in DDPG:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">Pseudocode: TD3 Implementation</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">class TD3Agent:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def __init__(self, state_dim, action_dim, action_bound):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.state_dim = state_dim</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.action_dim = action_dim</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.action_bound = action_bound</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Actor network (single)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.actor = self.create_actor_network()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.target_actor = self.create_actor_network()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Two critic networks (twin critics)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.critic1 = self.create_critic_network()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.critic2 = self.create_critic_network()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.target_critic1 = self.create_critic_network()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.target_critic2 = self.create_critic_network()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Initialize target networks</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.target_actor.set_weights(self.actor.get_weights())</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.target_critic1.set_weights(self.critic1.get_weights())</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.target_critic2.set_weights(self.critic2.get_weights())</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.replay_buffer = ReplayBuffer(max_size=100000)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.noise_std = 0.1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.target_policy_noise_std = 0.2</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.target_policy_noise_clip = 0.5</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.gamma = 0.99</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.tau = 0.005</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.policy_delay = 2  # Update policy every 2 critic updates</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.update_counter = 0</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def update_networks(self):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        if len(self.replay_buffer) &lt; 100:  # Minimum batch size</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            return</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Sample batch from replay buffer</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        states, actions, rewards, next_states, dones = self.replay_buffer.sample(100)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Add noise to target actions for smoothing</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        target_actions = self.target_actor(next_states)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        noise = tf.random.normal(shape=target_actions.shape, stddev=self.target_policy_noise_std)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        noise = tf.clip_by_value(noise, -self.target_policy_noise_clip, self.target_policy_noise_clip)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        noisy_target_actions = tf.clip_by_value(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            target_actions + noise,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            -self.action_bound,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            self.action_bound</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Compute target Q-values using minimum of twin critics</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        target_q1 = self.target_critic1([next_states, noisy_target_actions])</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        target_q2 = self.target_critic2([next_states, noisy_target_actions])</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        target_q = rewards + (1 - dones) * self.gamma * tf.minimum(target_q1, target_q2)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Update critics</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        with tf.GradientTape(persistent=True) as tape:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            current_q1 = self.critic1([states, actions])</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            current_q2 = self.critic2([states, actions])</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            critic1_loss = tf.reduce_mean(tf.square(target_q - current_q1))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            critic2_loss = tf.reduce_mean(tf.square(target_q - current_q2))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Apply critic gradients</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        critic1_grads = tape.gradient(critic1_loss, self.critic1.trainable_variables)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        critic2_grads = tape.gradient(critic2_loss, self.critic2.trainable_variables)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.critic1.optimizer.apply_gradients(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            zip(critic1_grads, self.critic1.trainable_variables)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.critic2.optimizer.apply_gradients(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            zip(critic2_grads, self.critic2.trainable_variables)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Update actor (delayed)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.update_counter += 1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        if self.update_counter % self.policy_delay == 0:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            with tf.GradientTape() as tape:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                current_actions = self.actor(states)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                actor_q = self.critic1([states, current_actions])</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                actor_loss = -tf.reduce_mean(actor_q)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            actor_grads = tape.gradient(actor_loss, self.actor.trainable_variables)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            self.actor.optimizer.apply_gradients(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                zip(actor_grads, self.actor.trainable_variables)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Update target networks</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.update_target_networks()</span><br></span></code></pre></div></div>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="soft-actor-critic-sac">Soft Actor-Critic (SAC)<a href="#soft-actor-critic-sac" class="hash-link" aria-label="Direct link to Soft Actor-Critic (SAC)" title="Direct link to Soft Actor-Critic (SAC)" translate="no">​</a></h3>
<p>SAC maximizes both expected reward and entropy for better exploration:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">Pseudocode: SAC Implementation</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">class SACAgent:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def __init__(self, state_dim, action_dim, action_bound):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.state_dim = state_dim</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.action_dim = action_dim</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.action_bound = action_bound</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Actor network (stochastic policy)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.actor = self.create_actor_network()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Two Q-networks (twin Q-learning)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.q_network1 = self.create_q_network()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.q_network2 = self.create_q_network()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.target_q_network1 = self.create_q_network()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.target_q_network2 = self.create_q_network()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Temperature parameter (entropy regularization)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.log_alpha = tf.Variable(0.0, trainable=True)  # log of alpha</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.alpha_optimizer = Adam(learning_rate=3e-4)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Initialize target networks</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.target_q_network1.set_weights(self.q_network1.get_weights())</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.target_q_network2.set_weights(self.q_network2.get_weights())</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.replay_buffer = ReplayBuffer(max_size=100000)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.target_entropy = -action_dim  # Target entropy for automatic alpha tuning</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.gamma = 0.99</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.tau = 0.005</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def create_actor_network(self):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Stochastic actor that outputs mean and std of Gaussian policy</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        state_input = Input(shape=(self.state_dim,))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        hidden = Dense(256, activation=&#x27;relu&#x27;)(state_input)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        hidden = Dense(256, activation=&#x27;relu&#x27;)(hidden)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Output mean and log_std for each action dimension</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        mean_output = Dense(self.action_dim, activation=&#x27;tanh&#x27;)(hidden)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        log_std_output = Dense(self.action_dim, activation=&#x27;tanh&#x27;)(hidden)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Scale log_std to reasonable range</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        log_std_output = tf.clip_by_value(log_std_output, -20, 2)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Reparameterization trick for sampling</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        def sample_action(inputs):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            mean, log_std = inputs</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            std = tf.exp(log_std)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            noise = tf.random.normal(shape=tf.shape(mean))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            raw_action = mean + std * noise</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            # Apply tanh to bound actions</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            bounded_action = tf.tanh(raw_action)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            return bounded_action</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        action_output = Lambda(sample_action)([mean_output, log_std_output])</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        model = Model(inputs=state_input, outputs=[mean_output, log_std_output, action_output])</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        return model</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def update_networks(self):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        if len(self.replay_buffer) &lt; 256:  # Batch size</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            return</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Sample batch from replay buffer</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        states, actions, rewards, next_states, dones = self.replay_buffer.sample(256)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Update Q-networks</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        with tf.GradientTape(persistent=True) as tape:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            # Get next state actions and log probabilities</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            next_means, next_log_stds, next_actions = self.actor(next_states)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            next_log_probs = self.gaussian_log_prob(next_means, next_log_stds, next_actions)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            # Compute target Q-values</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            next_q1 = self.target_q_network1(next_states)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            next_q2 = self.target_q_network2(next_states)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            next_q_min = tf.minimum(next_q1, next_q2)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            target_q = rewards + (1 - dones) * self.gamma * (</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                next_q_min - tf.exp(self.log_alpha) * next_log_probs</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            # Current Q-values</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            current_q1 = self.q_network1(states)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            current_q2 = self.q_network2(states)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            # Q-network losses</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            q1_loss = tf.reduce_mean(tf.square(target_q - current_q1))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            q2_loss = tf.reduce_mean(tf.square(target_q - current_q2))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Apply Q-network gradients</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        q1_grads = tape.gradient(q1_loss, self.q_network1.trainable_variables)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        q2_grads = tape.gradient(q2_loss, self.q_network2.trainable_variables)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.q_network1.optimizer.apply_gradients(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            zip(q1_grads, self.q_network1.trainable_variables)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.q_network2.optimizer.apply_gradients(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            zip(q2_grads, self.q_network2.trainable_variables)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Update actor network</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        with tf.GradientTape() as tape:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            means, log_stds, sampled_actions = self.actor(states)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            log_probs = self.gaussian_log_prob(means, log_stds, sampled_actions)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            # Q-values for current actions</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            q1_values = self.q_network1(states)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            q2_values = self.q_network2(states)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            min_q_values = tf.minimum(q1_values, q2_values)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            # Actor loss (maximize Q-value and entropy)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            actor_loss = tf.reduce_mean(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                tf.exp(self.log_alpha) * log_probs - min_q_values</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        actor_grads = tape.gradient(actor_loss, self.actor.trainable_variables)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.actor.optimizer.apply_gradients(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            zip(actor_grads, self.actor.trainable_variables)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Update temperature parameter (alpha)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        with tf.GradientTape() as tape:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            means, log_stds, sampled_actions = self.actor(states)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            log_probs = self.gaussian_log_prob(means, log_stds, sampled_actions)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            # Temperature loss (match target entropy)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            alpha_loss = -tf.reduce_mean(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                self.log_alpha * (log_probs + self.target_entropy)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        alpha_grads = tape.gradient(alpha_loss, [self.log_alpha])</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.alpha_optimizer.apply_gradients(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            zip(alpha_grads, [self.log_alpha])</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Update target networks</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.update_target_networks()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def gaussian_log_prob(self, mean, log_std, action):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Compute log probability of action under Gaussian distribution</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        std = tf.exp(log_std)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        var = std ** 2</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        log_prob = -0.5 * ((action - mean) ** 2) / var - 0.5 * tf.math.log(2 * np.pi * var)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        return tf.reduce_sum(log_prob, axis=1, keepdims=True)</span><br></span></code></pre></div></div>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="simulation-to-real-transfer">Simulation-to-Real Transfer<a href="#simulation-to-real-transfer" class="hash-link" aria-label="Direct link to Simulation-to-Real Transfer" title="Direct link to Simulation-to-Real Transfer" translate="no">​</a></h2>
<p>Transferring policies learned in simulation to real robots is a major challenge in robotics RL.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="domain-randomization">Domain Randomization<a href="#domain-randomization" class="hash-link" aria-label="Direct link to Domain Randomization" title="Direct link to Domain Randomization" translate="no">​</a></h3>
<p>Domain randomization helps policies generalize across different environments:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">Pseudocode: Domain Randomization System</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">class DomainRandomization:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def __init__(self):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.randomization_ranges = {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            &#x27;robot_dynamics&#x27;: {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                &#x27;mass_multiplier&#x27;: (0.8, 1.2),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                &#x27;friction_coefficient&#x27;: (0.1, 0.9),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                &#x27;joint_damping&#x27;: (0.01, 0.1),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                &#x27;actuator_delay&#x27;: (0.0, 0.02)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            },</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            &#x27;environment&#x27;: {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                &#x27;gravity&#x27;: (9.5, 10.1),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                &#x27;ground_friction&#x27;: (0.4, 0.8),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                &#x27;lighting_intensity&#x27;: (0.5, 2.0),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                &#x27;texture_roughness&#x27;: (0.0, 1.0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            },</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            &#x27;sensors&#x27;: {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                &#x27;imu_noise&#x27;: (0.001, 0.01),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                &#x27;camera_noise&#x27;: (0.001, 0.005),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                &#x27;delay&#x27;: (0.0, 0.01)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def randomize_environment(self, env):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Randomize robot dynamics</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        mass_multiplier = np.random.uniform(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            self.randomization_ranges[&#x27;robot_dynamics&#x27;][&#x27;mass_multiplier&#x27;][0],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            self.randomization_ranges[&#x27;robot_dynamics&#x27;][&#x27;mass_multiplier&#x27;][1]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        env.robot.set_mass_multiplier(mass_multiplier)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        friction = np.random.uniform(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            self.randomization_ranges[&#x27;robot_dynamics&#x27;][&#x27;friction_coefficient&#x27;][0],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            self.randomization_ranges[&#x27;robot_dynamics&#x27;][&#x27;friction_coefficient&#x27;][1]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        env.robot.set_friction_coefficient(friction)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Randomize environment properties</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        gravity = np.random.uniform(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            self.randomization_ranges[&#x27;environment&#x27;][&#x27;gravity&#x27;][0],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            self.randomization_ranges[&#x27;environment&#x27;][&#x27;gravity&#x27;][1]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        env.set_gravity(gravity)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Randomize sensor properties</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        imu_noise = np.random.uniform(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            self.randomization_ranges[&#x27;sensors&#x27;][&#x27;imu_noise&#x27;][0],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            self.randomization_ranges[&#x27;sensors&#x27;][&#x27;imu_noise&#x27;][1]</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        env.robot.set_imu_noise(imu_noise)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def curriculum_randomization(self, training_step, max_steps):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Gradually increase randomization as training progresses</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        progress = training_step / max_steps</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        randomization_factor = min(progress * 2, 1.0)  # Increase up to 2x range</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        for param, (min_val, max_val) in self.randomization_ranges[&#x27;robot_dynamics&#x27;].items():</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            range_size = (max_val - min_val) * randomization_factor</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            center = (max_val + min_val) / 2</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            new_min = center - range_size / 2</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            new_max = center + range_size / 2</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            self.randomization_ranges[&#x27;robot_dynamics&#x27;][param] = (new_min, new_max)</span><br></span></code></pre></div></div>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="system-identification-and-model-adaptation">System Identification and Model Adaptation<a href="#system-identification-and-model-adaptation" class="hash-link" aria-label="Direct link to System Identification and Model Adaptation" title="Direct link to System Identification and Model Adaptation" translate="no">​</a></h3>
<p>Adapting simulation models to better match real robot behavior:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">Pseudocode: System Identification</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">class SystemIdentifier:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def __init__(self, robot_model):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.robot_model = robot_model</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.param_optimizer = ParameterOptimizer()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.simulator = PhysicsSimulator()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.real_data_buffer = DataBuffer(max_size=10000)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def identify_robot_parameters(self, excitation_signal):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Apply excitation signal to real robot and collect data</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        real_responses = self.excite_robot(excitation_signal)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Optimize simulation parameters to match real responses</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        optimized_params = self.param_optimizer.optimize(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            target_data=real_responses,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            simulation_model=self.simulator,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            initial_params=self.robot_model.get_parameters()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Update robot model with identified parameters</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        self.robot_model.update_parameters(optimized_params)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        return optimized_params</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def excite_robot(self, signal):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Apply known excitation signal to robot joints</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        responses = []</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        for step, command in enumerate(signal):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            # Send command to robot</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            self.robot_model.send_command(command)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            # Record response</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            state = self.robot_model.get_state()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            responses.append({</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                &#x27;time&#x27;: step * self.dt,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                &#x27;position&#x27;: state[&#x27;position&#x27;],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                &#x27;velocity&#x27;: state[&#x27;velocity&#x27;],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                &#x27;torque&#x27;: state[&#x27;torque&#x27;],</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                &#x27;command&#x27;: command</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            })</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        return responses</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    def adaptive_model_learning(self, real_experience):</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        # Update simulation model based on real-world experience</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        for episode in real_experience:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            # Extract system behavior from real data</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            real_behavior = self.extract_behavior(episode)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            # Compare with simulation predictions</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            sim_behavior = self.simulator.predict(episode.initial_state, episode.actions)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            # Compute behavior mismatch</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            mismatch = self.compute_behavior_mismatch(real_behavior, sim_behavior)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            # Update model parameters to reduce mismatch</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            updated_params = self.update_model_parameters(mismatch)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            self.simulator.update_parameters(updated_params)</span><br></span></code></pre></div></div>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="learning-outcomes-2">Learning Outcomes<a href="#learning-outcomes-2" class="hash-link" aria-label="Direct link to Learning Outcomes" title="Direct link to Learning Outcomes" translate="no">​</a></h2>
<p>By the end of this week, students should be able to:</p>
<ol>
<li class="">
<p><strong>Design RL frameworks for robotics</strong> - Create reinforcement learning systems specifically tailored for robotic control tasks with appropriate state, action, and reward definitions.</p>
</li>
<li class="">
<p><strong>Implement advanced RL algorithms</strong> - Apply DDPG, TD3, and SAC algorithms to continuous control problems in humanoid robotics.</p>
</li>
<li class="">
<p><strong>Create effective reward functions</strong> - Design reward functions that promote desired behaviors while avoiding local optima and unsafe configurations.</p>
</li>
<li class="">
<p><strong>Address sim-to-real transfer challenges</strong> - Implement domain randomization and system identification techniques to improve policy transfer from simulation to reality.</p>
</li>
<li class="">
<p><strong>Evaluate and improve RL policies</strong> - Assess policy performance, identify failure modes, and implement strategies for continuous improvement.</p>
</li>
</ol>
<hr></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col noPrint_WFHX"><a href="https://github.com/AqsaIftikhar15/Textbook-for-Teaching-Physical-AI-Humanoid-Robotics-Course/docs/module-3-ai-brain/week-8-10-isaac-platform.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/Textbook-for-Teaching-Physical-AI-Humanoid-Robotics-Course/docs/module-2-digital-twin/simulation-concepts/"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Simulation Concepts</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/Textbook-for-Teaching-Physical-AI-Humanoid-Robotics-Course/docs/module-3-ai-brain/week-11-12-humanoid-dev/"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Humanoid Robot Development</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#introduction" class="table-of-contents__link toc-highlight">Introduction</a></li><li><a href="#nvidia-isaac-platform-architecture" class="table-of-contents__link toc-highlight">NVIDIA Isaac Platform Architecture</a><ul><li><a href="#isaac-platform-components" class="table-of-contents__link toc-highlight">Isaac Platform Components</a></li><li><a href="#gpu-acceleration-framework" class="table-of-contents__link toc-highlight">GPU Acceleration Framework</a></li></ul></li><li><a href="#isaac-ros-gpu-accelerated-ros-2-packages" class="table-of-contents__link toc-highlight">Isaac ROS: GPU-Accelerated ROS 2 Packages</a><ul><li><a href="#isaac-ros-package-ecosystem" class="table-of-contents__link toc-highlight">Isaac ROS Package Ecosystem</a></li><li><a href="#gpu-accelerated-perception-pipeline" class="table-of-contents__link toc-highlight">GPU-Accelerated Perception Pipeline</a></li></ul></li><li><a href="#isaac-sim-physics-simulation-and-synthetic-data-generation" class="table-of-contents__link toc-highlight">Isaac Sim: Physics Simulation and Synthetic Data Generation</a><ul><li><a href="#simulation-environment-architecture" class="table-of-contents__link toc-highlight">Simulation Environment Architecture</a></li><li><a href="#physics-simulation-capabilities" class="table-of-contents__link toc-highlight">Physics Simulation Capabilities</a></li></ul></li><li><a href="#isaac-lab-reinforcement-learning-framework" class="table-of-contents__link toc-highlight">Isaac Lab: Reinforcement Learning Framework</a><ul><li><a href="#reinforcement-learning-environment" class="table-of-contents__link toc-highlight">Reinforcement Learning Environment</a></li></ul></li><li><a href="#learning-outcomes" class="table-of-contents__link toc-highlight">Learning Outcomes</a></li><li><a href="#introduction-1" class="table-of-contents__link toc-highlight">Introduction</a></li><li><a href="#computer-vision-for-robotics" class="table-of-contents__link toc-highlight">Computer Vision for Robotics</a><ul><li><a href="#deep-learning-based-object-detection" class="table-of-contents__link toc-highlight">Deep Learning-Based Object Detection</a></li><li><a href="#semantic-segmentation" class="table-of-contents__link toc-highlight">Semantic Segmentation</a></li><li><a href="#3d-perception-and-depth-estimation" class="table-of-contents__link toc-highlight">3D Perception and Depth Estimation</a></li></ul></li><li><a href="#sensor-fusion-and-state-estimation" class="table-of-contents__link toc-highlight">Sensor Fusion and State Estimation</a><ul><li><a href="#multi-sensor-data-integration" class="table-of-contents__link toc-highlight">Multi-Sensor Data Integration</a></li><li><a href="#visual-inertial-odometry-vio" class="table-of-contents__link toc-highlight">Visual-Inertial Odometry (VIO)</a></li></ul></li><li><a href="#intelligent-control-systems" class="table-of-contents__link toc-highlight">Intelligent Control Systems</a><ul><li><a href="#model-predictive-control-mpc-with-learning" class="table-of-contents__link toc-highlight">Model Predictive Control (MPC) with Learning</a></li><li><a href="#adaptive-control-with-neural-networks" class="table-of-contents__link toc-highlight">Adaptive Control with Neural Networks</a></li></ul></li><li><a href="#learning-and-adaptation" class="table-of-contents__link toc-highlight">Learning and Adaptation</a><ul><li><a href="#online-learning-systems" class="table-of-contents__link toc-highlight">Online Learning Systems</a></li></ul></li><li><a href="#learning-outcomes-1" class="table-of-contents__link toc-highlight">Learning Outcomes</a></li><li><a href="#introduction-2" class="table-of-contents__link toc-highlight">Introduction</a></li><li><a href="#foundations-of-reinforcement-learning-for-robotics" class="table-of-contents__link toc-highlight">Foundations of Reinforcement Learning for Robotics</a><ul><li><a href="#rl-framework-for-robotics" class="table-of-contents__link toc-highlight">RL Framework for Robotics</a></li><li><a href="#reward-function-design" class="table-of-contents__link toc-highlight">Reward Function Design</a></li></ul></li><li><a href="#deep-reinforcement-learning-algorithms" class="table-of-contents__link toc-highlight">Deep Reinforcement Learning Algorithms</a><ul><li><a href="#deep-deterministic-policy-gradient-ddpg" class="table-of-contents__link toc-highlight">Deep Deterministic Policy Gradient (DDPG)</a></li><li><a href="#twin-delayed-ddpg-td3" class="table-of-contents__link toc-highlight">Twin Delayed DDPG (TD3)</a></li><li><a href="#soft-actor-critic-sac" class="table-of-contents__link toc-highlight">Soft Actor-Critic (SAC)</a></li></ul></li><li><a href="#simulation-to-real-transfer" class="table-of-contents__link toc-highlight">Simulation-to-Real Transfer</a><ul><li><a href="#domain-randomization" class="table-of-contents__link toc-highlight">Domain Randomization</a></li><li><a href="#system-identification-and-model-adaptation" class="table-of-contents__link toc-highlight">System Identification and Model Adaptation</a></li></ul></li><li><a href="#learning-outcomes-2" class="table-of-contents__link toc-highlight">Learning Outcomes</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Modules</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/Textbook-for-Teaching-Physical-AI-Humanoid-Robotics-Course/docs/module-1-ros2/week-1-2-intro-physical-ai/">Module 1: The Robotic Nervous System (ROS 2)</a></li><li class="footer__item"><a class="footer__link-item" href="/Textbook-for-Teaching-Physical-AI-Humanoid-Robotics-Course/docs/module-2-digital-twin/week-6-7-gazebo-unity/">Module 2: The Digital Twin (Gazebo &amp; Unity)</a></li><li class="footer__item"><a class="footer__link-item" href="/Textbook-for-Teaching-Physical-AI-Humanoid-Robotics-Course/docs/module-3-ai-brain/week-8-10-isaac-platform/">Module 3: The AI-Robot Brain (NVIDIA Isaac)</a></li><li class="footer__item"><a class="footer__link-item" href="/Textbook-for-Teaching-Physical-AI-Humanoid-Robotics-Course/docs/module-4-vla/week-13-vla-concepts/">Module 4: Vision-Language-Action (VLA)</a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/AqsaIftikhar15/Textbook-for-Teaching-Physical-AI-Humanoid-Robotics-Course/" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Physical AI & Humanoid Robotics Book. Built with Docusaurus.</div></div></div></footer><div class="chatWidget_KrGq"><button class="toggleBtn_o_Si" aria-label="Toggle chat widget">🤖 <!-- -->Ask AI</button></div></div>
</body>
</html>