"use strict";(globalThis.webpackChunkphysical_ai_robotics_book=globalThis.webpackChunkphysical_ai_robotics_book||[]).push([[3904],{4647:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>_,frontMatter:()=>r,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"module-3-ai-brain/week-8-10-isaac-platform","title":"NVIDIA Isaac Platform","description":"AI-powered perception and manipulation with Isaac SDK","source":"@site/docs/module-3-ai-brain/week-8-10-isaac-platform.md","sourceDirName":"module-3-ai-brain","slug":"/module-3-ai-brain/week-8-10-isaac-platform","permalink":"/Textbook-for-Teaching-Physical-AI-Humanoid-Robotics-Course/docs/module-3-ai-brain/week-8-10-isaac-platform","draft":false,"unlisted":false,"editUrl":"https://github.com/AqsaIftikhar15/Textbook-for-Teaching-Physical-AI-Humanoid-Robotics-Course/docs/module-3-ai-brain/week-8-10-isaac-platform.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"NVIDIA Isaac Platform","sidebar_position":1,"description":"AI-powered perception and manipulation with Isaac SDK"},"sidebar":"tutorialSidebar","previous":{"title":"Simulation Concepts","permalink":"/Textbook-for-Teaching-Physical-AI-Humanoid-Robotics-Course/docs/module-2-digital-twin/simulation-concepts"},"next":{"title":"Humanoid Robot Development","permalink":"/Textbook-for-Teaching-Physical-AI-Humanoid-Robotics-Course/docs/module-3-ai-brain/week-11-12-humanoid-dev"}}');var a=t(4848),o=t(8453);const r={title:"NVIDIA Isaac Platform",sidebar_position:1,description:"AI-powered perception and manipulation with Isaac SDK"},s="Week 8: NVIDIA Isaac Platform Introduction",l={},c=[{value:"Introduction",id:"introduction",level:2},{value:"NVIDIA Isaac Platform Architecture",id:"nvidia-isaac-platform-architecture",level:2},{value:"Isaac Platform Components",id:"isaac-platform-components",level:3},{value:"GPU Acceleration Framework",id:"gpu-acceleration-framework",level:3},{value:"Isaac ROS: GPU-Accelerated ROS 2 Packages",id:"isaac-ros-gpu-accelerated-ros-2-packages",level:2},{value:"Isaac ROS Package Ecosystem",id:"isaac-ros-package-ecosystem",level:3},{value:"GPU-Accelerated Perception Pipeline",id:"gpu-accelerated-perception-pipeline",level:3},{value:"Isaac Sim: Physics Simulation and Synthetic Data Generation",id:"isaac-sim-physics-simulation-and-synthetic-data-generation",level:2},{value:"Simulation Environment Architecture",id:"simulation-environment-architecture",level:3},{value:"Physics Simulation Capabilities",id:"physics-simulation-capabilities",level:3},{value:"Isaac Lab: Reinforcement Learning Framework",id:"isaac-lab-reinforcement-learning-framework",level:2},{value:"Reinforcement Learning Environment",id:"reinforcement-learning-environment",level:3},{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Introduction",id:"introduction-1",level:2},{value:"Computer Vision for Robotics",id:"computer-vision-for-robotics",level:2},{value:"Deep Learning-Based Object Detection",id:"deep-learning-based-object-detection",level:3},{value:"Semantic Segmentation",id:"semantic-segmentation",level:3},{value:"3D Perception and Depth Estimation",id:"3d-perception-and-depth-estimation",level:3},{value:"Sensor Fusion and State Estimation",id:"sensor-fusion-and-state-estimation",level:2},{value:"Multi-Sensor Data Integration",id:"multi-sensor-data-integration",level:3},{value:"Visual-Inertial Odometry (VIO)",id:"visual-inertial-odometry-vio",level:3},{value:"Intelligent Control Systems",id:"intelligent-control-systems",level:2},{value:"Model Predictive Control (MPC) with Learning",id:"model-predictive-control-mpc-with-learning",level:3},{value:"Adaptive Control with Neural Networks",id:"adaptive-control-with-neural-networks",level:3},{value:"Learning and Adaptation",id:"learning-and-adaptation",level:2},{value:"Online Learning Systems",id:"online-learning-systems",level:3},{value:"Learning Outcomes",id:"learning-outcomes-1",level:2},{value:"Introduction",id:"introduction-2",level:2},{value:"Foundations of Reinforcement Learning for Robotics",id:"foundations-of-reinforcement-learning-for-robotics",level:2},{value:"RL Framework for Robotics",id:"rl-framework-for-robotics",level:3},{value:"Reward Function Design",id:"reward-function-design",level:3},{value:"Deep Reinforcement Learning Algorithms",id:"deep-reinforcement-learning-algorithms",level:2},{value:"Deep Deterministic Policy Gradient (DDPG)",id:"deep-deterministic-policy-gradient-ddpg",level:3},{value:"Twin Delayed DDPG (TD3)",id:"twin-delayed-ddpg-td3",level:3},{value:"Soft Actor-Critic (SAC)",id:"soft-actor-critic-sac",level:3},{value:"Simulation-to-Real Transfer",id:"simulation-to-real-transfer",level:2},{value:"Domain Randomization",id:"domain-randomization",level:3},{value:"System Identification and Model Adaptation",id:"system-identification-and-model-adaptation",level:3},{value:"Learning Outcomes",id:"learning-outcomes-2",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"week-8-nvidia-isaac-platform-introduction",children:"Week 8: NVIDIA Isaac Platform Introduction"})}),"\n",(0,a.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,a.jsx)(n.p,{children:"The NVIDIA Isaac platform represents a comprehensive ecosystem for developing AI-powered robotic applications, specifically designed to leverage GPU acceleration for perception, navigation, and manipulation tasks. This week introduces the core components of the Isaac platform, including Isaac ROS, Isaac Sim, and Isaac Lab, and explores how these tools enable the development of sophisticated humanoid robots capable of intelligent behavior in complex environments."}),"\n",(0,a.jsx)(n.h2,{id:"nvidia-isaac-platform-architecture",children:"NVIDIA Isaac Platform Architecture"}),"\n",(0,a.jsx)(n.p,{children:"The Isaac platform provides a unified framework that bridges simulation, perception, and real-world deployment through a suite of interconnected tools and libraries."}),"\n",(0,a.jsx)(n.h3,{id:"isaac-platform-components",children:"Isaac Platform Components"}),"\n",(0,a.jsx)(n.p,{children:"The Isaac ecosystem consists of several key components:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Isaac ROS"}),": GPU-accelerated ROS 2 packages for perception and navigation"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Isaac Sim"}),": High-fidelity physics simulation and synthetic data generation"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Isaac Lab"}),": Reinforcement learning and imitation learning framework"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Isaac Apps"}),": Reference applications and demonstrations"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Isaac Core"}),": Foundational libraries and tools"]}),"\n"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"Pseudocode: Isaac Platform Architecture\nclass IsaacPlatform:\n    def __init__(self):\n        self.isaac_ros = IsaacROS()\n        self.isaac_sim = IsaacSim()\n        self.isaac_lab = IsaacLab()\n        self.isaac_apps = IsaacApps()\n        self.isaac_core = IsaacCore()\n\n        self.hardware_abstraction = HardwareAbstractionLayer()\n        self.gpu_manager = GPUResourceManager()\n        self.data_pipeline = DataPipeline()\n\n    def initialize_platform(self):\n        # Initialize core components\n        self.isaac_core.initialize()\n\n        # Configure GPU resources\n        self.gpu_manager.initialize_gpus()\n        self.gpu_manager.allocate_memory_pools()\n\n        # Set up data pipeline\n        self.data_pipeline.setup_message_passing()\n        self.data_pipeline.configure_compression()\n\n        # Initialize simulation environment\n        self.isaac_sim.initialize()\n\n        # Initialize ROS integration\n        self.isaac_ros.initialize()\n\n    def create_robot_application(self, robot_description):\n        # Create robot-specific application\n        robot_app = RobotApplication(robot_description)\n\n        # Configure perception pipeline\n        perception_pipeline = self.isaac_ros.create_perception_pipeline()\n        perception_pipeline.add_gpu_nodes([\n            'isaac_ros_detectnet',      # Object detection\n            'isaac_ros_pointcloud',     # 3D point cloud processing\n            'isaac_ros_image_proc'      # Image processing\n        ])\n\n        # Configure navigation pipeline\n        navigation_pipeline = self.isaac_ros.create_navigation_pipeline()\n        navigation_pipeline.add_gpu_nodes([\n            'isaac_ros_costmap_2d',     # 2D costmap generation\n            'isaac_ros_planner',        # Path planning\n            'isaac_ros_controller'      # Motion control\n        ])\n\n        robot_app.set_perception_pipeline(perception_pipeline)\n        robot_app.set_navigation_pipeline(navigation_pipeline)\n\n        return robot_app\n"})}),"\n",(0,a.jsx)(n.h3,{id:"gpu-acceleration-framework",children:"GPU Acceleration Framework"}),"\n",(0,a.jsx)(n.p,{children:"Isaac leverages NVIDIA's GPU ecosystem for accelerated computation:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"Pseudocode: GPU Acceleration System\nclass GPUAcceleration:\n    def __init__(self):\n        self.gpu_devices = self.detect_gpu_devices()\n        self.memory_manager = GPUMemoryManager()\n        self.compute_contexts = {}\n        self.tensor_cores = TensorCoreManager()\n\n    def setup_compute_context(self, device_id):\n        # Create CUDA context for specific GPU\n        context = cuda.Context(device_id)\n        self.compute_contexts[device_id] = context\n\n        # Allocate memory pools\n        self.memory_manager.allocate_pinned_memory(device_id, size_gb=4)\n        self.memory_manager.allocate_unified_memory(device_id, size_gb=8)\n\n        # Initialize Tensor Core operations for mixed precision\n        self.tensor_cores.enable_mixed_precision(device_id)\n\n        return context\n\n    def accelerate_perception_task(self, task_type, input_data):\n        # Select appropriate GPU based on task requirements\n        gpu_id = self.select_optimal_gpu(task_type)\n        context = self.compute_contexts[gpu_id]\n\n        with context:\n            # Transfer data to GPU memory\n            gpu_input = self.memory_manager.copy_to_gpu(input_data, gpu_id)\n\n            if task_type == 'object_detection':\n                # Use TensorRT for optimized inference\n                result = self.tensorrt_inference(\n                    model='detectnet_model',\n                    input=gpu_input,\n                    precision='fp16'  # Mixed precision\n                )\n\n            elif task_type == 'pointcloud_processing':\n                # Use CUDA kernels for point cloud operations\n                result = self.cuda_pointcloud_process(gpu_input)\n\n            elif task_type == 'image_processing':\n                # Use GPU-accelerated image processing\n                result = self.gpu_image_process(gpu_input)\n\n            # Transfer result back to CPU memory\n            cpu_result = self.memory_manager.copy_to_cpu(result, gpu_id)\n\n        return cpu_result\n\n    def optimize_memory_usage(self):\n        # Implement memory optimization strategies\n        self.memory_manager.enable_memory_pooling()\n        self.memory_manager.setup_memory_caching()\n        self.memory_manager.configure_streaming()\n"})}),"\n",(0,a.jsx)(n.h2,{id:"isaac-ros-gpu-accelerated-ros-2-packages",children:"Isaac ROS: GPU-Accelerated ROS 2 Packages"}),"\n",(0,a.jsx)(n.p,{children:"Isaac ROS provides GPU-accelerated implementations of common ROS 2 packages, significantly improving performance for perception and navigation tasks."}),"\n",(0,a.jsx)(n.h3,{id:"isaac-ros-package-ecosystem",children:"Isaac ROS Package Ecosystem"}),"\n",(0,a.jsx)(n.p,{children:"Key Isaac ROS packages include:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"isaac_ros_detectnet"}),": Object detection using NVIDIA's DetectNet"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"isaac_ros_pointcloud"}),": GPU-accelerated point cloud processing"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"isaac_ros_image_proc"}),": High-performance image processing"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"isaac_ros_visual_slam"}),": Visual SLAM with GPU acceleration"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"isaac_ros_pose_estimation"}),": Real-time pose estimation"]}),"\n"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"Pseudocode: Isaac ROS Node Implementation\nclass IsaacROSNode:\n    def __init__(self, node_name):\n        self.node = rclpy.create_node(node_name)\n        self.gpu_pipeline = GPUPipeline()\n        self.tensorrt_engine = None\n        self.input_queue = queue.Queue(maxsize=10)\n        self.output_queue = queue.Queue(maxsize=10)\n\n    def initialize_gpu_pipeline(self, model_path, input_shape):\n        # Load TensorRT engine for optimized inference\n        self.tensorrt_engine = self.load_tensorrt_engine(model_path)\n\n        # Configure GPU memory allocation\n        self.gpu_pipeline.allocate_tensors(\n            input_shape=input_shape,\n            output_shape=self.get_output_shape(model_path)\n        )\n\n        # Set up CUDA streams for asynchronous processing\n        self.gpu_pipeline.setup_cuda_streams(num_streams=2)\n\n    def process_gpu_inference(self, input_data):\n        # Asynchronous GPU inference\n        with self.gpu_pipeline.get_stream() as stream:\n            # Copy input to GPU memory\n            gpu_input = self.gpu_pipeline.copy_input(input_data, stream)\n\n            # Execute TensorRT inference\n            gpu_output = self.tensorrt_engine.execute_async(\n                bindings=[gpu_input, self.gpu_pipeline.get_output_buffer()],\n                stream_handle=stream.cuda_stream\n            )\n\n            # Copy result back to CPU\n            output_result = self.gpu_pipeline.copy_output(gpu_output, stream)\n\n        return output_result\n\n    def create_isaac_ros_publisher(self, msg_type, topic_name):\n        # Create publisher with GPU-optimized message handling\n        publisher = self.node.create_publisher(msg_type, topic_name, 10)\n\n        # Optimize for GPU-accelerated data\n        publisher.set_qos_profile(\n            rclpy.qos.QoSProfile(\n                reliability=rclpy.qos.ReliabilityPolicy.RELIABLE,\n                durability=rclpy.qos.DurabilityPolicy.VOLATILE,\n                history=rclpy.qos.HistoryPolicy.KEEP_LAST,\n                depth=1  # Minimize memory usage for high-frequency data\n            )\n        )\n\n        return publisher\n"})}),"\n",(0,a.jsx)(n.h3,{id:"gpu-accelerated-perception-pipeline",children:"GPU-Accelerated Perception Pipeline"}),"\n",(0,a.jsx)(n.p,{children:"Building an optimized perception pipeline using Isaac ROS:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"Pseudocode: Perception Pipeline\nclass PerceptionPipeline:\n    def __init__(self):\n        self.image_subscriber = None\n        self.detection_publisher = None\n        self.pointcloud_subscriber = None\n        self.gpu_pipeline = GPUPipeline()\n        self.synchronization = MessageSynchronizer()\n\n    def setup_gpu_perception_nodes(self):\n        # Initialize Isaac ROS nodes with GPU acceleration\n        self.image_subscriber = self.node.create_subscription(\n            sensor_msgs.msg.Image,\n            '/camera/image_raw',\n            self.image_callback,\n            10\n        )\n\n        # Object detection node\n        self.detection_node = self.create_gpu_detection_node()\n\n        # Depth processing node\n        self.depth_node = self.create_gpu_depth_node()\n\n        # Point cloud fusion node\n        self.fusion_node = self.create_gpu_fusion_node()\n\n    def image_callback(self, image_msg):\n        # Process image using GPU acceleration\n        image_tensor = self.convert_image_to_tensor(image_msg)\n\n        # Run object detection on GPU\n        detections = self.detection_node.run_inference(image_tensor)\n\n        # Process depth information\n        depth_tensor = self.get_depth_tensor(image_msg.header.stamp)\n        depth_processed = self.depth_node.process(depth_tensor)\n\n        # Fuse detection and depth data\n        fused_result = self.fusion_node.fuse_data(detections, depth_processed)\n\n        # Publish results\n        self.publish_detection_results(fused_result, image_msg.header)\n\n    def create_gpu_detection_node(self):\n        # Create GPU-optimized detection pipeline\n        detection_node = IsaacDetectionNode()\n\n        # Load optimized model\n        detection_node.load_model(\n            model_path='detectnet_model.plan',  # TensorRT optimized\n            input_shape=(3, 512, 512),\n            output_shape=(1, 100, 7)  # batch, detections, (class, conf, x, y, w, h, angle)\n        )\n\n        # Configure GPU memory\n        detection_node.configure_gpu_memory(\n            input_buffer_size=512*512*3*4,  # 4 bytes per float\n            output_buffer_size=100*7*4,\n            max_batch_size=8\n        )\n\n        return detection_node\n"})}),"\n",(0,a.jsx)(n.h2,{id:"isaac-sim-physics-simulation-and-synthetic-data-generation",children:"Isaac Sim: Physics Simulation and Synthetic Data Generation"}),"\n",(0,a.jsx)(n.p,{children:"Isaac Sim provides high-fidelity physics simulation and synthetic data generation capabilities essential for training AI models without real-world data collection."}),"\n",(0,a.jsx)(n.h3,{id:"simulation-environment-architecture",children:"Simulation Environment Architecture"}),"\n",(0,a.jsx)(n.p,{children:"Isaac Sim builds upon Omniverse technology to provide realistic simulation:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"Pseudocode: Isaac Sim Environment\nclass IsaacSimEnvironment:\n    def __init__(self):\n        self.sim_app = omni.kit.app.get_app()\n        self.physics_context = PhysicsContext()\n        self.renderer = KitRenderer()\n        self.synthetic_data_generator = SyntheticDataGenerator()\n        self.domain_randomization = DomainRandomization()\n\n    def create_simulation_world(self, world_config):\n        # Create physics scene\n        self.physics_context.create_simulation()\n\n        # Set up physics properties\n        self.physics_context.set_gravity(world_config.gravity)\n        self.physics_context.set_timestep(world_config.timestep)\n        self.physics_context.set_solver_iterations(world_config.solver_iterations)\n\n        # Create ground plane\n        ground_plane = self.create_ground_plane(\n            size=world_config.ground_size,\n            friction=world_config.ground_friction,\n            restitution=world_config.ground_restitution\n        )\n\n        # Create lighting environment\n        self.setup_environment_lighting(world_config.lighting_config)\n\n        # Create dynamic objects\n        for obj_config in world_config.objects:\n            self.create_dynamic_object(obj_config)\n\n    def setup_synthetic_data_generation(self):\n        # Configure synthetic data generation pipeline\n        self.synthetic_data_generator.set_camera_config(\n            resolution=(1920, 1080),\n            fov=60.0,\n            sensor_noise={'mean': 0.0, 'std': 0.001}\n        )\n\n        # Set up annotation generation\n        self.synthetic_data_generator.enable_annotations([\n            'bounding_box_2d_tight',\n            'instance_segmentation',\n            'depth',\n            'normals',\n            'motion_vectors'\n        ])\n\n        # Configure domain randomization\n        self.domain_randomization.set_randomization_ranges({\n            'lighting': {'intensity_range': (0.5, 2.0)},\n            'textures': {'roughness_range': (0.1, 0.9)},\n            'materials': {'color_range': ('red', 'blue', 'green')}\n        })\n\n    def generate_synthetic_dataset(self, num_samples, scenario_configs):\n        dataset = []\n\n        for i in range(num_samples):\n            # Apply domain randomization\n            self.domain_randomization.randomize_scene()\n\n            # Execute scenario\n            scenario_config = random.choice(scenario_configs)\n            self.execute_scenario(scenario_config)\n\n            # Capture synthetic data\n            frame_data = self.synthetic_data_generator.capture_frame()\n            annotations = self.synthetic_data_generator.generate_annotations()\n\n            sample = {\n                'image': frame_data['rgb'],\n                'depth': frame_data['depth'],\n                'annotations': annotations,\n                'scenario': scenario_config,\n                'randomization_params': self.domain_randomization.get_current_params()\n            }\n\n            dataset.append(sample)\n\n        return dataset\n"})}),"\n",(0,a.jsx)(n.h3,{id:"physics-simulation-capabilities",children:"Physics Simulation Capabilities"}),"\n",(0,a.jsx)(n.p,{children:"Advanced physics simulation for humanoid robot development:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"Pseudocode: Advanced Physics Simulation\nclass AdvancedPhysicsSimulation:\n    def __init__(self):\n        self.material_properties = MaterialProperties()\n        self.contact_properties = ContactProperties()\n        self.deformable_bodies = DeformableBodySystem()\n        self.fluid_simulation = FluidSimulation()\n\n    def setup_material_properties(self):\n        # Configure realistic material properties\n        self.material_properties.set_defaults({\n            'robot_metal': {\n                'density': 7800,  # kg/m^3\n                'youngs_modulus': 200e9,  # Pa\n                'poissons_ratio': 0.3,\n                'static_friction': 0.5,\n                'dynamic_friction': 0.4,\n                'restitution': 0.2\n            },\n            'rubber_foot': {\n                'density': 1200,\n                'youngs_modulus': 1e6,\n                'poissons_ratio': 0.49,\n                'static_friction': 0.8,\n                'dynamic_friction': 0.7,\n                'restitution': 0.1\n            }\n        })\n\n    def simulate_robot_environment_interaction(self, robot, environment):\n        # Simulate complex interactions between robot and environment\n        for link in robot.links:\n            for contact_point in self.detect_contacts(link, environment):\n                # Calculate contact forces\n                contact_force = self.calculate_contact_force(\n                    contact_point,\n                    link.material,\n                    environment.material\n                )\n\n                # Apply forces to robot dynamics\n                link.apply_force(contact_force, contact_point.position)\n\n                # Handle friction and sliding\n                if self.is_sliding(contact_point):\n                    friction_force = self.calculate_friction_force(contact_point)\n                    link.apply_force(friction_force, contact_point.position)\n\n    def simulate_deformable_contacts(self, robot, soft_objects):\n        # Handle interactions with deformable objects\n        for soft_obj in soft_objects:\n            deformation = self.calculate_deformation(robot, soft_obj)\n            soft_obj.apply_deformation(deformation)\n\n            # Update collision geometry based on deformation\n            soft_obj.update_collision_mesh(deformation)\n"})}),"\n",(0,a.jsx)(n.h2,{id:"isaac-lab-reinforcement-learning-framework",children:"Isaac Lab: Reinforcement Learning Framework"}),"\n",(0,a.jsx)(n.p,{children:"Isaac Lab provides a comprehensive framework for training robotic policies using reinforcement learning and imitation learning."}),"\n",(0,a.jsx)(n.h3,{id:"reinforcement-learning-environment",children:"Reinforcement Learning Environment"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"Pseudocode: Isaac Lab RL Environment\nclass IsaacLabEnvironment:\n    def __init__(self, task_config):\n        self.task_config = task_config\n        self.simulation = IsaacSimEnvironment()\n        self.observation_space = self.define_observation_space()\n        self.action_space = self.define_action_space()\n        self.reward_function = self.define_reward_function()\n        self.termination_conditions = self.define_termination_conditions()\n\n    def define_observation_space(self):\n        # Define observation space for RL agent\n        observation_spec = {\n            'joint_positions': {\n                'shape': (self.robot.num_joints,),\n                'dtype': np.float32,\n                'low': -np.pi,\n                'high': np.pi\n            },\n            'joint_velocities': {\n                'shape': (self.robot.num_joints,),\n                'dtype': np.float32,\n                'low': -10.0,\n                'high': 10.0\n            },\n            'base_pose': {\n                'shape': (7,),  # [x, y, z, qw, qx, qy, qz]\n                'dtype': np.float32,\n                'low': [-np.inf, -np.inf, 0, -1, -1, -1, -1],\n                'high': [np.inf, np.inf, np.inf, 1, 1, 1, 1]\n            },\n            'sensor_data': {\n                'shape': (self.sensor_size,),\n                'dtype': np.float32,\n                'low': -np.inf,\n                'high': np.inf\n            }\n        }\n        return observation_spec\n\n    def define_action_space(self):\n        # Define action space for humanoid robot\n        action_spec = {\n            'joint_commands': {\n                'shape': (self.robot.num_joints,),\n                'dtype': np.float32,\n                'low': -np.pi,\n                'high': np.pi\n            },\n            'gains': {\n                'shape': (self.robot.num_joints * 2,),  # [position_gains, velocity_gains]\n                'dtype': np.float32,\n                'low': 0.0,\n                'high': 1000.0\n            }\n        }\n        return action_spec\n\n    def define_reward_function(self):\n        # Define reward function for humanoid locomotion\n        def reward_function(state, action, next_state, info):\n            reward = 0.0\n\n            # Progress reward - move forward\n            progress = next_state['base_pose'][0] - state['base_pose'][0]\n            reward += progress * 10.0\n\n            # Velocity reward - maintain target speed\n            current_velocity = self.calculate_base_velocity(next_state)\n            target_velocity = self.task_config.target_velocity\n            velocity_reward = -abs(current_velocity - target_velocity) * 5.0\n            reward += velocity_reward\n\n            # Balance reward - maintain upright posture\n            base_orientation = next_state['base_pose'][3:7]  # quaternion\n            upright_reward = self.calculate_upright_reward(base_orientation) * 2.0\n            reward += upright_reward\n\n            # Smoothness reward - penalize jerky movements\n            action_smoothness = -np.sum(np.abs(action)) * 0.1\n            reward += action_smoothness\n\n            return reward\n\n        return reward_function\n\n    def reset(self):\n        # Reset environment to initial state\n        self.simulation.reset()\n\n        # Apply randomization if enabled\n        if self.task_config.randomize:\n            self.randomize_environment()\n\n        # Get initial observation\n        initial_state = self.get_current_state()\n        return self.format_observation(initial_state)\n\n    def step(self, action):\n        # Execute one step of the environment\n        previous_state = self.get_current_state()\n\n        # Apply action to robot\n        self.apply_action_to_robot(action)\n\n        # Step simulation\n        self.simulation.step()\n\n        # Get new state\n        current_state = self.get_current_state()\n\n        # Calculate reward\n        reward = self.reward_function(previous_state, action, current_state, {})\n\n        # Check termination conditions\n        terminated = self.check_termination(current_state)\n        truncated = self.check_truncation(current_state)\n\n        # Get observation\n        observation = self.format_observation(current_state)\n\n        # Get info dictionary\n        info = self.get_info_dict(current_state, reward)\n\n        return observation, reward, terminated, truncated, info\n"})}),"\n",(0,a.jsx)(n.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,a.jsx)(n.p,{children:"By the end of this week, students should be able to:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Understand Isaac platform architecture"})," - Explain the components and capabilities of the NVIDIA Isaac ecosystem for AI-powered robotics."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Implement GPU-accelerated perception"})," - Create perception pipelines using Isaac ROS packages that leverage GPU acceleration for real-time performance."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Configure Isaac Sim environments"})," - Set up high-fidelity simulation environments with appropriate physics properties and synthetic data generation capabilities."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Design reinforcement learning environments"})," - Create RL environments in Isaac Lab with appropriate observation spaces, action spaces, and reward functions for humanoid robot tasks."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Integrate Isaac components"})," - Connect simulation, perception, and control systems using the Isaac platform for comprehensive humanoid robot development."]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h1,{id:"week-9-ai-perception-and-control",children:"Week 9: AI Perception and Control"}),"\n",(0,a.jsx)(n.h2,{id:"introduction-1",children:"Introduction"}),"\n",(0,a.jsx)(n.p,{children:"AI perception and control form the cognitive core of humanoid robots, enabling them to understand their environment, make intelligent decisions, and execute complex behaviors. This week explores the integration of artificial intelligence techniques with robotic perception and control systems, focusing on computer vision, sensor fusion, and intelligent control strategies that enable humanoid robots to operate autonomously in complex environments."}),"\n",(0,a.jsx)(n.h2,{id:"computer-vision-for-robotics",children:"Computer Vision for Robotics"}),"\n",(0,a.jsx)(n.p,{children:"Computer vision provides humanoid robots with the ability to interpret visual information from cameras and other optical sensors, enabling navigation, object recognition, and interaction with the environment."}),"\n",(0,a.jsx)(n.h3,{id:"deep-learning-based-object-detection",children:"Deep Learning-Based Object Detection"}),"\n",(0,a.jsx)(n.p,{children:"Modern object detection systems use deep neural networks to identify and locate objects in images:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"Pseudocode: Object Detection System\nclass ObjectDetectionSystem:\n    def __init__(self):\n        self.detection_model = self.load_pretrained_model('yolov8')\n        self.feature_extractor = FeatureExtractor()\n        self.tracker = ObjectTracker()\n        self.classifier = ObjectClassifier()\n\n    def detect_objects(self, image):\n        # Preprocess image for neural network\n        processed_image = self.preprocess_image(image)\n\n        # Run object detection\n        detections = self.detection_model.inference(processed_image)\n\n        # Filter detections by confidence threshold\n        confident_detections = [\n            det for det in detections\n            if det.confidence > self.confidence_threshold\n        ]\n\n        # Extract features for tracking\n        for detection in confident_detections:\n            detection.features = self.feature_extractor.extract(\n                image, detection.bbox\n            )\n\n        # Update object tracker\n        tracked_objects = self.tracker.update(confident_detections)\n\n        return tracked_objects\n\n    def preprocess_image(self, image):\n        # Resize and normalize image\n        resized = cv2.resize(image, (640, 640))\n        normalized = resized.astype(np.float32) / 255.0\n        normalized = np.transpose(normalized, (2, 0, 1))  # HWC to CHW\n        return normalized\n\n    def load_pretrained_model(self, model_name):\n        # Load pre-trained model with GPU acceleration\n        if model_name == 'yolov8':\n            model = YOLOv8Model()\n            model.load_weights('yolov8_weights.pt')\n            model.to_gpu()\n            return model\n        elif model_name == 'detectnet':\n            return DetectNetModel()\n"})}),"\n",(0,a.jsx)(n.h3,{id:"semantic-segmentation",children:"Semantic Segmentation"}),"\n",(0,a.jsx)(n.p,{children:"Semantic segmentation provides pixel-level understanding of the environment:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"Pseudocode: Semantic Segmentation System\nclass SemanticSegmentation:\n    def __init__(self):\n        self.segmentation_model = SegmentationModel('deeplabv3')\n        self.color_map = self.create_color_map()\n        self.post_processor = SegmentationPostProcessor()\n\n    def segment_image(self, image):\n        # Run segmentation inference\n        raw_output = self.segmentation_model.inference(image)\n\n        # Apply softmax to get class probabilities\n        probabilities = self.softmax(raw_output)\n\n        # Get predicted class for each pixel\n        predicted_classes = np.argmax(probabilities, axis=0)\n\n        # Apply post-processing to refine boundaries\n        refined_mask = self.post_processor.refine_boundaries(\n            predicted_classes, image\n        )\n\n        # Convert to colored segmentation map\n        colored_map = self.colorize_segmentation(refined_mask)\n\n        return {\n            'segmentation_map': refined_mask,\n            'colored_map': colored_map,\n            'class_probabilities': probabilities\n        }\n\n    def colorize_segmentation(self, segmentation_map):\n        # Map class indices to RGB colors\n        height, width = segmentation_map.shape\n        colored_map = np.zeros((height, width, 3), dtype=np.uint8)\n\n        for class_idx in np.unique(segmentation_map):\n            mask = segmentation_map == class_idx\n            colored_map[mask] = self.color_map[class_idx]\n\n        return colored_map\n\n    def extract_traversable_regions(self, segmentation_result):\n        # Identify walkable areas from segmentation\n        walkable_classes = ['floor', 'grass', 'carpet', 'road']\n        walkable_mask = np.zeros_like(segmentation_result['segmentation_map'])\n\n        for class_name in walkable_classes:\n            class_idx = self.get_class_index(class_name)\n            walkable_mask[segmentation_result['segmentation_map'] == class_idx] = 1\n\n        return walkable_mask\n"})}),"\n",(0,a.jsx)(n.h3,{id:"3d-perception-and-depth-estimation",children:"3D Perception and Depth Estimation"}),"\n",(0,a.jsx)(n.p,{children:"Understanding 3D structure is crucial for humanoid robot navigation and manipulation:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"Pseudocode: 3D Perception System\nclass ThreeDPerception:\n    def __init__(self):\n        self.depth_estimator = DepthEstimator('monodepth2')\n        self.pointcloud_generator = PointCloudGenerator()\n        self.surface_analyzer = SurfaceAnalyzer()\n        self.obstacle_detector = ObstacleDetector()\n\n    def process_3d_data(self, stereo_images=None, depth_image=None, rgb_image=None):\n        if depth_image is not None:\n            # Use provided depth image\n            depth_map = depth_image\n        elif stereo_images is not None:\n            # Estimate depth from stereo images\n            depth_map = self.depth_estimator.from_stereo(stereo_images)\n        elif rgb_image is not None:\n            # Estimate depth from single RGB image\n            depth_map = self.depth_estimator.from_monocular(rgb_image)\n\n        # Generate point cloud\n        pointcloud = self.pointcloud_generator.from_depth(\n            depth_map, self.camera_intrinsics\n        )\n\n        # Analyze surfaces for navigation\n        surfaces = self.surface_analyzer.analyze(pointcloud)\n\n        # Detect obstacles\n        obstacles = self.obstacle_detector.detect(pointcloud, surfaces)\n\n        return {\n            'depth_map': depth_map,\n            'pointcloud': pointcloud,\n            'surfaces': surfaces,\n            'obstacles': obstacles\n        }\n\n    def generate_occupancy_grid(self, pointcloud, resolution=0.1):\n        # Create 2D occupancy grid from 3D point cloud\n        min_x, min_y = np.min(pointcloud[:, :2], axis=0)\n        max_x, max_y = np.max(pointcloud[:, :2], axis=0)\n\n        grid_width = int((max_x - min_x) / resolution)\n        grid_height = int((max_y - min_y) / resolution)\n\n        occupancy_grid = np.zeros((grid_width, grid_height), dtype=np.float32)\n\n        for point in pointcloud:\n            x_idx = int((point[0] - min_x) / resolution)\n            y_idx = int((point[1] - min_y) / resolution)\n\n            if 0 <= x_idx < grid_width and 0 <= y_idx < grid_height:\n                # Mark as occupied if point is below robot height threshold\n                if point[2] < self.robot_height_threshold:\n                    occupancy_grid[x_idx, y_idx] = 1.0\n\n        return occupancy_grid\n"})}),"\n",(0,a.jsx)(n.h2,{id:"sensor-fusion-and-state-estimation",children:"Sensor Fusion and State Estimation"}),"\n",(0,a.jsx)(n.p,{children:"Integrating data from multiple sensors provides a more complete and accurate understanding of the robot's state and environment."}),"\n",(0,a.jsx)(n.h3,{id:"multi-sensor-data-integration",children:"Multi-Sensor Data Integration"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"Pseudocode: Sensor Fusion System\nclass SensorFusion:\n    def __init__(self):\n        self.kalman_filter = ExtendedKalmanFilter()\n        self.particle_filter = ParticleFilter()\n        self.imu_processor = IMUProcessor()\n        self.odometry_processor = OdometryProcessor()\n        self.vision_processor = VisionProcessor()\n\n    def initialize_state_estimator(self, initial_pose, initial_covariance):\n        # Initialize Kalman filter state\n        self.kalman_filter.initialize(\n            state=initial_pose,\n            covariance=initial_covariance\n        )\n\n        # Initialize particle filter for multimodal distributions\n        self.particle_filter.initialize(\n            num_particles=1000,\n            initial_distribution=initial_pose\n        )\n\n    def fuse_sensor_data(self, imu_data, odometry_data, vision_data, dt):\n        # Process IMU data (high frequency, relative measurements)\n        imu_prediction = self.imu_processor.process(imu_data, dt)\n\n        # Process odometry data (medium frequency, relative measurements)\n        odometry_correction = self.odometry_processor.process(odometry_data)\n\n        # Process vision data (low frequency, absolute measurements)\n        vision_correction = self.vision_processor.process(vision_data)\n\n        # Fuse all sensor data using Kalman filter\n        prediction = self.kalman_filter.predict(imu_prediction, dt)\n        corrected_state = self.kalman_filter.update(\n            prediction,\n            [odometry_correction, vision_correction]\n        )\n\n        # Update particle filter for non-linear estimation\n        self.particle_filter.update(\n            odometry_correction,\n            vision_correction,\n            imu_prediction\n        )\n\n        return {\n            'estimated_pose': corrected_state,\n            'uncertainty': self.kalman_filter.covariance,\n            'particles': self.particle_filter.particles\n        }\n\n    def handle_sensor_failures(self, sensor_data):\n        # Implement sensor failure detection and graceful degradation\n        for sensor_name, data in sensor_data.items():\n            if not self.is_sensor_valid(data):\n                # Switch to alternative sensor or prediction-only mode\n                self.enable_sensor_backup(sensor_name)\n                self.log_sensor_failure(sensor_name)\n"})}),"\n",(0,a.jsx)(n.h3,{id:"visual-inertial-odometry-vio",children:"Visual-Inertial Odometry (VIO)"}),"\n",(0,a.jsx)(n.p,{children:"Combining visual and inertial measurements for robust pose estimation:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"Pseudocode: Visual-Inertial Odometry\nclass VisualInertialOdometry:\n    def __init__(self):\n        self.feature_detector = FeatureDetector('orb')\n        self.feature_tracker = FeatureTracker()\n        self.imu_integrator = IMUIntegrator()\n        self.optimization_backend = OptimizationBackend()\n        self.keyframe_manager = KeyframeManager()\n\n    def estimate_pose(self, current_image, imu_measurements, previous_pose):\n        # Track features from previous frame\n        tracked_features = self.feature_tracker.track(\n            previous_image=self.previous_image,\n            current_image=current_image\n        )\n\n        # Integrate IMU measurements for prediction\n        imu_prediction = self.imu_integrator.integrate(\n            imu_measurements, self.dt\n        )\n\n        # Estimate pose from feature correspondences\n        visual_pose_estimate = self.estimate_pose_from_features(\n            tracked_features, self.camera_intrinsics\n        )\n\n        # Fuse visual and inertial estimates\n        fused_pose = self.fuse_visual_inertial(\n            visual_estimate=visual_pose_estimate,\n            inertial_estimate=imu_prediction,\n            previous_pose=previous_pose\n        )\n\n        # Optimize pose graph for consistency\n        optimized_pose = self.optimize_pose_graph(fused_pose)\n\n        # Manage keyframes for map building\n        if self.should_add_keyframe(optimized_pose):\n            self.keyframe_manager.add_keyframe(\n                image=current_image,\n                pose=optimized_pose\n            )\n\n        self.previous_image = current_image\n        return optimized_pose\n\n    def estimate_pose_from_features(self, features, intrinsics):\n        # Use PnP (Perspective-n-Point) algorithm\n        object_points = self.get_3d_points(features)\n        image_points = self.get_2d_points(features)\n\n        success, rvec, tvec = cv2.solvePnP(\n            object_points, image_points, intrinsics, None\n        )\n\n        if success:\n            # Convert rotation vector to rotation matrix\n            rotation_matrix, _ = cv2.Rodrigues(rvec)\n\n            # Create transformation matrix\n            transform = np.eye(4)\n            transform[:3, :3] = rotation_matrix\n            transform[:3, 3] = tvec.flatten()\n\n            return transform\n        else:\n            return None\n"})}),"\n",(0,a.jsx)(n.h2,{id:"intelligent-control-systems",children:"Intelligent Control Systems"}),"\n",(0,a.jsx)(n.p,{children:"AI-based control systems enable humanoid robots to execute complex behaviors and adapt to changing conditions."}),"\n",(0,a.jsx)(n.h3,{id:"model-predictive-control-mpc-with-learning",children:"Model Predictive Control (MPC) with Learning"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"Pseudocode: Learning-Based MPC\nclass LearningBasedMPC:\n    def __init__(self, robot_model):\n        self.robot_model = robot_model\n        self.dynamics_predictor = DynamicsPredictor()\n        self.trajectory_optimizer = TrajectoryOptimizer()\n        self.learning_module = LearningModule()\n        self.safety_checker = SafetyChecker()\n\n    def compute_control_command(self, current_state, reference_trajectory, horizon=10):\n        # Use learned dynamics model to predict future states\n        predicted_trajectories = []\n\n        for control_sequence in self.generate_control_candidates(horizon):\n            predicted_states = []\n            state = current_state.copy()\n\n            for control in control_sequence:\n                # Predict next state using learned dynamics\n                next_state = self.dynamics_predictor.predict(\n                    state, control, self.dt\n                )\n\n                # Check safety constraints\n                if not self.safety_checker.is_safe(next_state):\n                    break\n\n                predicted_states.append(next_state)\n                state = next_state\n\n            if len(predicted_states) == len(control_sequence):\n                cost = self.evaluate_trajectory_cost(\n                    predicted_states, reference_trajectory\n                )\n                predicted_trajectories.append({\n                    'states': predicted_states,\n                    'controls': control_sequence,\n                    'cost': cost\n                })\n\n        # Select optimal trajectory\n        optimal_trajectory = min(\n            predicted_trajectories,\n            key=lambda x: x['cost']\n        )\n\n        # Apply learning to improve future predictions\n        self.learning_module.update(\n            current_state,\n            optimal_trajectory['controls'][0]\n        )\n\n        return optimal_trajectory['controls'][0]\n\n    def evaluate_trajectory_cost(self, predicted_states, reference_trajectory):\n        total_cost = 0.0\n\n        for i, (pred_state, ref_state) in enumerate(zip(predicted_states, reference_trajectory)):\n            # Tracking cost\n            tracking_error = np.linalg.norm(pred_state - ref_state)\n            total_cost += tracking_error ** 2\n\n            # Control effort cost\n            if i < len(predicted_states) - 1:\n                control_diff = predicted_states[i+1]['control'] - pred_state['control']\n                total_cost += 0.1 * np.sum(control_diff ** 2)\n\n            # Safety cost\n            if not self.safety_checker.is_safe(pred_state):\n                total_cost += 1000.0  # High penalty for unsafe states\n\n        return total_cost\n"})}),"\n",(0,a.jsx)(n.h3,{id:"adaptive-control-with-neural-networks",children:"Adaptive Control with Neural Networks"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"Pseudocode: Neural Adaptive Controller\nclass NeuralAdaptiveController:\n    def __init__(self, robot_dof):\n        self.robot_dof = robot_dof\n        self.adaptive_network = self.build_adaptive_network()\n        self.feedback_controller = PIDController()\n        self.reference_model = ReferenceModel()\n        self.parameter_estimator = ParameterEstimator()\n\n    def build_adaptive_network(self):\n        # Build neural network for adaptive control\n        network = Sequential([\n            Dense(128, activation='relu', input_shape=(self.robot_dof * 4,)),  # [pos, vel, ref_pos, ref_vel]\n            Dense(64, activation='relu'),\n            Dense(32, activation='relu'),\n            Dense(self.robot_dof, activation='linear')  # Adaptive control terms\n        ])\n        return network\n\n    def compute_adaptive_control(self, current_state, desired_state, dt):\n        # Extract relevant state information\n        state_error = current_state['position'] - desired_state['position']\n        velocity_error = current_state['velocity'] - desired_state['velocity']\n\n        # Prepare network input\n        network_input = np.concatenate([\n            current_state['position'],\n            current_state['velocity'],\n            desired_state['position'],\n            desired_state['velocity']\n        ])\n\n        # Compute adaptive control terms\n        adaptive_terms = self.adaptive_network.predict(network_input)\n\n        # Compute feedback control\n        feedback_control = self.feedback_controller.compute(\n            state_error, velocity_error, dt\n        )\n\n        # Combine adaptive and feedback control\n        total_control = feedback_control + adaptive_terms\n\n        # Update network based on tracking performance\n        self.update_adaptive_network(\n            network_input, total_control, state_error\n        )\n\n        return total_control\n\n    def update_adaptive_network(self, state_input, control_output, tracking_error):\n        # Define target adaptive terms to minimize tracking error\n        target_adaptive = self.compute_target_adaptive(\n            tracking_error, control_output\n        )\n\n        # Train network to predict required adaptive terms\n        self.adaptive_network.fit(\n            x=state_input.reshape(1, -1),\n            y=target_adaptive.reshape(1, -1),\n            epochs=1,\n            verbose=0\n        )\n"})}),"\n",(0,a.jsx)(n.h2,{id:"learning-and-adaptation",children:"Learning and Adaptation"}),"\n",(0,a.jsx)(n.p,{children:"Enabling robots to learn from experience and adapt to new situations is crucial for autonomous operation."}),"\n",(0,a.jsx)(n.h3,{id:"online-learning-systems",children:"Online Learning Systems"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"Pseudocode: Online Learning System\nclass OnlineLearningSystem:\n    def __init__(self):\n        self.behavior_models = {}\n        self.experience_buffer = ExperienceBuffer(max_size=10000)\n        self.learning_algorithm = IncrementalLearner()\n        self.performance_monitor = PerformanceMonitor()\n\n    def update_behavior_model(self, task, observation, action, reward, next_observation):\n        # Store experience in buffer\n        experience = {\n            'task': task,\n            'observation': observation,\n            'action': action,\n            'reward': reward,\n            'next_observation': next_observation,\n            'timestamp': time.time()\n        }\n        self.experience_buffer.add(experience)\n\n        # Update behavior model incrementally\n        if self.should_update_model(task):\n            self.learning_algorithm.incremental_update(\n                task,\n                self.experience_buffer.get_recent_experiences(task, 100)\n            )\n\n        # Monitor performance and trigger retraining if needed\n        performance = self.performance_monitor.evaluate(task)\n        if performance < self.performance_threshold:\n            self.trigger_retraining(task)\n\n    def predict_action(self, task, observation):\n        # Use learned model to predict best action\n        if task in self.behavior_models:\n            return self.behavior_models[task].predict(observation)\n        else:\n            # Use default controller for unknown tasks\n            return self.default_controller(observation)\n\n    def transfer_learning(self, source_task, target_task):\n        # Transfer knowledge from source task to target task\n        source_model = self.behavior_models[source_task]\n        target_model = self.initialize_model(target_task)\n\n        # Fine-tune target model using source model weights\n        target_model.transfer_weights(\n            source_model,\n            learning_rate=0.001\n        )\n\n        # Adapt to target task using few examples\n        for experience in self.get_target_task_experiences(target_task, 50):\n            target_model.update(\n                experience['observation'],\n                experience['action']\n            )\n\n        self.behavior_models[target_task] = target_model\n"})}),"\n",(0,a.jsx)(n.h2,{id:"learning-outcomes-1",children:"Learning Outcomes"}),"\n",(0,a.jsx)(n.p,{children:"By the end of this week, students should be able to:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Implement computer vision systems"})," - Design and implement object detection, semantic segmentation, and 3D perception systems for robotic applications."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Fuse multi-sensor data"})," - Integrate data from cameras, IMUs, odometry, and other sensors using Kalman filters and other fusion techniques."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Design intelligent control systems"})," - Create adaptive and learning-based controllers that enable robots to improve their performance over time."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Apply model predictive control"})," - Implement MPC systems that use learned dynamics models for optimal trajectory planning and control."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Develop online learning capabilities"})," - Create systems that enable robots to learn from experience and adapt their behavior in real-time."]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h1,{id:"week-10-reinforcement-learning-for-robot-control",children:"Week 10: Reinforcement Learning for Robot Control"}),"\n",(0,a.jsx)(n.h2,{id:"introduction-2",children:"Introduction"}),"\n",(0,a.jsx)(n.p,{children:"Reinforcement Learning (RL) has emerged as a powerful paradigm for developing adaptive and intelligent robot control systems. This week explores how RL algorithms can be applied to teach humanoid robots complex behaviors, from basic locomotion to sophisticated manipulation tasks. We examine various RL approaches, their implementation in robotic systems, and the challenges of applying these techniques to real-world robot control problems."}),"\n",(0,a.jsx)(n.h2,{id:"foundations-of-reinforcement-learning-for-robotics",children:"Foundations of Reinforcement Learning for Robotics"}),"\n",(0,a.jsx)(n.p,{children:"Reinforcement learning provides a framework for learning optimal behaviors through interaction with the environment, making it particularly well-suited for robotic applications where explicit programming of all possible scenarios is infeasible."}),"\n",(0,a.jsx)(n.h3,{id:"rl-framework-for-robotics",children:"RL Framework for Robotics"}),"\n",(0,a.jsx)(n.p,{children:"The RL framework in robotics consists of:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Agent"}),": The robot learning to perform tasks"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Environment"}),": The physical or simulated world where the robot operates"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"State"}),": Robot's current configuration and environmental information"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Action"}),": Control commands sent to the robot's actuators"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Reward"}),": Feedback signal indicating task success or failure"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Policy"}),": Strategy that maps states to actions"]}),"\n"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"Pseudocode: Basic RL Framework for Robotics\nclass RobotRLAgent:\n    def __init__(self, robot_env, policy_network, learning_rate=0.001):\n        self.env = robot_env\n        self.policy_network = policy_network\n        self.learning_rate = learning_rate\n        self.optimizer = AdamOptimizer(learning_rate)\n        self.replay_buffer = ReplayBuffer(max_size=100000)\n        self.exploration_noise = OrnsteinUhlenbeckProcess()\n\n    def train_step(self, state, action, reward, next_state, done):\n        # Store experience in replay buffer\n        self.replay_buffer.add(state, action, reward, next_state, done)\n\n        # Sample batch from replay buffer\n        batch = self.replay_buffer.sample(batch_size=32)\n\n        # Update policy network\n        with tf.GradientTape() as tape:\n            # Compute predicted actions\n            predicted_actions = self.policy_network(batch.states)\n\n            # Compute loss (negative expected return)\n            loss = self.compute_policy_loss(batch, predicted_actions)\n\n        # Apply gradients\n        gradients = tape.gradient(loss, self.policy_network.trainable_variables)\n        self.optimizer.apply_gradients(\n            zip(gradients, self.policy_network.trainable_variables)\n        )\n\n        return loss\n\n    def select_action(self, state, explore=True):\n        # Get action from policy network\n        action = self.policy_network.predict(state)\n\n        if explore:\n            # Add exploration noise\n            noise = self.exploration_noise.sample()\n            action = action + noise\n\n        # Ensure action is within bounds\n        action = np.clip(action, self.env.action_space.low, self.env.action_space.high)\n\n        return action\n\n    def compute_policy_loss(self, batch, predicted_actions):\n        # Compute policy gradient loss\n        # This is a simplified version - actual implementation depends on specific algorithm\n        advantages = self.compute_advantages(batch.rewards, batch.values)\n        log_probs = self.compute_log_probs(predicted_actions, batch.actions)\n\n        policy_loss = -tf.reduce_mean(advantages * log_probs)\n        return policy_loss\n"})}),"\n",(0,a.jsx)(n.h3,{id:"reward-function-design",children:"Reward Function Design"}),"\n",(0,a.jsx)(n.p,{children:"Designing effective reward functions is critical for successful RL in robotics:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"Pseudocode: Reward Function Design\nclass RewardFunctionDesigner:\n    def __init__(self):\n        self.components = {}\n        self.weights = {}\n        self.normalization_factors = {}\n\n    def design_locomotion_reward(self):\n        # Design reward function for bipedal locomotion\n        def reward_function(state, action, next_state, info):\n            reward = 0.0\n\n            # Forward progress reward\n            forward_velocity = next_state['base_linear_velocity'][0]  # x-direction\n            target_velocity = 1.0  # m/s\n            velocity_reward = max(0, forward_velocity)  # Only reward forward motion\n            reward += velocity_reward * 2.0\n\n            # Balance reward - maintain upright posture\n            base_orientation = next_state['base_orientation']  # quaternion\n            upright_reward = self.calculate_upright_reward(base_orientation)\n            reward += upright_reward * 3.0\n\n            # Energy efficiency reward - penalize excessive joint torques\n            joint_torques = next_state['joint_torques']\n            energy_penalty = -np.mean(np.abs(joint_torques)) * 0.1\n            reward += energy_penalty\n\n            # Smoothness reward - penalize jerky movements\n            action_smoothness = -np.sum(np.abs(action)) * 0.05\n            reward += action_smoothness\n\n            # Safety reward - penalize dangerous configurations\n            if self.is_unsafe_configuration(next_state):\n                reward += -10.0  # Large penalty for unsafe states\n\n            # Normalize reward\n            reward = np.clip(reward, -10.0, 10.0)\n\n            return reward\n\n        return reward_function\n\n    def design_manipulation_reward(self):\n        # Design reward function for manipulation tasks\n        def reward_function(state, action, next_state, info):\n            reward = 0.0\n\n            # Distance to target reward\n            target_pos = info['target_position']\n            end_effector_pos = next_state['end_effector_position']\n            distance = np.linalg.norm(target_pos - end_effector_pos)\n            distance_reward = -distance  # Negative distance (closer is better)\n            reward += distance_reward * 5.0\n\n            # Grasping reward\n            if self.is_grasping_object(next_state):\n                reward += 10.0\n                # Bonus for stable grasp\n                grasp_stability = self.calculate_grasp_stability(next_state)\n                reward += grasp_stability * 2.0\n\n            # Avoid collisions\n            if self.is_in_collision(next_state):\n                reward += -5.0\n\n            # Joint limit penalty\n            joint_angles = next_state['joint_positions']\n            joint_limit_penalty = self.calculate_joint_limit_penalty(joint_angles)\n            reward += joint_limit_penalty * -1.0\n\n            return reward\n\n        return reward_function\n\n    def calculate_upright_reward(self, orientation_quat):\n        # Calculate how upright the robot is (1.0 = perfectly upright, -1.0 = upside down)\n        # Convert quaternion to rotation matrix and extract z-axis\n        rotation_matrix = self.quaternion_to_rotation_matrix(orientation_quat)\n        world_up = np.array([0, 0, 1])\n        robot_up = rotation_matrix[:, 2]  # z-axis of robot frame\n\n        dot_product = np.dot(world_up, robot_up)\n        return max(-1.0, min(1.0, dot_product))  # Clamp between -1 and 1\n"})}),"\n",(0,a.jsx)(n.h2,{id:"deep-reinforcement-learning-algorithms",children:"Deep Reinforcement Learning Algorithms"}),"\n",(0,a.jsx)(n.p,{children:"Deep RL algorithms combine neural networks with reinforcement learning to handle high-dimensional state and action spaces common in robotics."}),"\n",(0,a.jsx)(n.h3,{id:"deep-deterministic-policy-gradient-ddpg",children:"Deep Deterministic Policy Gradient (DDPG)"}),"\n",(0,a.jsx)(n.p,{children:"DDPG is suitable for continuous control tasks like robot joint control:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"Pseudocode: DDPG Implementation\nclass DDPGAgent:\n    def __init__(self, state_dim, action_dim, action_bound):\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.action_bound = action_bound\n\n        # Actor network (policy)\n        self.actor = self.create_actor_network()\n        self.target_actor = self.create_actor_network()\n\n        # Critic network (value function)\n        self.critic = self.create_critic_network()\n        self.target_critic = self.create_critic_network()\n\n        # Initialize target networks with same weights as main networks\n        self.target_actor.set_weights(self.actor.get_weights())\n        self.target_critic.set_weights(self.critic.get_weights())\n\n        self.replay_buffer = ReplayBuffer(max_size=100000)\n        self.noise_process = OrnsteinUhlenbeckProcess(size=action_dim)\n\n        # Hyperparameters\n        self.gamma = 0.99  # Discount factor\n        self.tau = 0.005   # Target network update rate\n        self.learning_rate = 0.001\n\n    def create_actor_network(self):\n        # Actor network: state -> action\n        model = Sequential([\n            Dense(400, activation='relu', input_shape=(self.state_dim,)),\n            Dense(300, activation='relu'),\n            Dense(self.action_dim, activation='tanh')  # Output in [-1, 1]\n        ])\n\n        # Scale output to action bounds\n        def scale_action(action):\n            return action * self.action_bound\n\n        model.add(Lambda(scale_action))\n        model.compile(optimizer=Adam(learning_rate=self.learning_rate))\n        return model\n\n    def create_critic_network(self):\n        # Critic network: (state, action) -> Q-value\n        state_input = Input(shape=(self.state_dim,))\n        action_input = Input(shape=(self.action_dim,))\n\n        # Process state\n        state_hidden = Dense(400, activation='relu')(state_input)\n        state_hidden = Dense(300)(state_hidden)\n\n        # Process action\n        action_hidden = Dense(300)(action_input)\n\n        # Combine state and action\n        combined = Add()([state_hidden, action_hidden])\n        combined = Activation('relu')(combined)\n\n        # Output Q-value\n        q_value = Dense(1)(combined)\n\n        model = Model(inputs=[state_input, action_input], outputs=q_value)\n        model.compile(optimizer=Adam(learning_rate=self.learning_rate), loss='mse')\n        return model\n\n    def update_networks(self):\n        if len(self.replay_buffer) < 64:  # Batch size\n            return\n\n        # Sample batch from replay buffer\n        batch = self.replay_buffer.sample(64)\n        states, actions, rewards, next_states, dones = batch\n\n        # Update critic network\n        with tf.GradientTape() as tape:\n            # Get target Q-values from target networks\n            next_actions = self.target_actor(next_states)\n            target_q_values = self.target_critic([next_states, next_actions])\n            target_q_values = rewards + (1 - dones) * self.gamma * target_q_values\n\n            # Current Q-values\n            current_q_values = self.critic([states, actions])\n\n            # Critic loss\n            critic_loss = tf.reduce_mean(tf.square(target_q_values - current_q_values))\n\n        # Apply critic gradients\n        critic_gradients = tape.gradient(critic_loss, self.critic.trainable_variables)\n        self.critic.optimizer.apply_gradients(\n            zip(critic_gradients, self.critic.trainable_variables)\n        )\n\n        # Update actor network (policy gradient)\n        with tf.GradientTape() as tape:\n            # Get actions from current policy\n            current_actions = self.actor(states)\n\n            # Get Q-values for these actions\n            q_values = self.critic([states, current_actions])\n\n            # Actor loss (maximize Q-values)\n            actor_loss = -tf.reduce_mean(q_values)\n\n        # Apply actor gradients\n        actor_gradients = tape.gradient(actor_loss, self.actor.trainable_variables)\n        self.actor.optimizer.apply_gradients(\n            zip(actor_gradients, self.actor.trainable_variables)\n        )\n\n        # Update target networks (soft update)\n        self.update_target_networks()\n\n    def update_target_networks(self):\n        # Soft update target networks\n        actor_weights = self.actor.get_weights()\n        target_actor_weights = self.target_actor.get_weights()\n        for i in range(len(actor_weights)):\n            target_actor_weights[i] = (\n                self.tau * actor_weights[i] + (1 - self.tau) * target_actor_weights[i]\n            )\n        self.target_actor.set_weights(target_actor_weights)\n\n        critic_weights = self.critic.get_weights()\n        target_critic_weights = self.target_critic.get_weights()\n        for i in range(len(critic_weights)):\n            target_critic_weights[i] = (\n                self.tau * critic_weights[i] + (1 - self.tau) * target_critic_weights[i]\n            )\n        self.target_critic.set_weights(target_critic_weights)\n"})}),"\n",(0,a.jsx)(n.h3,{id:"twin-delayed-ddpg-td3",children:"Twin Delayed DDPG (TD3)"}),"\n",(0,a.jsx)(n.p,{children:"TD3 addresses overestimation bias in DDPG:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"Pseudocode: TD3 Implementation\nclass TD3Agent:\n    def __init__(self, state_dim, action_dim, action_bound):\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.action_bound = action_bound\n\n        # Actor network (single)\n        self.actor = self.create_actor_network()\n        self.target_actor = self.create_actor_network()\n\n        # Two critic networks (twin critics)\n        self.critic1 = self.create_critic_network()\n        self.critic2 = self.create_critic_network()\n        self.target_critic1 = self.create_critic_network()\n        self.target_critic2 = self.create_critic_network()\n\n        # Initialize target networks\n        self.target_actor.set_weights(self.actor.get_weights())\n        self.target_critic1.set_weights(self.critic1.get_weights())\n        self.target_critic2.set_weights(self.critic2.get_weights())\n\n        self.replay_buffer = ReplayBuffer(max_size=100000)\n        self.noise_std = 0.1\n        self.target_policy_noise_std = 0.2\n        self.target_policy_noise_clip = 0.5\n\n        self.gamma = 0.99\n        self.tau = 0.005\n        self.policy_delay = 2  # Update policy every 2 critic updates\n        self.update_counter = 0\n\n    def update_networks(self):\n        if len(self.replay_buffer) < 100:  # Minimum batch size\n            return\n\n        # Sample batch from replay buffer\n        states, actions, rewards, next_states, dones = self.replay_buffer.sample(100)\n\n        # Add noise to target actions for smoothing\n        target_actions = self.target_actor(next_states)\n        noise = tf.random.normal(shape=target_actions.shape, stddev=self.target_policy_noise_std)\n        noise = tf.clip_by_value(noise, -self.target_policy_noise_clip, self.target_policy_noise_clip)\n        noisy_target_actions = tf.clip_by_value(\n            target_actions + noise,\n            -self.action_bound,\n            self.action_bound\n        )\n\n        # Compute target Q-values using minimum of twin critics\n        target_q1 = self.target_critic1([next_states, noisy_target_actions])\n        target_q2 = self.target_critic2([next_states, noisy_target_actions])\n        target_q = rewards + (1 - dones) * self.gamma * tf.minimum(target_q1, target_q2)\n\n        # Update critics\n        with tf.GradientTape(persistent=True) as tape:\n            current_q1 = self.critic1([states, actions])\n            current_q2 = self.critic2([states, actions])\n\n            critic1_loss = tf.reduce_mean(tf.square(target_q - current_q1))\n            critic2_loss = tf.reduce_mean(tf.square(target_q - current_q2))\n\n        # Apply critic gradients\n        critic1_grads = tape.gradient(critic1_loss, self.critic1.trainable_variables)\n        critic2_grads = tape.gradient(critic2_loss, self.critic2.trainable_variables)\n\n        self.critic1.optimizer.apply_gradients(\n            zip(critic1_grads, self.critic1.trainable_variables)\n        )\n        self.critic2.optimizer.apply_gradients(\n            zip(critic2_grads, self.critic2.trainable_variables)\n        )\n\n        # Update actor (delayed)\n        self.update_counter += 1\n        if self.update_counter % self.policy_delay == 0:\n            with tf.GradientTape() as tape:\n                current_actions = self.actor(states)\n                actor_q = self.critic1([states, current_actions])\n                actor_loss = -tf.reduce_mean(actor_q)\n\n            actor_grads = tape.gradient(actor_loss, self.actor.trainable_variables)\n            self.actor.optimizer.apply_gradients(\n                zip(actor_grads, self.actor.trainable_variables)\n            )\n\n        # Update target networks\n        self.update_target_networks()\n"})}),"\n",(0,a.jsx)(n.h3,{id:"soft-actor-critic-sac",children:"Soft Actor-Critic (SAC)"}),"\n",(0,a.jsx)(n.p,{children:"SAC maximizes both expected reward and entropy for better exploration:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"Pseudocode: SAC Implementation\nclass SACAgent:\n    def __init__(self, state_dim, action_dim, action_bound):\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.action_bound = action_bound\n\n        # Actor network (stochastic policy)\n        self.actor = self.create_actor_network()\n\n        # Two Q-networks (twin Q-learning)\n        self.q_network1 = self.create_q_network()\n        self.q_network2 = self.create_q_network()\n        self.target_q_network1 = self.create_q_network()\n        self.target_q_network2 = self.create_q_network()\n\n        # Temperature parameter (entropy regularization)\n        self.log_alpha = tf.Variable(0.0, trainable=True)  # log of alpha\n        self.alpha_optimizer = Adam(learning_rate=3e-4)\n\n        # Initialize target networks\n        self.target_q_network1.set_weights(self.q_network1.get_weights())\n        self.target_q_network2.set_weights(self.q_network2.get_weights())\n\n        self.replay_buffer = ReplayBuffer(max_size=100000)\n        self.target_entropy = -action_dim  # Target entropy for automatic alpha tuning\n\n        self.gamma = 0.99\n        self.tau = 0.005\n\n    def create_actor_network(self):\n        # Stochastic actor that outputs mean and std of Gaussian policy\n        state_input = Input(shape=(self.state_dim,))\n\n        hidden = Dense(256, activation='relu')(state_input)\n        hidden = Dense(256, activation='relu')(hidden)\n\n        # Output mean and log_std for each action dimension\n        mean_output = Dense(self.action_dim, activation='tanh')(hidden)\n        log_std_output = Dense(self.action_dim, activation='tanh')(hidden)\n\n        # Scale log_std to reasonable range\n        log_std_output = tf.clip_by_value(log_std_output, -20, 2)\n\n        # Reparameterization trick for sampling\n        def sample_action(inputs):\n            mean, log_std = inputs\n            std = tf.exp(log_std)\n            noise = tf.random.normal(shape=tf.shape(mean))\n            raw_action = mean + std * noise\n            # Apply tanh to bound actions\n            bounded_action = tf.tanh(raw_action)\n            return bounded_action\n\n        action_output = Lambda(sample_action)([mean_output, log_std_output])\n\n        model = Model(inputs=state_input, outputs=[mean_output, log_std_output, action_output])\n        return model\n\n    def update_networks(self):\n        if len(self.replay_buffer) < 256:  # Batch size\n            return\n\n        # Sample batch from replay buffer\n        states, actions, rewards, next_states, dones = self.replay_buffer.sample(256)\n\n        # Update Q-networks\n        with tf.GradientTape(persistent=True) as tape:\n            # Get next state actions and log probabilities\n            next_means, next_log_stds, next_actions = self.actor(next_states)\n            next_log_probs = self.gaussian_log_prob(next_means, next_log_stds, next_actions)\n\n            # Compute target Q-values\n            next_q1 = self.target_q_network1(next_states)\n            next_q2 = self.target_q_network2(next_states)\n            next_q_min = tf.minimum(next_q1, next_q2)\n            target_q = rewards + (1 - dones) * self.gamma * (\n                next_q_min - tf.exp(self.log_alpha) * next_log_probs\n            )\n\n            # Current Q-values\n            current_q1 = self.q_network1(states)\n            current_q2 = self.q_network2(states)\n\n            # Q-network losses\n            q1_loss = tf.reduce_mean(tf.square(target_q - current_q1))\n            q2_loss = tf.reduce_mean(tf.square(target_q - current_q2))\n\n        # Apply Q-network gradients\n        q1_grads = tape.gradient(q1_loss, self.q_network1.trainable_variables)\n        q2_grads = tape.gradient(q2_loss, self.q_network2.trainable_variables)\n\n        self.q_network1.optimizer.apply_gradients(\n            zip(q1_grads, self.q_network1.trainable_variables)\n        )\n        self.q_network2.optimizer.apply_gradients(\n            zip(q2_grads, self.q_network2.trainable_variables)\n        )\n\n        # Update actor network\n        with tf.GradientTape() as tape:\n            means, log_stds, sampled_actions = self.actor(states)\n            log_probs = self.gaussian_log_prob(means, log_stds, sampled_actions)\n\n            # Q-values for current actions\n            q1_values = self.q_network1(states)\n            q2_values = self.q_network2(states)\n            min_q_values = tf.minimum(q1_values, q2_values)\n\n            # Actor loss (maximize Q-value and entropy)\n            actor_loss = tf.reduce_mean(\n                tf.exp(self.log_alpha) * log_probs - min_q_values\n            )\n\n        actor_grads = tape.gradient(actor_loss, self.actor.trainable_variables)\n        self.actor.optimizer.apply_gradients(\n            zip(actor_grads, self.actor.trainable_variables)\n        )\n\n        # Update temperature parameter (alpha)\n        with tf.GradientTape() as tape:\n            means, log_stds, sampled_actions = self.actor(states)\n            log_probs = self.gaussian_log_prob(means, log_stds, sampled_actions)\n\n            # Temperature loss (match target entropy)\n            alpha_loss = -tf.reduce_mean(\n                self.log_alpha * (log_probs + self.target_entropy)\n            )\n\n        alpha_grads = tape.gradient(alpha_loss, [self.log_alpha])\n        self.alpha_optimizer.apply_gradients(\n            zip(alpha_grads, [self.log_alpha])\n        )\n\n        # Update target networks\n        self.update_target_networks()\n\n    def gaussian_log_prob(self, mean, log_std, action):\n        # Compute log probability of action under Gaussian distribution\n        std = tf.exp(log_std)\n        var = std ** 2\n        log_prob = -0.5 * ((action - mean) ** 2) / var - 0.5 * tf.math.log(2 * np.pi * var)\n        return tf.reduce_sum(log_prob, axis=1, keepdims=True)\n"})}),"\n",(0,a.jsx)(n.h2,{id:"simulation-to-real-transfer",children:"Simulation-to-Real Transfer"}),"\n",(0,a.jsx)(n.p,{children:"Transferring policies learned in simulation to real robots is a major challenge in robotics RL."}),"\n",(0,a.jsx)(n.h3,{id:"domain-randomization",children:"Domain Randomization"}),"\n",(0,a.jsx)(n.p,{children:"Domain randomization helps policies generalize across different environments:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"Pseudocode: Domain Randomization System\nclass DomainRandomization:\n    def __init__(self):\n        self.randomization_ranges = {\n            'robot_dynamics': {\n                'mass_multiplier': (0.8, 1.2),\n                'friction_coefficient': (0.1, 0.9),\n                'joint_damping': (0.01, 0.1),\n                'actuator_delay': (0.0, 0.02)\n            },\n            'environment': {\n                'gravity': (9.5, 10.1),\n                'ground_friction': (0.4, 0.8),\n                'lighting_intensity': (0.5, 2.0),\n                'texture_roughness': (0.0, 1.0)\n            },\n            'sensors': {\n                'imu_noise': (0.001, 0.01),\n                'camera_noise': (0.001, 0.005),\n                'delay': (0.0, 0.01)\n            }\n        }\n\n    def randomize_environment(self, env):\n        # Randomize robot dynamics\n        mass_multiplier = np.random.uniform(\n            self.randomization_ranges['robot_dynamics']['mass_multiplier'][0],\n            self.randomization_ranges['robot_dynamics']['mass_multiplier'][1]\n        )\n        env.robot.set_mass_multiplier(mass_multiplier)\n\n        friction = np.random.uniform(\n            self.randomization_ranges['robot_dynamics']['friction_coefficient'][0],\n            self.randomization_ranges['robot_dynamics']['friction_coefficient'][1]\n        )\n        env.robot.set_friction_coefficient(friction)\n\n        # Randomize environment properties\n        gravity = np.random.uniform(\n            self.randomization_ranges['environment']['gravity'][0],\n            self.randomization_ranges['environment']['gravity'][1]\n        )\n        env.set_gravity(gravity)\n\n        # Randomize sensor properties\n        imu_noise = np.random.uniform(\n            self.randomization_ranges['sensors']['imu_noise'][0],\n            self.randomization_ranges['sensors']['imu_noise'][1]\n        )\n        env.robot.set_imu_noise(imu_noise)\n\n    def curriculum_randomization(self, training_step, max_steps):\n        # Gradually increase randomization as training progresses\n        progress = training_step / max_steps\n        randomization_factor = min(progress * 2, 1.0)  # Increase up to 2x range\n\n        for param, (min_val, max_val) in self.randomization_ranges['robot_dynamics'].items():\n            range_size = (max_val - min_val) * randomization_factor\n            center = (max_val + min_val) / 2\n            new_min = center - range_size / 2\n            new_max = center + range_size / 2\n            self.randomization_ranges['robot_dynamics'][param] = (new_min, new_max)\n"})}),"\n",(0,a.jsx)(n.h3,{id:"system-identification-and-model-adaptation",children:"System Identification and Model Adaptation"}),"\n",(0,a.jsx)(n.p,{children:"Adapting simulation models to better match real robot behavior:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"Pseudocode: System Identification\nclass SystemIdentifier:\n    def __init__(self, robot_model):\n        self.robot_model = robot_model\n        self.param_optimizer = ParameterOptimizer()\n        self.simulator = PhysicsSimulator()\n        self.real_data_buffer = DataBuffer(max_size=10000)\n\n    def identify_robot_parameters(self, excitation_signal):\n        # Apply excitation signal to real robot and collect data\n        real_responses = self.excite_robot(excitation_signal)\n\n        # Optimize simulation parameters to match real responses\n        optimized_params = self.param_optimizer.optimize(\n            target_data=real_responses,\n            simulation_model=self.simulator,\n            initial_params=self.robot_model.get_parameters()\n        )\n\n        # Update robot model with identified parameters\n        self.robot_model.update_parameters(optimized_params)\n\n        return optimized_params\n\n    def excite_robot(self, signal):\n        # Apply known excitation signal to robot joints\n        responses = []\n\n        for step, command in enumerate(signal):\n            # Send command to robot\n            self.robot_model.send_command(command)\n\n            # Record response\n            state = self.robot_model.get_state()\n            responses.append({\n                'time': step * self.dt,\n                'position': state['position'],\n                'velocity': state['velocity'],\n                'torque': state['torque'],\n                'command': command\n            })\n\n        return responses\n\n    def adaptive_model_learning(self, real_experience):\n        # Update simulation model based on real-world experience\n        for episode in real_experience:\n            # Extract system behavior from real data\n            real_behavior = self.extract_behavior(episode)\n\n            # Compare with simulation predictions\n            sim_behavior = self.simulator.predict(episode.initial_state, episode.actions)\n\n            # Compute behavior mismatch\n            mismatch = self.compute_behavior_mismatch(real_behavior, sim_behavior)\n\n            # Update model parameters to reduce mismatch\n            updated_params = self.update_model_parameters(mismatch)\n            self.simulator.update_parameters(updated_params)\n"})}),"\n",(0,a.jsx)(n.h2,{id:"learning-outcomes-2",children:"Learning Outcomes"}),"\n",(0,a.jsx)(n.p,{children:"By the end of this week, students should be able to:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Design RL frameworks for robotics"})," - Create reinforcement learning systems specifically tailored for robotic control tasks with appropriate state, action, and reward definitions."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Implement advanced RL algorithms"})," - Apply DDPG, TD3, and SAC algorithms to continuous control problems in humanoid robotics."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Create effective reward functions"})," - Design reward functions that promote desired behaviors while avoiding local optima and unsafe configurations."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Address sim-to-real transfer challenges"})," - Implement domain randomization and system identification techniques to improve policy transfer from simulation to reality."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Evaluate and improve RL policies"})," - Assess policy performance, identify failure modes, and implement strategies for continuous improvement."]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.hr,{})]})}function _(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>s});var i=t(6540);const a={},o=i.createContext(a);function r(e){const n=i.useContext(o);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),i.createElement(o.Provider,{value:n},e.children)}}}]);