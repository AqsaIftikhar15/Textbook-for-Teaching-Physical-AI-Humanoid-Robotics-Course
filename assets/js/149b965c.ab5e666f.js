"use strict";(globalThis.webpackChunkphysical_ai_robotics_book=globalThis.webpackChunkphysical_ai_robotics_book||[]).push([[4916],{1148:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>_,frontMatter:()=>r,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module-2-digital-twin/week-6-7-gazebo-unity","title":"Robot Simulation with Gazebo and Unity","description":"Physics simulation and environment building","source":"@site/docs/module-2-digital-twin/week-6-7-gazebo-unity.md","sourceDirName":"module-2-digital-twin","slug":"/module-2-digital-twin/week-6-7-gazebo-unity","permalink":"/Textbook-for-Teaching-Physical-AI-Humanoid-Robotics-Course/docs/module-2-digital-twin/week-6-7-gazebo-unity","draft":false,"unlisted":false,"editUrl":"https://github.com/AqsaIftikhar15/Textbook-for-Teaching-Physical-AI-Humanoid-Robotics-Course/docs/module-2-digital-twin/week-6-7-gazebo-unity.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"Robot Simulation with Gazebo and Unity","sidebar_position":1,"description":"Physics simulation and environment building"},"sidebar":"tutorialSidebar","previous":{"title":"Advanced ROS 2 Concepts","permalink":"/Textbook-for-Teaching-Physical-AI-Humanoid-Robotics-Course/docs/module-1-ros2/week-4-advanced-ros2"},"next":{"title":"Simulation Concepts","permalink":"/Textbook-for-Teaching-Physical-AI-Humanoid-Robotics-Course/docs/module-2-digital-twin/simulation-concepts"}}');var o=i(4848),s=i(8453);const r={title:"Robot Simulation with Gazebo and Unity",sidebar_position:1,description:"Physics simulation and environment building"},a="Week 6: Gazebo Simulation Concepts",l={},c=[{value:"Introduction",id:"introduction",level:2},{value:"Physics Simulation Fundamentals",id:"physics-simulation-fundamentals",level:2},{value:"Physics Engine Architecture",id:"physics-engine-architecture",level:3},{value:"Gravity and Environmental Forces",id:"gravity-and-environmental-forces",level:3},{value:"Collision Detection and Response",id:"collision-detection-and-response",level:3},{value:"Environment Building and World Modeling",id:"environment-building-and-world-modeling",level:2},{value:"World Description Format (SDF)",id:"world-description-format-sdf",level:3},{value:"Terrain and Surface Modeling",id:"terrain-and-surface-modeling",level:3},{value:"Dynamic Environment Elements",id:"dynamic-environment-elements",level:3},{value:"Sensor Simulation Concepts",id:"sensor-simulation-concepts",level:2},{value:"LiDAR Simulation",id:"lidar-simulation",level:3},{value:"Camera Simulation",id:"camera-simulation",level:3},{value:"IMU Simulation",id:"imu-simulation",level:3},{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Introduction",id:"introduction-1",level:2},{value:"Unity Architecture for Robotics",id:"unity-architecture-for-robotics",level:2},{value:"Unity Scene Structure",id:"unity-scene-structure",level:3},{value:"Coordinate Systems and Transformations",id:"coordinate-systems-and-transformations",level:3},{value:"High-Fidelity Rendering and Materials",id:"high-fidelity-rendering-and-materials",level:2},{value:"Physically-Based Rendering (PBR)",id:"physically-based-rendering-pbr",level:3},{value:"Advanced Lighting Systems",id:"advanced-lighting-systems",level:3},{value:"Human-Robot Interaction Design",id:"human-robot-interaction-design",level:2},{value:"User Interface Systems",id:"user-interface-systems",level:3},{value:"Gesture Recognition and Input Systems",id:"gesture-recognition-and-input-systems",level:3},{value:"Real-time Simulation Synchronization",id:"real-time-simulation-synchronization",level:2},{value:"ROS Integration",id:"ros-integration",level:3},{value:"Performance Optimization",id:"performance-optimization",level:3},{value:"Augmented Reality Integration",id:"augmented-reality-integration",level:2},{value:"Learning Outcomes",id:"learning-outcomes-1",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"week-6-gazebo-simulation-concepts",children:"Week 6: Gazebo Simulation Concepts"})}),"\n",(0,o.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,o.jsx)(n.p,{children:"Gazebo serves as a critical simulation environment for developing and testing humanoid robots in a safe, controlled, and cost-effective manner. This week explores the fundamental concepts of physics simulation, environment modeling, and sensor simulation that enable effective robot development before real-world deployment. Gazebo's realistic physics engine and comprehensive sensor models make it an invaluable tool for Sim2Real transfer, where skills learned in simulation can be applied to real robots."}),"\n",(0,o.jsx)(n.h2,{id:"physics-simulation-fundamentals",children:"Physics Simulation Fundamentals"}),"\n",(0,o.jsx)(n.p,{children:"Gazebo's physics engine forms the foundation for realistic robot simulation, modeling the complex interactions between robots, objects, and environments that occur in the physical world."}),"\n",(0,o.jsx)(n.h3,{id:"physics-engine-architecture",children:"Physics Engine Architecture"}),"\n",(0,o.jsx)(n.p,{children:"Gazebo supports multiple physics engines, with each offering different trade-offs between accuracy and performance:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"ODE (Open Dynamics Engine)"}),": Fast, stable for most robotics applications"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Bullet"}),": More accurate collision detection and response"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"DART"}),": Advanced kinematic and dynamic modeling capabilities"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Simbody"}),": High-fidelity multibody dynamics"]}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"Pseudocode: Physics Simulation Core\nclass PhysicsEngine:\n    def __init__(self, engine_type='ode'):\n        self.engine = self.initialize_engine(engine_type)\n        self.world = World()\n        self.contact_manager = ContactManager()\n        self.integrator = Integrator(method='runge_kutta')\n\n    def simulate_step(self, dt):\n        # Update collision detection\n        collisions = self.contact_manager.detect_collisions(self.world)\n\n        # Apply contact forces\n        for collision in collisions:\n            force = self.compute_contact_force(collision)\n            collision.body1.apply_force(force)\n            collision.body2.apply_force(-force)\n\n        # Update body states (position, velocity, acceleration)\n        for body in self.world.bodies:\n            # Apply gravity\n            body.apply_force(body.mass * self.gravity)\n\n            # Apply user-defined forces\n            for force in body.external_forces:\n                body.apply_force(force)\n\n            # Integrate equations of motion\n            body.integrate(self.integrator, dt)\n\n        # Update world state\n        self.world.update(dt)\n\n    def compute_contact_force(self, collision):\n        # Compute normal and friction forces based on collision properties\n        penetration_depth = collision.penetration_depth\n        normal = collision.normal\n        relative_velocity = collision.body1.velocity - collision.body2.velocity\n\n        # Normal force (repulsive)\n        normal_force_magnitude = self.springs_constant * penetration_depth\n        normal_force = normal * normal_force_magnitude\n\n        # Friction force (opposes relative motion)\n        tangent_velocity = relative_velocity - (relative_velocity.dot(normal)) * normal\n        friction_force = -tangent_velocity.normalize() * min(\n            self.friction_coefficient * normal_force_magnitude,\n            tangent_velocity.length()\n        )\n\n        return normal_force + friction_force\n"})}),"\n",(0,o.jsx)(n.h3,{id:"gravity-and-environmental-forces",children:"Gravity and Environmental Forces"}),"\n",(0,o.jsx)(n.p,{children:"Realistic simulation of environmental forces is crucial for humanoid robot development:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"Pseudocode: Environmental Force Simulation\nclass EnvironmentalForces:\n    def __init__(self):\n        self.gravity = Vector3(0, 0, -9.81)  # Earth's gravity\n        self.wind = Vector3(0, 0, 0)  # Default no wind\n        self.magnetic_field = Vector3(0.25, 0, 0.45)  # Approximate magnetic field\n\n    def apply_gravity(self, body):\n        # Apply gravitational force to each body\n        gravity_force = body.mass * self.gravity\n        body.apply_force(gravity_force, body.center_of_mass)\n\n    def apply_wind_force(self, body, wind_speed, drag_coefficient):\n        # Compute wind resistance based on body's relative velocity\n        relative_velocity = body.velocity - self.wind\n        wind_force_magnitude = 0.5 * self.air_density * drag_coefficient * body.frontal_area * (\n            relative_velocity.length() ** 2\n        )\n        wind_force = -relative_velocity.normalize() * wind_force_magnitude\n        body.apply_force(wind_force, body.center_of_mass)\n\n    def simulate_atmospheric_effects(self, altitude):\n        # Adjust air density based on altitude\n        self.air_density = self.sea_level_density * math.exp(-altitude / 8500)  # Scale height approximation\n"})}),"\n",(0,o.jsx)(n.h3,{id:"collision-detection-and-response",children:"Collision Detection and Response"}),"\n",(0,o.jsx)(n.p,{children:"Accurate collision detection and response are essential for realistic humanoid robot simulation:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Broad Phase"}),": Fast culling of non-colliding pairs using bounding volumes"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Narrow Phase"}),": Precise collision detection between potentially colliding objects"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Contact Resolution"}),": Computing appropriate forces to prevent penetration"]}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"Pseudocode: Collision Detection System\nclass CollisionDetector:\n    def __init__(self):\n        self.broad_phase = BoundingVolumeHierarchy()\n        self.narrow_phase = GJKAlgorithm()  # Gilbert-Johnson-Keerthi\n        self.contact_resolver = SequentialImpulses()\n\n    def detect_collisions(self, bodies):\n        # Broad phase: identify potential collisions\n        potential_pairs = self.broad_phase.find_collisions(bodies)\n\n        actual_collisions = []\n        for body1, body2 in potential_pairs:\n            # Narrow phase: precise collision detection\n            collision_info = self.narrow_phase.check_collision(body1, body2)\n\n            if collision_info.collides:\n                # Compute contact points and normals\n                contact_points = self.compute_contact_points(collision_info)\n                for point in contact_points:\n                    collision = Collision(\n                        body1=body1,\n                        body2=body2,\n                        contact_point=point.position,\n                        normal=point.normal,\n                        penetration_depth=point.penetration\n                    )\n                    actual_collisions.append(collision)\n\n        return actual_collisions\n\n    def compute_contact_points(self, collision_info):\n        # Compute multiple contact points for stable contact\n        contact_points = []\n        for i in range(collision_info.num_contact_points):\n            point = ContactPoint()\n            point.position = collision_info.contact_positions[i]\n            point.normal = collision_info.contact_normals[i]\n            point.penetration = collision_info.penetrations[i]\n            contact_points.append(point)\n        return contact_points\n"})}),"\n",(0,o.jsx)(n.h2,{id:"environment-building-and-world-modeling",children:"Environment Building and World Modeling"}),"\n",(0,o.jsx)(n.p,{children:"Creating realistic simulation environments is crucial for effective humanoid robot training and testing. The environment must accurately represent the physical space where the robot will operate."}),"\n",(0,o.jsx)(n.h3,{id:"world-description-format-sdf",children:"World Description Format (SDF)"}),"\n",(0,o.jsx)(n.p,{children:"SDF (Simulation Description Format) provides a standardized way to describe simulation environments:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-xml",children:'\x3c!-- example_world.sdf --\x3e\n<sdf version="1.7">\n  <world name="humanoid_world">\n    \x3c!-- Physics engine configuration --\x3e\n    <physics type="ode">\n      <max_step_size>0.001</max_step_size>\n      <real_time_factor>1.0</real_time_factor>\n      <gravity>0 0 -9.8</gravity>\n    </physics>\n\n    \x3c!-- Ground plane --\x3e\n    <model name="ground_plane">\n      <static>true</static>\n      <link name="link">\n        <collision name="collision">\n          <geometry>\n            <plane>\n              <normal>0 0 1</normal>\n            </plane>\n          </geometry>\n        </collision>\n        <visual name="visual">\n          <geometry>\n            <plane>\n              <normal>0 0 1</normal>\n              <size>100 100</size>\n            </plane>\n          </geometry>\n          <material>\n            <ambient>0.7 0.7 0.7 1</ambient>\n            <diffuse>0.7 0.7 0.7 1</diffuse>\n          </material>\n        </visual>\n      </link>\n    </model>\n\n    \x3c!-- Obstacles and furniture --\x3e\n    <model name="table">\n      <pose>2 0 0 0 0 0</pose>\n      <link name="table_top">\n        <collision name="collision">\n          <geometry>\n            <box>\n              <size>1.0 0.8 0.02</size>\n            </box>\n          </geometry>\n        </collision>\n        <visual name="visual">\n          <geometry>\n            <box>\n              <size>1.0 0.8 0.02</size>\n            </box>\n          </geometry>\n        </visual>\n      </link>\n      <link name="leg1">\n        <pose>0.4 0.35 0.35 0 0 0</pose>\n        <collision name="collision">\n          <geometry>\n            <cylinder>\n              <radius>0.02</radius>\n              <length>0.7</length>\n            </cylinder>\n          </geometry>\n        </collision>\n      </link>\n    </model>\n  </world>\n</sdf>\n'})}),"\n",(0,o.jsx)(n.h3,{id:"terrain-and-surface-modeling",children:"Terrain and Surface Modeling"}),"\n",(0,o.jsx)(n.p,{children:"Realistic terrain modeling is essential for humanoid locomotion training:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"Pseudocode: Terrain Generation\nclass TerrainGenerator:\n    def __init__(self):\n        self.height_map = HeightMap()\n        self.material_properties = {}\n\n    def generate_realistic_terrain(self, size_x, size_y, resolution):\n        # Generate height map using Perlin noise for natural terrain\n        height_data = self.generate_perlin_noise(size_x, size_y, resolution)\n\n        # Add specific terrain features\n        height_data = self.add_hills(height_data, num_hills=5)\n        height_data = self.add_obstacles(height_data, obstacle_positions)\n        height_data = self.add_ramps(height_data, ramp_positions)\n\n        # Create collision and visual meshes\n        collision_mesh = self.create_collision_mesh(height_data)\n        visual_mesh = self.create_visual_mesh(height_data)\n\n        return collision_mesh, visual_mesh\n\n    def generate_perlin_noise(self, width, height, scale):\n        # Generate Perlin noise for natural terrain variation\n        noise_map = []\n        for y in range(height):\n            row = []\n            for x in range(width):\n                # Scale coordinates for noise function\n                nx = x / width * scale\n                ny = y / height * scale\n                height_value = self.perlin_noise(nx, ny)\n                row.append(height_value)\n            noise_map.append(row)\n        return noise_map\n\n    def add_hills(self, height_data, num_hills):\n        # Add random hills to terrain\n        for _ in range(num_hills):\n            hill_center_x = random.randint(0, len(height_data[0]) - 1)\n            hill_center_y = random.randint(0, len(height_data) - 1)\n            hill_radius = random.randint(5, 15)\n            hill_height = random.uniform(0.1, 0.5)\n\n            for y in range(len(height_data)):\n                for x in range(len(height_data[0])):\n                    distance = math.sqrt((x - hill_center_x)**2 + (y - hill_center_y)**2)\n                    if distance <= hill_radius:\n                        height_contribution = hill_height * (1 - distance / hill_radius)\n                        height_data[y][x] += height_contribution\n\n        return height_data\n\n    def assign_surface_properties(self, terrain_mesh):\n        # Assign different friction and restitution properties to terrain regions\n        surface_properties = {}\n\n        # Walkable surfaces (grass, concrete, etc.)\n        surface_properties['grass'] = {\n            'friction': 0.7,\n            'restitution': 0.1,\n            'texture': 'grass_texture.png'\n        }\n\n        # Slippery surfaces (ice, wet surfaces)\n        surface_properties['ice'] = {\n            'friction': 0.1,\n            'restitution': 0.8,\n            'texture': 'ice_texture.png'\n        }\n\n        return surface_properties\n"})}),"\n",(0,o.jsx)(n.h3,{id:"dynamic-environment-elements",children:"Dynamic Environment Elements"}),"\n",(0,o.jsx)(n.p,{children:"Simulating dynamic environments with moving objects and changing conditions:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"Pseudocode: Dynamic Environment System\nclass DynamicEnvironment:\n    def __init__(self):\n        self.moving_objects = []\n        self.environment_states = {}\n        self.event_scheduler = EventScheduler()\n\n    def add_moving_object(self, object_model, trajectory):\n        moving_obj = MovingObject()\n        moving_obj.model = object_model\n        moving_obj.trajectory = trajectory\n        moving_obj.current_time = 0.0\n        self.moving_objects.append(moving_obj)\n\n    def update_dynamic_elements(self, sim_time):\n        for obj in self.moving_objects:\n            # Update position based on trajectory\n            new_pose = obj.trajectory.interpolate(sim_time)\n            obj.model.set_pose(new_pose)\n\n            # Trigger events at specific times\n            if obj.current_time < sim_time and obj.has_events():\n                self.trigger_environment_event(obj.get_next_event())\n\n        # Update environment states (weather, lighting, etc.)\n        self.update_environment_states(sim_time)\n\n    def trigger_environment_event(self, event):\n        # Handle various environmental events\n        if event.type == 'door_open':\n            self.open_door(event.target)\n        elif event.type == 'light_change':\n            self.change_lighting(event.parameters)\n        elif event.type == 'obstacle_appear':\n            self.add_obstacle(event.position)\n"})}),"\n",(0,o.jsx)(n.h2,{id:"sensor-simulation-concepts",children:"Sensor Simulation Concepts"}),"\n",(0,o.jsx)(n.p,{children:"Accurate sensor simulation is crucial for developing robust perception and control systems that can transfer from simulation to reality."}),"\n",(0,o.jsx)(n.h3,{id:"lidar-simulation",children:"LiDAR Simulation"}),"\n",(0,o.jsx)(n.p,{children:"LiDAR sensors in simulation must accurately model the physical properties of laser ranging:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"Pseudocode: LiDAR Simulation\nclass LiDARSimulator:\n    def __init__(self, num_beams, max_range, min_range, resolution):\n        self.num_beams = num_beams\n        self.max_range = max_range\n        self.min_range = min_range\n        self.fov = 2 * math.pi  # 360-degree scan\n        self.angle_increment = self.fov / num_beams\n        self.resolution = resolution\n        self.noise_model = GaussianNoise(std_dev=0.01)\n\n    def simulate_scan(self, robot_pose, world_geometry):\n        scan_data = []\n\n        for i in range(self.num_beams):\n            angle = robot_pose.rotation + i * self.angle_increment\n\n            # Ray casting to find intersection\n            ray_origin = robot_pose.position\n            ray_direction = Vector2(\n                math.cos(angle),\n                math.sin(angle)\n            )\n\n            # Find closest intersection\n            min_distance = self.max_range\n            for geometry in world_geometry:\n                distance = self.ray_intersect(ray_origin, ray_direction, geometry)\n                if distance and distance < min_distance:\n                    min_distance = distance\n\n            # Apply noise and range limits\n            if min_distance < self.max_range:\n                noisy_distance = min_distance + self.noise_model.sample()\n                scan_data.append(max(self.min_range, noisy_distance))\n            else:\n                scan_data.append(float('inf'))  # No obstacle detected\n\n        return scan_data\n\n    def ray_intersect(self, origin, direction, geometry):\n        # Implement ray-geometry intersection tests\n        if geometry.type == 'box':\n            return self.ray_box_intersection(origin, direction, geometry)\n        elif geometry.type == 'sphere':\n            return self.ray_sphere_intersection(origin, direction, geometry)\n        elif geometry.type == 'mesh':\n            return self.ray_mesh_intersection(origin, direction, geometry)\n        return None\n"})}),"\n",(0,o.jsx)(n.h3,{id:"camera-simulation",children:"Camera Simulation"}),"\n",(0,o.jsx)(n.p,{children:"Camera sensors must model optical properties, distortion, and image formation:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"Pseudocode: Camera Simulation\nclass CameraSimulator:\n    def __init__(self, width, height, fov, distortion_params):\n        self.width = width\n        self.height = height\n        self.fov = fov\n        self.fx = width / (2 * math.tan(fov / 2))  # Focal length x\n        self.fy = height / (2 * math.tan(fov / 2))  # Focal length y\n        self.cx = width / 2  # Principal point x\n        self.cy = height / 2  # Principal point y\n        self.distortion = distortion_params  # [k1, k2, p1, p2, k3]\n\n    def simulate_image(self, camera_pose, scene_objects):\n        # Create empty image buffer\n        image = np.zeros((self.height, self.width, 3), dtype=np.uint8)\n\n        for y in range(self.height):\n            for x in range(self.width):\n                # Convert pixel coordinates to camera coordinates\n                x_norm = (x - self.cx) / self.fx\n                y_norm = (y - self.cy) / self.fy\n\n                # Apply distortion correction\n                x_distorted, y_distorted = self.apply_distortion(x_norm, y_norm)\n\n                # Ray direction in camera space\n                ray_direction = Vector3(x_distorted, y_distorted, 1.0)\n\n                # Transform to world space\n                world_ray = camera_pose.transform_vector(ray_direction)\n\n                # Find intersection with scene\n                color = self.ray_trace_intersection(world_ray, scene_objects)\n                image[y, x] = color\n\n        # Add noise and artifacts\n        image = self.add_sensor_noise(image)\n        image = self.add_motion_blur(image, camera_pose.velocity)\n\n        return image\n\n    def apply_distortion(self, x, y):\n        # Apply radial and tangential distortion\n        r_squared = x*x + y*y\n        radial_distortion = 1 + self.distortion[0]*r_squared + self.distortion[1]*r_squared*r_squared + self.distortion[4]*r_squared*r_squared*r_squared\n        tangential_x = 2*self.distortion[2]*x*y + self.distortion[3]*(r_squared + 2*x*x)\n        tangential_y = self.distortion[2]*(r_squared + 2*y*y) + 2*self.distortion[3]*x*y\n\n        x_corrected = x*radial_distortion + tangential_x\n        y_corrected = y*radial_distortion + tangential_y\n\n        return x_corrected, y_corrected\n"})}),"\n",(0,o.jsx)(n.h3,{id:"imu-simulation",children:"IMU Simulation"}),"\n",(0,o.jsx)(n.p,{children:"IMU sensors in simulation must model the physics of acceleration and rotation measurement:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"Pseudocode: IMU Simulation\nclass IMUSimulator:\n    def __init__(self):\n        self.accel_noise = GaussianNoise(std_dev=0.01)\n        self.gyro_noise = GaussianNoise(std_dev=0.001)\n        self.mag_noise = GaussianNoise(std_dev=0.0001)\n\n        self.accel_bias = Vector3(0, 0, 0)\n        self.gyro_bias = Vector3(0, 0, 0)\n        self.mag_bias = Vector3(0, 0, 0)\n\n    def simulate_imu_data(self, robot_state, dt):\n        # Get true acceleration from robot dynamics\n        true_acceleration = robot_state.linear_acceleration\n        true_angular_velocity = robot_state.angular_velocity\n\n        # Add gravity to acceleration (IMU measures proper acceleration + gravity)\n        gravity_vector = Vector3(0, 0, 9.81)\n        measured_accel = true_acceleration + robot_state.orientation.rotate(gravity_vector)\n\n        # Add sensor noise and bias\n        noisy_accel = measured_accel + self.accel_bias + self.accel_noise.sample_vector3()\n        noisy_gyro = true_angular_velocity + self.gyro_bias + self.gyro_noise.sample_vector3()\n        noisy_mag = self.get_magnetic_field() + self.mag_bias + self.mag_noise.sample_vector3()\n\n        # Update bias over time (drift)\n        self.update_bias_drift(dt)\n\n        return {\n            'linear_acceleration': noisy_accel,\n            'angular_velocity': noisy_gyro,\n            'magnetic_field': noisy_mag,\n            'orientation': self.integrate_orientation(noisy_gyro, dt)\n        }\n\n    def integrate_orientation(self, angular_velocity, dt):\n        # Integrate angular velocity to get orientation (simplified)\n        delta_quaternion = self.angular_velocity_to_quaternion(angular_velocity, dt)\n        current_orientation = self.current_orientation * delta_quaternion\n        return current_orientation.normalize()\n"})}),"\n",(0,o.jsx)(n.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,o.jsx)(n.p,{children:"By the end of this week, students should be able to:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Configure physics simulation environments"})," - Set up realistic physics parameters, gravity, and environmental forces for humanoid robot simulation."]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Build complex simulation worlds"})," - Create detailed environments using SDF format with appropriate terrain, obstacles, and dynamic elements."]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Simulate realistic sensor data"})," - Implement accurate models for LiDAR, camera, and IMU sensors that reflect real-world behavior and limitations."]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Model contact forces and collisions"})," - Understand and implement collision detection and response systems that accurately simulate physical interactions."]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Design transferable simulation scenarios"})," - Create simulation environments and tasks that enable effective Sim2Real transfer for humanoid robot applications."]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h1,{id:"week-7-unity-visualization-concepts",children:"Week 7: Unity Visualization Concepts"}),"\n",(0,o.jsx)(n.h2,{id:"introduction-1",children:"Introduction"}),"\n",(0,o.jsx)(n.p,{children:"Unity provides a powerful platform for high-fidelity visualization and human-robot interaction in digital twin environments. While Gazebo excels at physics simulation, Unity offers superior rendering capabilities, realistic lighting, and immersive visualization that are essential for human-robot interaction design, teleoperation interfaces, and advanced visualization of robot behaviors. This week explores how Unity integrates with robotic systems to create compelling visual experiences and user interfaces for humanoid robotics applications."}),"\n",(0,o.jsx)(n.h2,{id:"unity-architecture-for-robotics",children:"Unity Architecture for Robotics"}),"\n",(0,o.jsx)(n.p,{children:"Unity's architecture provides a flexible framework for creating realistic 3D environments that can be synchronized with robotic simulation and real-world data."}),"\n",(0,o.jsx)(n.h3,{id:"unity-scene-structure",children:"Unity Scene Structure"}),"\n",(0,o.jsx)(n.p,{children:"Unity organizes 3D content using a hierarchical scene structure:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"GameObjects"}),": The fundamental objects in a Unity scene"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Components"}),": Attachable behaviors and properties (Transform, Mesh, Scripts)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Scenes"}),": Collections of GameObjects forming complete environments"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Prefabs"}),": Reusable object templates for consistent robot models"]}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'Pseudocode: Unity Robot Model Structure\nclass RobotModel:\n    def __init__(self):\n        self.root_object = GameObject("Robot")\n        self.joints = {}\n        self.links = {}\n        self.sensors = {}\n        self.controllers = {}\n\n    def create_link(self, name, mass, inertia, visual_mesh):\n        link_object = GameObject(name)\n        link_object.AddComponent(Rigidbody)\n        link_object.GetComponent(Rigidbody).mass = mass\n        link_object.GetComponent(Rigidbody).inertia = inertia\n        link_object.AddComponent(MeshRenderer).mesh = visual_mesh\n        link_object.AddComponent(MeshCollider).sharedMesh = visual_mesh\n\n        return link_object\n\n    def create_joint(self, name, joint_type, parent_link, child_link, limits):\n        joint_object = GameObject(name)\n        joint_object.transform.parent = parent_link.transform\n\n        if joint_type == "revolute":\n            joint_component = joint_object.AddComponent(HingeJoint)\n            joint_component.axis = limits.axis\n            joint_component.limits = JointLimits(\n                min=limits.min_angle,\n                max=limits.max_angle,\n                bounciness=limits.bounce\n            )\n        elif joint_type == "prismatic":\n            joint_component = joint_object.AddComponent(ConfigurableJoint)\n            # Configure prismatic joint parameters\n\n        return joint_object\n'})}),"\n",(0,o.jsx)(n.h3,{id:"coordinate-systems-and-transformations",children:"Coordinate Systems and Transformations"}),"\n",(0,o.jsx)(n.p,{children:"Unity uses a left-handed coordinate system that must be carefully converted when integrating with ROS and other robotics frameworks:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Unity"}),": X-right, Y-up, Z-forward (left-handed)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"ROS"}),": X-forward, Y-left, Z-up (right-handed)"]}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"Pseudocode: Coordinate System Conversion\nclass CoordinateConverter:\n    def __init__(self):\n        # Transformation matrix from ROS to Unity\n        self.ros_to_unity = Matrix4x4([\n            [0, 0, 1, 0],\n            [-1, 0, 0, 0],\n            [0, 1, 0, 0],\n            [0, 0, 0, 1]\n        ])\n\n    def ros_to_unity_position(self, ros_position):\n        # Convert ROS position (x, y, z) to Unity position\n        ros_vec = Vector4(ros_position.x, ros_position.y, ros_position.z, 1)\n        unity_vec = self.ros_to_unity * ros_vec\n        return Vector3(unity_vec.x, unity_vec.y, unity_vec.z)\n\n    def ros_to_unity_orientation(self, ros_quaternion):\n        # Convert ROS quaternion to Unity quaternion\n        # Apply coordinate system transformation\n        unity_quat = Quaternion(\n            w=ros_quaternion.w,\n            x=-ros_quaternion.z,\n            y=ros_quaternion.x,\n            z=ros_quaternion.y\n        )\n        return unity_quat.normalize()\n\n    def unity_to_ros_transform(self, unity_transform):\n        # Convert Unity transform to ROS transform\n        ros_position = self.unity_to_ros_position(unity_transform.position)\n        ros_orientation = self.unity_to_ros_orientation(unity_transform.rotation)\n        return ROS_Transform(ros_position, ros_orientation)\n"})}),"\n",(0,o.jsx)(n.h2,{id:"high-fidelity-rendering-and-materials",children:"High-Fidelity Rendering and Materials"}),"\n",(0,o.jsx)(n.p,{children:"Unity's rendering pipeline enables photorealistic visualization that's crucial for human-robot interaction and teleoperation scenarios."}),"\n",(0,o.jsx)(n.h3,{id:"physically-based-rendering-pbr",children:"Physically-Based Rendering (PBR)"}),"\n",(0,o.jsx)(n.p,{children:"PBR materials provide realistic surface properties that respond appropriately to lighting:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'Pseudocode: PBR Material System\nclass PBRMaterialSystem:\n    def __init__(self):\n        self.material_library = {}\n        self.shader_properties = {}\n\n    def create_robot_material(self, material_type):\n        if material_type == "metal":\n            material = Material(StandardShader)\n            material.SetTexture("_MainTex", self.load_texture("robot_metal_albedo.png"))\n            material.SetTexture("_BumpMap", self.load_texture("robot_metal_normal.png"))\n            material.SetTexture("_MetallicGlossMap", self.load_texture("robot_metal_metallic.png"))\n            material.SetFloat("_Metallic", 0.9)  # High metallic value\n            material.SetFloat("_Smoothness", 0.8)  # High smoothness\n            material.SetColor("_Color", Color(0.7, 0.7, 0.8))  # Metallic blue-gray\n\n        elif material_type == "plastic":\n            material = Material(StandardShader)\n            material.SetTexture("_MainTex", self.load_texture("robot_plastic_albedo.png"))\n            material.SetFloat("_Metallic", 0.1)  # Low metallic value\n            material.SetFloat("_Smoothness", 0.3)  # Medium smoothness\n            material.SetColor("_Color", Color(0.3, 0.6, 0.9))  # Plastic blue\n\n        return material\n\n    def apply_material_properties(self, game_object, material_properties):\n        # Apply wear patterns, scratches, and aging effects\n        material = game_object.GetComponent(MeshRenderer).material\n        material.SetTexture("_DetailMask", self.create_wear_pattern(material_properties))\n        material.SetFloat("_OcclusionStrength", material_properties.occlusion_strength)\n        material.SetFloat("_Parallax", material_properties.parallax_scale)\n'})}),"\n",(0,o.jsx)(n.h3,{id:"advanced-lighting-systems",children:"Advanced Lighting Systems"}),"\n",(0,o.jsx)(n.p,{children:"Realistic lighting is crucial for creating immersive robot environments:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'Pseudocode: Lighting System\nclass AdvancedLighting:\n    def __init__(self):\n        self.light_probes = []\n        self.reflection_probes = []\n        self.global_illumination = GlobalIllumination()\n\n    def setup_environment_lighting(self, environment_type):\n        if environment_type == "indoor_office":\n            # Setup directional light (simulating windows/overhead lights)\n            sun_light = GameObject("SunLight")\n            sun_light.AddComponent(Light)\n            sun_light.GetComponent(Light).type = LightType.Directional\n            sun_light.GetComponent(Light).color = Color(0.98, 0.92, 0.85)  # Warm white\n            sun_light.GetComponent(Light).intensity = 1.2\n            sun_light.transform.rotation = Quaternion.Euler(50, -30, 0)\n\n            # Add ambient lighting\n            RenderSettings.ambientMode = UnityEngine.Rendering.AmbientMode.Trilight\n            RenderSettings.ambientSkyColor = Color(0.4, 0.5, 0.7)\n            RenderSettings.ambientEquatorColor = Color(0.2, 0.3, 0.5)\n            RenderSettings.ambientGroundColor = Color(0.1, 0.15, 0.25)\n\n            # Setup reflection probes for metallic surfaces\n            self.create_reflection_probes_for_environment()\n\n        elif environment_type == "outdoor":\n            # More intense directional light (sun)\n            sun_light = GameObject("Sun")\n            sun_light.AddComponent(Light)\n            sun_light.GetComponent(Light).type = LightType.Directional\n            sun_light.GetComponent(Light).color = Color(1.0, 0.95, 0.8)\n            sun_light.GetComponent(Light).intensity = 1.5\n            sun_light.transform.rotation = Quaternion.Euler(60, 120, 0)\n\n            # Skybox for realistic background\n            RenderSettings.skybox = self.load_skybox_material("outdoor_skybox")\n\n    def create_light_probes(self, positions):\n        # Create light probe volumes for accurate lighting on moving objects\n        for pos in positions:\n            probe = GameObject("LightProbe")\n            probe.transform.position = pos\n            probe.AddComponent(LightProbeGroup)\n            self.light_probes.append(probe)\n\n    def setup_global_illumination(self, environment_bounds):\n        # Configure baked lighting for static objects\n        self.global_illumination.bake_static_objects = True\n        self.global_illumination.lightmap_resolution = 64  # texels per unit\n        self.global_illumination.indirect_intensity = 1.5\n        self.global_illumination.environment_lighting = True\n'})}),"\n",(0,o.jsx)(n.h2,{id:"human-robot-interaction-design",children:"Human-Robot Interaction Design"}),"\n",(0,o.jsx)(n.p,{children:"Unity's interface capabilities make it ideal for designing human-robot interaction systems, including teleoperation interfaces and augmented reality applications."}),"\n",(0,o.jsx)(n.h3,{id:"user-interface-systems",children:"User Interface Systems"}),"\n",(0,o.jsx)(n.p,{children:"Creating intuitive interfaces for robot control and monitoring:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'Pseudocode: HRI Interface System\nclass HumanRobotInterface:\n    def __init__(self):\n        self.canvas = Canvas()\n        self.robot_status_panel = self.create_robot_status_panel()\n        self.control_interface = self.create_control_interface()\n        self.camera_system = self.create_camera_system()\n\n    def create_robot_status_panel(self):\n        # Create status panel showing robot vitals\n        status_panel = GameObject("RobotStatusPanel")\n        status_panel.AddComponent(RectTransform)\n\n        # Battery indicator\n        battery_bar = self.create_progress_bar("Battery", Color.green, Color.red)\n        battery_text = self.create_text_element("Battery: 85%", 16)\n\n        # Joint status display\n        joint_status_container = GameObject("JointStatusContainer")\n        for joint_name in self.robot_joints:\n            joint_display = self.create_joint_status_display(joint_name)\n            joint_status_container.AddChild(joint_display)\n\n        return status_panel\n\n    def create_control_interface(self):\n        # Create teleoperation controls\n        control_panel = GameObject("ControlPanel")\n\n        # Movement controls\n        movement_controls = self.create_movement_joystick()\n\n        # Action buttons\n        action_buttons = []\n        for action in ["walk", "grasp", "speak", "emergency_stop"]:\n            button = self.create_action_button(action)\n            action_buttons.append(button)\n\n        # Gesture recognition interface\n        gesture_recognition = self.create_gesture_interface()\n\n        return control_panel\n\n    def create_camera_system(self):\n        # Multiple camera views for comprehensive robot monitoring\n        cameras = {}\n\n        # Main view - third person follow\n        main_camera = self.create_follow_camera("MainCamera", offset=Vector3(-3, 2, 0))\n        cameras["main"] = main_camera\n\n        # First person view from robot perspective\n        fpv_camera = self.create_robot_camera("FPVCamera", mount_point="head")\n        cameras["fpv"] = fpv_camera\n\n        # Sensor view cameras (LiDAR, depth camera simulation)\n        sensor_cameras = self.create_sensor_cameras()\n        cameras.update(sensor_cameras)\n\n        return cameras\n'})}),"\n",(0,o.jsx)(n.h3,{id:"gesture-recognition-and-input-systems",children:"Gesture Recognition and Input Systems"}),"\n",(0,o.jsx)(n.p,{children:"Advanced input systems for natural human-robot interaction:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'Pseudocode: Gesture Recognition System\nclass GestureRecognition:\n    def __init__(self):\n        self.tracked_joints = []\n        self.gesture_templates = {}\n        self.current_gesture_buffer = []\n        self.recognition_threshold = 0.8\n\n    def initialize_gesture_system(self):\n        # Setup camera for gesture recognition\n        self.camera = GameObject("GestureCamera")\n        self.camera.AddComponent(WebCamTexture)\n        self.camera.AddComponent(ImageProcessingComponent)\n\n        # Initialize gesture templates\n        self.load_gesture_templates()\n\n    def recognize_gesture(self, hand_positions):\n        # Process hand tracking data and match to templates\n        current_gesture = self.extract_gesture_features(hand_positions)\n        self.current_gesture_buffer.append(current_gesture)\n\n        # Keep only recent gesture data\n        if len(self.current_gesture_buffer) > 10:\n            self.current_gesture_buffer.pop(0)\n\n        # Match against templates\n        best_match = None\n        best_score = 0\n\n        for template_name, template in self.gesture_templates.items():\n            score = self.match_gesture_to_template(\n                self.current_gesture_buffer,\n                template\n            )\n            if score > best_score:\n                best_score = score\n                best_match = template_name\n\n        if best_score > self.recognition_threshold:\n            return best_match\n        return None\n\n    def extract_gesture_features(self, positions):\n        # Extract meaningful features from hand positions\n        features = {}\n\n        # Calculate joint angles\n        features["finger_angles"] = self.calculate_finger_angles(positions)\n\n        # Calculate hand velocity\n        features["hand_velocity"] = self.calculate_velocity(positions)\n\n        # Calculate spatial relationships\n        features["finger_distances"] = self.calculate_finger_distances(positions)\n\n        return features\n'})}),"\n",(0,o.jsx)(n.h2,{id:"real-time-simulation-synchronization",children:"Real-time Simulation Synchronization"}),"\n",(0,o.jsx)(n.p,{children:"Synchronizing Unity visualization with real-time robotic systems requires careful timing and data management."}),"\n",(0,o.jsx)(n.h3,{id:"ros-integration",children:"ROS Integration"}),"\n",(0,o.jsx)(n.p,{children:"Connecting Unity to ROS systems for real-time data exchange:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"Pseudocode: ROS Integration System\nclass UnityROSBridge:\n    def __init__(self):\n        self.ros_node = None\n        self.subscribers = {}\n        self.publishers = {}\n        self.synchronization_manager = SynchronizationManager()\n\n    def initialize_ros_connection(self):\n        # Initialize ROS connection\n        self.ros_node = rclpy.create_node('unity_bridge')\n\n        # Create subscribers for robot data\n        self.subscribers['joint_states'] = self.ros_node.create_subscription(\n            sensor_msgs.msg.JointState,\n            '/joint_states',\n            self.joint_state_callback,\n            10\n        )\n\n        self.subscribers['robot_pose'] = self.ros_node.create_subscription(\n            geometry_msgs.msg.PoseStamped,\n            '/robot_pose',\n            self.robot_pose_callback,\n            10\n        )\n\n        # Create publishers for Unity commands\n        self.publishers['unity_commands'] = self.ros_node.create_publisher(\n            std_msgs.msg.String,\n            '/unity_commands',\n            10\n        )\n\n    def joint_state_callback(self, msg):\n        # Update Unity robot model with joint states\n        for i, joint_name in enumerate(msg.name):\n            if joint_name in self.robot_model.joints:\n                joint_angle = msg.position[i]\n                self.update_joint_in_unity(joint_name, joint_angle)\n\n    def robot_pose_callback(self, msg):\n        # Update Unity robot position and orientation\n        unity_position = self.ros_to_unity_position(msg.pose.position)\n        unity_rotation = self.ros_to_unity_orientation(msg.pose.orientation)\n\n        self.robot_model.root_object.transform.position = unity_position\n        self.robot_model.root_object.transform.rotation = unity_rotation\n\n    def synchronize_simulation(self, unity_time, ros_time):\n        # Ensure Unity and ROS simulation are synchronized\n        time_difference = abs(unity_time - ros_time)\n\n        if time_difference > 0.1:  # 100ms threshold\n            # Resynchronize by interpolating between states\n            target_time = max(unity_time, ros_time)\n            self.interpolate_robot_state(target_time)\n\n        # Maintain consistent frame rate\n        self.maintain_frame_rate()\n"})}),"\n",(0,o.jsx)(n.h3,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,o.jsx)(n.p,{children:"Optimizing Unity for real-time robotics applications:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'Pseudocode: Performance Optimization System\nclass PerformanceOptimizer:\n    def __init__(self):\n        self.lod_system = LODSystem()\n        self.occlusion_culling = OcclusionCulling()\n        self.object_pooling = ObjectPooling()\n        self.rendering_pipeline = RenderingPipeline()\n\n    def optimize_robot_rendering(self, robot_model):\n        # Apply Level of Detail (LOD) to robot components\n        for component in robot_model.get_components():\n            lod_group = component.AddComponent(LODGroup)\n            lods = [\n                LOD(0.0, [self.get_high_detail_mesh(component)]),  # High detail\n                LOD(0.01, [self.get_medium_detail_mesh(component)]),  # Medium detail\n                LOD(0.05, [self.get_low_detail_mesh(component)])  # Low detail\n            ]\n            lod_group.SetLODs(lods)\n\n        # Enable occlusion culling for robot components\n        self.occlusion_culling.add_static_objects(robot_model.get_environment_objects())\n\n    def implement_object_pooling(self):\n        # Pre-instantiate objects to avoid runtime allocation\n        for obj_type in ["laser_points", "particles", "effects"]:\n            pool = []\n            for i in range(100):  # Pre-allocate 100 objects\n                obj = self.instantiate_object(obj_type)\n                obj.SetActive(False)  # Inactive in pool\n                pool.append(obj)\n            self.object_pooling.register_pool(obj_type, pool)\n\n    def optimize_rendering_pipeline(self):\n        # Configure rendering for robotics applications\n        self.rendering_pipeline.use_forward_rendering = True  # Better for real-time\n        self.rendering_pipeline.shadow_distance = 50.0  # Optimize shadow rendering\n        self.rendering_pipeline.texture_resolution = 1024  # Balance quality/performance\n        self.rendering_pipeline.occlusion_culling = True\n        self.rendering_pipeline.dynamic_batching = True\n        self.rendering_pipeline.static_batching = True\n'})}),"\n",(0,o.jsx)(n.h2,{id:"augmented-reality-integration",children:"Augmented Reality Integration"}),"\n",(0,o.jsx)(n.p,{children:"Unity's AR capabilities enable overlaying robot information onto real-world environments:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"Pseudocode: AR Integration System\nclass ARIntegration:\n    def __init__(self):\n        self.ar_session = ARSession()\n        self.robot_visualization = RobotARVisualization()\n        self.spatial_mapping = SpatialMapping()\n\n    def initialize_ar_session(self):\n        # Initialize AR session with camera and plane detection\n        self.ar_session.Initialize()\n\n        # Setup plane detection for placing robot visualizations\n        self.ar_session.plane_detection = PlaneDetection.Horizontal\n        self.ar_session.plane_detection_callback = self.on_plane_detected\n\n        # Setup image tracking for robot markers\n        self.ar_session.image_tracking = True\n        self.ar_session.tracked_images = self.load_robot_markers()\n\n    def visualize_robot_in_ar(self, robot_pose, robot_state):\n        # Create AR visualization of robot at real-world location\n        robot_ar_object = self.robot_visualization.create_robot_model()\n\n        # Position robot in AR space\n        ar_position = self.transform_ros_to_ar(robot_pose.position)\n        ar_rotation = self.transform_ros_to_ar(robot_pose.orientation)\n\n        robot_ar_object.transform.position = ar_position\n        robot_ar_object.transform.rotation = ar_rotation\n\n        # Update robot state visualization\n        self.update_robot_state_visualization(robot_ar_object, robot_state)\n\n        # Add interaction affordances\n        self.add_interaction_handles(robot_ar_object)\n\n    def create_robot_state_visualization(self, robot_object, state):\n        # Visualize robot state information in AR\n        # Joint angles as overlay indicators\n        for joint_name, angle in state.joint_angles.items():\n            angle_indicator = self.create_angle_indicator(joint_name, angle)\n            self.attach_to_joint(robot_object, joint_name, angle_indicator)\n\n        # Sensor data visualization\n        if hasattr(state, 'lidar_data'):\n            lidar_visualization = self.create_lidar_visualization(state.lidar_data)\n            self.attach_sensor_visualization(robot_object, lidar_visualization)\n\n        # Planning and navigation visualization\n        if hasattr(state, 'planned_path'):\n            path_visualization = self.create_path_visualization(state.planned_path)\n            self.attach_path_visualization(robot_object, path_visualization)\n"})}),"\n",(0,o.jsx)(n.h2,{id:"learning-outcomes-1",children:"Learning Outcomes"}),"\n",(0,o.jsx)(n.p,{children:"By the end of this week, students should be able to:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Design Unity scenes for robotics"})," - Create hierarchical robot models with appropriate coordinate system conversions and component structures."]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Implement high-fidelity visualization"})," - Apply physically-based rendering, advanced lighting, and realistic materials to create compelling robot visualizations."]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Develop human-robot interaction interfaces"})," - Create intuitive user interfaces, gesture recognition systems, and teleoperation controls in Unity."]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Integrate with real-time systems"})," - Connect Unity to ROS and other robotic systems for synchronized visualization and control."]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Optimize performance for real-time applications"})," - Apply LOD systems, object pooling, and rendering optimization techniques for smooth robot visualization."]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.hr,{})]})}function _(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>a});var t=i(6540);const o={},s=t.createContext(o);function r(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);