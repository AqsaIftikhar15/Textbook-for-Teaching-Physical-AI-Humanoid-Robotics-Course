"use strict";(globalThis.webpackChunkphysical_ai_robotics_book=globalThis.webpackChunkphysical_ai_robotics_book||[]).push([[4933],{3628:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>s,default:()=>p,frontMatter:()=>r,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"module-3-ai-brain/week-11-12-humanoid-dev","title":"Humanoid Robot Development","description":"Kinematics, dynamics, and locomotion for humanoid robots","source":"@site/docs/module-3-ai-brain/week-11-12-humanoid-dev.md","sourceDirName":"module-3-ai-brain","slug":"/module-3-ai-brain/week-11-12-humanoid-dev","permalink":"/Textbook-for-Teaching-Physical-AI-Humanoid-Robotics-Course/docs/module-3-ai-brain/week-11-12-humanoid-dev","draft":false,"unlisted":false,"editUrl":"https://github.com/AqsaIftikhar15/Textbook-for-Teaching-Physical-AI-Humanoid-Robotics-Course/docs/module-3-ai-brain/week-11-12-humanoid-dev.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"title":"Humanoid Robot Development","sidebar_position":2,"description":"Kinematics, dynamics, and locomotion for humanoid robots"},"sidebar":"tutorialSidebar","previous":{"title":"NVIDIA Isaac Platform","permalink":"/Textbook-for-Teaching-Physical-AI-Humanoid-Robotics-Course/docs/module-3-ai-brain/week-8-10-isaac-platform"},"next":{"title":"Conversational Robotics Overview","permalink":"/Textbook-for-Teaching-Physical-AI-Humanoid-Robotics-Course/docs/module-3-ai-brain/week-13-conversational-robotics"}}');var i=t(4848),a=t(8453);const r={title:"Humanoid Robot Development",sidebar_position:2,description:"Kinematics, dynamics, and locomotion for humanoid robots"},s="Week 11: Humanoid Kinematics & Dynamics",l={},c=[{value:"Introduction",id:"introduction",level:2},{value:"Forward and Inverse Kinematics",id:"forward-and-inverse-kinematics",level:2},{value:"Forward Kinematics",id:"forward-kinematics",level:3},{value:"Inverse Kinematics",id:"inverse-kinematics",level:3},{value:"Dynamics and Motion Equations",id:"dynamics-and-motion-equations",level:2},{value:"Lagrangian Dynamics",id:"lagrangian-dynamics",level:3},{value:"Recursive Newton-Euler Algorithm",id:"recursive-newton-euler-algorithm",level:3},{value:"Center of Mass and Stability Analysis",id:"center-of-mass-and-stability-analysis",level:2},{value:"Center of Mass Computation",id:"center-of-mass-computation",level:3},{value:"Walking Pattern Generation",id:"walking-pattern-generation",level:2},{value:"Capture Point and Walking Control",id:"capture-point-and-walking-control",level:3},{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Introduction",id:"introduction-1",level:2},{value:"Bipedal Locomotion Control",id:"bipedal-locomotion-control",level:2},{value:"Balance Control Systems",id:"balance-control-systems",level:3},{value:"Walking Pattern Generation and Execution",id:"walking-pattern-generation-and-execution",level:3},{value:"Walking Stability and Disturbance Rejection",id:"walking-stability-and-disturbance-rejection",level:3},{value:"Dexterous Manipulation",id:"dexterous-manipulation",level:2},{value:"Grasp Planning and Execution",id:"grasp-planning-and-execution",level:3},{value:"Multi-Modal Manipulation",id:"multi-modal-manipulation",level:3},{value:"Human-Robot Interaction",id:"human-robot-interaction",level:2},{value:"Social Interaction Framework",id:"social-interaction-framework",level:3},{value:"Collaborative Task Execution",id:"collaborative-task-execution",level:3},{value:"Learning Outcomes",id:"learning-outcomes-1",level:2}];function _(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,a.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.header,{children:(0,i.jsx)(e.h1,{id:"week-11-humanoid-kinematics--dynamics",children:"Week 11: Humanoid Kinematics & Dynamics"})}),"\n",(0,i.jsx)(e.h2,{id:"introduction",children:"Introduction"}),"\n",(0,i.jsx)(e.p,{children:"Humanoid kinematics and dynamics form the mathematical foundation for understanding and controlling the complex movements of human-like robots. This week explores the geometric relationships (kinematics) and force interactions (dynamics) that govern humanoid robot motion. Understanding these principles is essential for developing stable, efficient, and natural-looking robot behaviors that mimic human movement patterns."}),"\n",(0,i.jsx)(e.h2,{id:"forward-and-inverse-kinematics",children:"Forward and Inverse Kinematics"}),"\n",(0,i.jsx)(e.p,{children:"Kinematics describes the geometric relationships between joint angles and end-effector positions without considering forces. Forward kinematics computes end-effector positions from joint angles, while inverse kinematics solves for joint angles given desired end-effector positions."}),"\n",(0,i.jsx)(e.h3,{id:"forward-kinematics",children:"Forward Kinematics"}),"\n",(0,i.jsx)(e.p,{children:"Forward kinematics transforms joint space coordinates to Cartesian space:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"Pseudocode: Forward Kinematics Implementation\nclass ForwardKinematics:\n    def __init__(self, robot_model):\n        self.robot_model = robot_model\n        self.dh_parameters = robot_model.get_dh_parameters()\n\n    def compute_forward_kinematics(self, joint_angles):\n        # Initialize transformation matrix as identity\n        transform = np.eye(4)\n\n        for i, (joint_angle, dh_params) in enumerate(zip(joint_angles, self.dh_parameters)):\n            # Compute transformation matrix for this joint\n            a, alpha, d, theta_offset = dh_params\n            theta = joint_angle + theta_offset\n\n            # DH transformation matrix\n            cos_theta = np.cos(theta)\n            sin_theta = np.sin(theta)\n            cos_alpha = np.cos(alpha)\n            sin_alpha = np.sin(alpha)\n\n            joint_transform = np.array([\n                [cos_theta, -sin_theta * cos_alpha, sin_theta * sin_alpha, a * cos_theta],\n                [sin_theta, cos_theta * cos_alpha, -cos_theta * sin_alpha, a * sin_theta],\n                [0, sin_alpha, cos_alpha, d],\n                [0, 0, 0, 1]\n            ])\n\n            # Combine with previous transformations\n            transform = np.dot(transform, joint_transform)\n\n        return transform  # 4x4 homogeneous transformation matrix\n\n    def compute_end_effector_pose(self, joint_angles, chain_name):\n        # Compute pose for specific kinematic chain (arm, leg, etc.)\n        if chain_name == 'right_arm':\n            chain_joints = self.robot_model.right_arm_joints\n        elif chain_name == 'left_arm':\n            chain_joints = self.robot_model.left_arm_joints\n        elif chain_name == 'right_leg':\n            chain_joints = self.robot_model.right_leg_joints\n        elif chain_name == 'left_leg':\n            chain_joints = self.robot_model.left_leg_joints\n\n        # Extract relevant joint angles\n        relevant_angles = [joint_angles[i] for i in chain_joints]\n\n        # Compute forward kinematics for the chain\n        final_transform = self.compute_forward_kinematics(relevant_angles)\n\n        # Extract position and orientation\n        position = final_transform[:3, 3]\n        orientation_matrix = final_transform[:3, :3]\n\n        # Convert rotation matrix to quaternion\n        quaternion = self.rotation_matrix_to_quaternion(orientation_matrix)\n\n        return {\n            'position': position,\n            'orientation': quaternion,\n            'transform': final_transform\n        }\n\n    def compute_jacobian(self, joint_angles, end_effector_link):\n        # Compute geometric Jacobian matrix\n        jacobian = np.zeros((6, len(joint_angles)))  # [linear, angular]\n\n        # Get end-effector position and orientation\n        ee_pose = self.compute_end_effector_pose(joint_angles, end_effector_link)\n        ee_position = ee_pose['position']\n\n        # For each joint, compute its contribution to end-effector velocity\n        current_transform = np.eye(4)\n\n        for i, joint_angle in enumerate(joint_angles):\n            # Compute transformation up to this joint\n            joint_transform = self.compute_single_joint_transform(joint_angle, i)\n            current_transform = np.dot(current_transform, joint_transform)\n\n            # Get joint position and axis in global frame\n            joint_position = current_transform[:3, 3]\n            joint_axis = current_transform[:3, 2]  # z-axis of joint frame\n\n            # Linear velocity contribution\n            r = ee_position - joint_position\n            jacobian[:3, i] = np.cross(joint_axis, r)\n\n            # Angular velocity contribution\n            jacobian[3:, i] = joint_axis\n\n        return jacobian\n"})}),"\n",(0,i.jsx)(e.h3,{id:"inverse-kinematics",children:"Inverse Kinematics"}),"\n",(0,i.jsx)(e.p,{children:"Inverse kinematics solves for joint angles that achieve desired end-effector positions:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"Pseudocode: Inverse Kinematics Implementation\nclass InverseKinematics:\n    def __init__(self, robot_model):\n        self.robot_model = robot_model\n        self.forward_kinematics = ForwardKinematics(robot_model)\n        self.optimizer = OptimizationSolver()\n\n    def solve_analytical_ik(self, target_pose, chain_name):\n        # Analytical solution for simple kinematic chains (e.g., 6-DOF arm)\n        if chain_name == 'right_arm':\n            return self.solve_arm_ik(target_pose)\n        elif chain_name == 'left_arm':\n            return self.solve_arm_ik(target_pose)  # Similar to right arm\n        else:\n            # Fall back to numerical method for complex chains\n            return self.solve_numerical_ik(target_pose, chain_name)\n\n    def solve_arm_ik(self, target_pose):\n        # Analytical solution for 6-DOF arm\n        # Extract target position and orientation\n        target_pos = target_pose['position']\n        target_rot = target_pose['orientation']\n\n        # Step 1: Find shoulder position (wrist center position)\n        # Move back from target by wrist-to-elbow distance\n        wrist_to_elbow = self.robot_model.wrist_to_elbow_length\n        target_orientation_matrix = self.quaternion_to_rotation_matrix(target_rot)\n        approach_vector = target_orientation_matrix[:, 2]  # z-axis (approach direction)\n\n        shoulder_pos = target_pos - approach_vector * wrist_to_elbow\n\n        # Step 2: Solve for first 3 joints (position)\n        theta1 = np.arctan2(shoulder_pos[1], shoulder_pos[0])\n\n        # Distance from base to shoulder in x-y plane\n        r = np.sqrt(shoulder_pos[0]**2 + shoulder_pos[1]**2)\n        d = shoulder_pos[2] - self.robot_model.shoulder_height\n\n        # Law of cosines for shoulder and elbow joints\n        l1 = self.robot_model.upper_arm_length\n        l2 = self.robot_model.forearm_length\n\n        cos_theta3 = (r**2 + d**2 - l1**2 - l2**2) / (2 * l1 * l2)\n        cos_theta3 = np.clip(cos_theta3, -1, 1)  # Handle numerical errors\n        theta3 = np.arccos(cos_theta3)\n\n        # Determine elbow configuration (up or down)\n        theta3 = theta3 if self.robot_model.elbow_up else -theta3\n\n        k1 = l1 + l2 * np.cos(theta3)\n        k2 = l2 * np.sin(theta3)\n\n        theta2 = np.arctan2(d, r) - np.arctan2(k2, k1)\n\n        # Step 3: Solve for last 3 joints (orientation)\n        # Compute wrist orientation matrix\n        wrist_orientation = self.compute_wrist_orientation(\n            theta1, theta2, theta3, target_rot\n        )\n\n        # Extract Euler angles for wrist joints\n        theta4, theta5, theta6 = self.matrix_to_euler(wrist_orientation)\n\n        return [theta1, theta2, theta3, theta4, theta5, theta6]\n\n    def solve_numerical_ik(self, target_pose, chain_name, initial_guess=None):\n        # Numerical solution using optimization\n        def objective_function(joint_angles):\n            # Compute current end-effector pose\n            current_pose = self.forward_kinematics.compute_end_effector_pose(\n                joint_angles, chain_name\n            )\n\n            # Position error\n            pos_error = np.linalg.norm(\n                current_pose['position'] - target_pose['position']\n            )\n\n            # Orientation error (using quaternion distance)\n            quat_error = self.quaternion_distance(\n                current_pose['orientation'],\n                target_pose['orientation']\n            )\n\n            return pos_error + 0.1 * quat_error  # Weight position more heavily\n\n        # Set up constraints (joint limits)\n        joint_limits = self.robot_model.get_joint_limits(chain_name)\n        bounds = [(lim[0], lim[1]) for lim in joint_limits]\n\n        # Initial guess\n        if initial_guess is None:\n            initial_guess = self.robot_model.get_default_angles(chain_name)\n\n        # Solve optimization problem\n        result = scipy.optimize.minimize(\n            objective_function,\n            initial_guess,\n            method='L-BFGS-B',\n            bounds=bounds,\n            options={'maxiter': 1000}\n        )\n\n        if result.success and result.fun < 0.01:  # Acceptable error threshold\n            return result.x\n        else:\n            # Return best solution found or initial guess\n            return result.x if result.fun < 1.0 else initial_guess\n\n    def solve_redundant_ik(self, target_pose, chain_name, joint_weights=None):\n        # Handle redundant manipulators (more DOF than task space)\n        # Use null-space optimization to achieve secondary objectives\n\n        if joint_weights is None:\n            joint_weights = np.ones(self.robot_model.get_num_joints(chain_name))\n\n        # Primary solution using numerical IK\n        primary_solution = self.solve_numerical_ik(target_pose, chain_name)\n\n        # Compute Jacobian at primary solution\n        jacobian = self.forward_kinematics.compute_jacobian(\n            primary_solution, chain_name\n        )\n\n        # Null space projection\n        # J# = W^(-1) * J^T * (J * W^(-1) * J^T)^(-1)\n        # where W is diagonal matrix of joint weights\n        W = np.diag(joint_weights)\n        J_weighted = np.linalg.solve(W, jacobian.T).T\n        JJT_inv = np.linalg.inv(np.dot(jacobian, J_weighted.T))\n        jacobian_pseudoinverse = np.dot(J_weighted.T, JJT_inv)\n\n        # Null space projector: I - J# * J\n        identity = np.eye(len(primary_solution))\n        null_space_projector = identity - np.dot(jacobian_pseudoinverse, jacobian)\n\n        # Add null space motion for secondary objectives\n        # (e.g., joint centering, obstacle avoidance)\n        null_space_motion = self.compute_null_space_objectives(\n            primary_solution, chain_name\n        )\n\n        final_solution = primary_solution + np.dot(\n            null_space_projector, null_space_motion\n        )\n\n        return final_solution\n"})}),"\n",(0,i.jsx)(e.h2,{id:"dynamics-and-motion-equations",children:"Dynamics and Motion Equations"}),"\n",(0,i.jsx)(e.p,{children:"Robot dynamics describes the relationship between forces, torques, and motion. Understanding dynamics is crucial for controlling robot movements and ensuring stability."}),"\n",(0,i.jsx)(e.h3,{id:"lagrangian-dynamics",children:"Lagrangian Dynamics"}),"\n",(0,i.jsx)(e.p,{children:"The Lagrangian formulation provides a systematic way to derive equations of motion:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"Pseudocode: Lagrangian Dynamics Implementation\nclass LagrangianDynamics:\n    def __init__(self, robot_model):\n        self.robot_model = robot_model\n        self.forward_kinematics = ForwardKinematics(robot_model)\n\n    def compute_mass_matrix(self, joint_positions):\n        # Compute mass matrix M(q) using recursive Newton-Euler algorithm\n        n = len(joint_positions)\n        M = np.zeros((n, n))\n\n        # Get link transformations and velocities\n        transforms = []\n        velocities = []\n\n        for i in range(n):\n            # Compute transform from base to link i\n            transform = self.compute_link_transform(joint_positions, i)\n            transforms.append(transform)\n\n            # Compute link velocity\n            velocity = self.compute_link_velocity(joint_positions, joint_positions, i)\n            velocities.append(velocity)\n\n        # Compute mass matrix elements\n        for i in range(n):\n            for j in range(n):\n                M[i, j] = self.compute_inertia_interaction(\n                    transforms[i], velocities[i],\n                    transforms[j], velocities[j]\n                )\n\n        return M\n\n    def compute_coriolis_matrix(self, joint_positions, joint_velocities):\n        # Compute Coriolis and centrifugal forces matrix C(q, q_dot)\n        n = len(joint_positions)\n        C = np.zeros((n, n))\n\n        # Use Christoffel symbols to compute Coriolis terms\n        M = self.compute_mass_matrix(joint_positions)\n\n        for i in range(n):\n            for j in range(n):\n                c_sum = 0\n                for k in range(n):\n                    # Christoffel symbol of the first kind\n                    christoffel = self.compute_christoffel_symbol(\n                        M, i, j, k, joint_positions\n                    )\n                    c_sum += christoffel * joint_velocities[k]\n\n                C[i, j] = c_sum\n\n        return C\n\n    def compute_gravity_vector(self, joint_positions):\n        # Compute gravity vector G(q)\n        n = len(joint_positions)\n        G = np.zeros(n)\n\n        # Transform gravity vector to each link's frame\n        gravity = np.array([0, 0, -9.81])  # Gravity in world frame\n\n        for i in range(n):\n            # Get link transformation\n            transform = self.compute_link_transform(joint_positions, i)\n\n            # Transform gravity to link frame\n            gravity_link = np.dot(transform[:3, :3].T, gravity)\n\n            # Compute gravity torque contribution\n            link_mass = self.robot_model.links[i].mass\n            link_com = self.robot_model.links[i].center_of_mass\n\n            # Gravity force at center of mass\n            gravity_force = link_mass * gravity_link\n\n            # Torque due to gravity force\n            jacobian = self.compute_link_jacobian(joint_positions, i)\n            gravity_torque = np.dot(jacobian.T[:3, :], gravity_force)\n\n            G += gravity_torque\n\n        return G\n\n    def compute_inverse_dynamics(self, joint_positions, joint_velocities, joint_accelerations):\n        # Compute required joint torques using inverse dynamics\n        # \u03c4 = M(q)q_ddot + C(q, q_dot)q_dot + G(q) + F_ext\n\n        # Mass matrix term\n        M = self.compute_mass_matrix(joint_positions)\n        mass_term = np.dot(M, joint_accelerations)\n\n        # Coriolis and centrifugal term\n        C = self.compute_coriolis_matrix(joint_positions, joint_velocities)\n        coriolis_term = np.dot(C, joint_velocities)\n\n        # Gravity term\n        G = self.compute_gravity_vector(joint_positions)\n\n        # External forces (optional)\n        F_ext = self.compute_external_forces(joint_positions, joint_velocities)\n\n        # Total required torque\n        torques = mass_term + coriolis_term + G + F_ext\n\n        return torques\n\n    def compute_forward_dynamics(self, joint_positions, joint_velocities, joint_torques):\n        # Compute joint accelerations from applied torques\n        # M(q)q_ddot = \u03c4 - C(q, q_dot)q_dot - G(q) - F_ext\n\n        # Compute dynamics matrices\n        M = self.compute_mass_matrix(joint_positions)\n        C = self.compute_coriolis_matrix(joint_positions, joint_velocities)\n        G = self.compute_gravity_vector(joint_positions)\n        F_ext = self.compute_external_forces(joint_positions, joint_velocities)\n\n        # Compute acceleration\n        bias_term = np.dot(C, joint_velocities) + G + F_ext\n        acceleration = np.linalg.solve(M, joint_torques - bias_term)\n\n        return acceleration\n"})}),"\n",(0,i.jsx)(e.h3,{id:"recursive-newton-euler-algorithm",children:"Recursive Newton-Euler Algorithm"}),"\n",(0,i.jsx)(e.p,{children:"The RNEA provides an efficient way to compute inverse dynamics:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"Pseudocode: Recursive Newton-Euler Algorithm\nclass RecursiveNewtonEuler:\n    def __init__(self, robot_model):\n        self.robot_model = robot_model\n\n    def compute_inverse_dynamics_rnea(self, q, q_dot, q_ddot):\n        n = len(q)\n\n        # Initialize variables\n        v = [np.zeros(6) for _ in range(n)]  # Link velocities [angular, linear]\n        a = [np.zeros(6) for _ in range(n)]  # Link accelerations [angular, linear]\n        f = [np.zeros(6) for _ in range(n)]  # Link forces [torque, force]\n        tau = np.zeros(n)  # Joint torques\n\n        # Outward recursion (compute velocities and accelerations)\n        # Initialize base velocity and acceleration\n        v[0][:3] = np.zeros(3)  # Base angular velocity\n        v[0][3:] = np.zeros(3)  # Base linear velocity\n        a[0][:3] = np.zeros(3)  # Base angular acceleration\n        a[0][3:] = np.array([0, 0, -9.81])  # Base linear acceleration (gravity)\n\n        for i in range(n):\n            # Compute joint transformation\n            T_i = self.compute_transform(q[i], i)\n\n            # Compute joint axis in current frame\n            if self.robot_model.joints[i].type == 'revolute':\n                joint_axis = np.array([0, 0, 1])  # z-axis\n                S_i = np.hstack([joint_axis, np.cross(-self.robot_model.joints[i].offset, joint_axis)])\n            else:  # prismatic\n                joint_axis = np.array([0, 0, 1])\n                S_i = np.hstack([np.zeros(3), joint_axis])\n\n            # Link velocity\n            if i == 0:\n                v[i] = S_i * q_dot[i]\n            else:\n                # Transform velocity from parent\n                v[i] = np.dot(T_i, np.dot(self.transform_velocity(v[i-1]), T_i.T))\n                v[i] += S_i * q_dot[i]\n\n            # Link acceleration\n            if i == 0:\n                a[i] = S_i * q_ddot[i] + np.cross(v[i][:3], S_i[3:])\n            else:\n                # Transform acceleration from parent\n                a[i] = np.dot(T_i, np.dot(self.transform_acceleration(a[i-1]), T_i.T))\n                a[i] += S_i * q_ddot[i]\n                a[i] += np.cross(v[i][:3], S_i[3:])\n\n        # Inward recursion (compute forces and torques)\n        for i in range(n-1, -1, -1):\n            # Link force\n            I_i = self.robot_model.links[i].inertia_matrix\n            m_i = self.robot_model.links[i].mass\n            com_i = self.robot_model.links[i].center_of_mass\n\n            # Compute force due to link acceleration\n            f[i] = np.dot(I_i, a[i][:3]) + np.cross(v[i][:3], np.dot(I_i, v[i][:3]))\n            f[i][3:] = m_i * a[i][3:] + np.cross(v[i][:3], m_i * v[i][3:])\n\n            # Joint torque\n            tau[i] = np.dot(S_i, f[i])\n\n            # Propagate force to parent\n            if i > 0:\n                T_i_inv = np.linalg.inv(self.compute_transform(q[i], i))\n                f[i-1] = f[i-1] + np.dot(T_i_inv, np.dot(f[i], T_i_inv.T))\n\n        return tau\n\n    def compute_transform(self, joint_angle, link_index):\n        # Compute homogeneous transformation matrix for joint\n        dh = self.robot_model.dh_parameters[link_index]\n        a, alpha, d, theta_offset = dh\n\n        theta = joint_angle + theta_offset\n\n        transform = np.array([\n            [np.cos(theta), -np.sin(theta)*np.cos(alpha), np.sin(theta)*np.sin(alpha), a*np.cos(theta)],\n            [np.sin(theta), np.cos(theta)*np.cos(alpha), -np.cos(theta)*np.sin(alpha), a*np.sin(theta)],\n            [0, np.sin(alpha), np.cos(alpha), d],\n            [0, 0, 0, 1]\n        ])\n\n        return transform\n"})}),"\n",(0,i.jsx)(e.h2,{id:"center-of-mass-and-stability-analysis",children:"Center of Mass and Stability Analysis"}),"\n",(0,i.jsx)(e.p,{children:"Understanding center of mass dynamics is crucial for humanoid robot stability and balance control."}),"\n",(0,i.jsx)(e.h3,{id:"center-of-mass-computation",children:"Center of Mass Computation"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"Pseudocode: Center of Mass Analysis\nclass CenterOfMassAnalyzer:\n    def __init__(self, robot_model):\n        self.robot_model = robot_model\n        self.forward_kinematics = ForwardKinematics(robot_model)\n\n    def compute_center_of_mass(self, joint_positions):\n        # Compute overall center of mass position\n        total_mass = 0.0\n        weighted_sum = np.zeros(3)\n\n        # Get transforms for all links\n        transforms = self.compute_all_link_transforms(joint_positions)\n\n        for link, transform in zip(self.robot_model.links, transforms):\n            # Get link mass and center of mass in link frame\n            mass = link.mass\n            local_com = link.center_of_mass\n\n            # Transform local COM to world frame\n            world_com = np.dot(transform, np.hstack([local_com, 1]))[:3]\n\n            # Accumulate weighted position\n            weighted_sum += mass * world_com\n            total_mass += mass\n\n        if total_mass > 0:\n            com_position = weighted_sum / total_mass\n        else:\n            com_position = np.zeros(3)\n\n        return com_position, total_mass\n\n    def compute_com_jacobian(self, joint_positions):\n        # Compute Jacobian that maps joint velocities to COM velocity\n        n = len(joint_positions)\n        com_jacobian = np.zeros((3, n))\n\n        # Get transforms and masses for all links\n        transforms = self.compute_all_link_transforms(joint_positions)\n\n        # Compute total mass\n        total_mass = sum(link.mass for link in self.robot_model.links)\n\n        for i in range(n):\n            weighted_velocity_sum = np.zeros(3)\n\n            for j, (link, transform) in enumerate(zip(self.robot_model.links, transforms)):\n                # Compute velocity of link COM due to joint i motion\n                if j >= i:  # Only joints affecting this link\n                    local_com = link.center_of_mass\n                    world_com = np.dot(transform, np.hstack([local_com, 1]))[:3]\n\n                    # Get joint axis in world frame\n                    joint_transform = self.compute_single_joint_transform(joint_positions, i)\n                    joint_axis_world = np.dot(joint_transform[:3, :3], np.array([0, 0, 1]))\n\n                    # Position from joint i to link COM\n                    joint_pos = joint_transform[:3, 3]\n                    r = world_com - joint_pos\n\n                    # Velocity contribution\n                    velocity_contribution = np.cross(joint_axis_world, r)\n                    weighted_velocity_sum += link.mass * velocity_contribution\n\n            # Normalize by total mass\n            com_jacobian[:, i] = weighted_velocity_sum / total_mass\n\n        return com_jacobian\n\n    def compute_zmp(self, com_position, com_velocity, com_acceleration, support_foot_height):\n        # Compute Zero Moment Point (ZMP)\n        # ZMP_x = com_x - (com_height - support_height) * com_acc_x / g\n        # ZMP_y = com_y - (com_height - support_height) * com_acc_y / g\n\n        g = 9.81  # Gravity constant\n\n        # Height difference\n        height_diff = com_position[2] - support_foot_height\n\n        # ZMP calculation\n        zmp_x = com_position[0] - (height_diff * com_acceleration[0]) / g\n        zmp_y = com_position[1] - (height_diff * com_acceleration[1]) / g\n\n        return np.array([zmp_x, zmp_y, support_foot_height])\n\n    def analyze_stability_margin(self, com_position, support_polygon):\n        # Analyze stability based on COM position relative to support polygon\n        # Support polygon is typically defined by feet positions when walking\n\n        # Project COM to ground plane\n        com_xy = com_position[:2]\n\n        # Check if COM is within support polygon\n        is_stable = self.point_in_polygon(com_xy, support_polygon)\n\n        if is_stable:\n            # Calculate stability margin (distance to polygon boundary)\n            min_distance = float('inf')\n            for edge in self.get_polygon_edges(support_polygon):\n                distance = self.point_to_line_distance(com_xy, edge)\n                min_distance = min(min_distance, distance)\n\n            stability_margin = min_distance\n        else:\n            # Calculate distance to nearest support point\n            min_distance = float('inf')\n            for vertex in support_polygon:\n                distance = np.linalg.norm(com_xy - vertex[:2])\n                min_distance = min(min_distance, distance)\n\n            stability_margin = -min_distance  # Negative for unstable\n\n        return {\n            'is_stable': is_stable,\n            'stability_margin': stability_margin,\n            'com_position': com_xy\n        }\n"})}),"\n",(0,i.jsx)(e.h2,{id:"walking-pattern-generation",children:"Walking Pattern Generation"}),"\n",(0,i.jsx)(e.p,{children:"Generating stable walking patterns requires understanding the dynamics of bipedal locomotion."}),"\n",(0,i.jsx)(e.h3,{id:"capture-point-and-walking-control",children:"Capture Point and Walking Control"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"Pseudocode: Walking Pattern Generator\nclass WalkingPatternGenerator:\n    def __init__(self, robot_model):\n        self.robot_model = robot_model\n        self.com_analyzer = CenterOfMassAnalyzer(robot_model)\n        self.foot_step_planner = FootstepPlanner()\n\n    def generate_walking_trajectory(self, step_length, step_width, step_height, walking_speed):\n        # Generate walking pattern using inverted pendulum model\n        trajectory = []\n\n        # Walking parameters\n        com_height = self.robot_model.com_height\n        omega = np.sqrt(9.81 / com_height)  # Natural frequency of inverted pendulum\n\n        # Step timing\n        step_duration = self.calculate_step_duration(walking_speed)\n        double_support_duration = 0.1  # 10% of step time\n        single_support_duration = step_duration - double_support_duration\n\n        # Generate steps\n        current_left_foot = np.array([0, step_width/2, 0])\n        current_right_foot = np.array([0, -step_width/2, 0])\n\n        for step in range(10):  # Generate 10 steps\n            # Determine support foot\n            is_left_support = step % 2 == 0\n\n            # Calculate capture point for balance\n            if is_left_support:\n                support_foot = current_left_foot\n                swing_foot = current_right_foot\n            else:\n                support_foot = current_right_foot\n                swing_foot = current_left_foot\n\n            # Calculate desired COM trajectory\n            com_trajectory = self.generate_com_trajectory(\n                support_foot, swing_foot, step_length,\n                single_support_duration, omega\n            )\n\n            # Calculate foot trajectory\n            foot_trajectory = self.generate_foot_trajectory(\n                swing_foot, step_length, step_width, step_height,\n                single_support_duration, double_support_duration\n            )\n\n            # Combine trajectories\n            step_trajectory = self.combine_trajectories(\n                com_trajectory, foot_trajectory,\n                is_left_support, step_duration\n            )\n\n            trajectory.extend(step_trajectory)\n\n            # Update foot positions for next step\n            if is_left_support:\n                current_right_foot[0] += step_length\n                current_right_foot[1] = -step_width/2 if step % 4 < 2 else step_width/2\n            else:\n                current_left_foot[0] += step_length\n                current_left_foot[1] = step_width/2 if step % 4 < 2 else -step_width/2\n\n        return trajectory\n\n    def generate_com_trajectory(self, support_foot, swing_foot, step_length, duration, omega):\n        # Generate COM trajectory using 3rd order polynomial\n        # to ensure smooth transitions and satisfy boundary conditions\n\n        # Initial and final COM positions\n        initial_com = np.array([0, 0, self.robot_model.com_height])\n        final_com = np.array([step_length/2, 0, self.robot_model.com_height])\n\n        # Capture point trajectory (for balance)\n        initial_capture = support_foot[:2]\n        final_capture = swing_foot[:2]\n\n        # Time vector\n        t = np.linspace(0, duration, int(duration * 100))  # 100 Hz sampling\n\n        # Generate smooth trajectory using 3rd order polynomial\n        com_trajectory = []\n\n        for time in t:\n            # Progress (0 to 1)\n            progress = time / duration\n\n            # 3rd order polynomial: a + bt + ct^2 + dt^3\n            # Boundary conditions: start at initial, end at final, zero velocity at start/end\n            p = (3 * progress**2 - 2 * progress**3)  # Cubic interpolation\n            dp = (6 * progress - 6 * progress**2) / duration  # First derivative\n            ddp = (6 - 12 * progress) / (duration**2)  # Second derivative\n\n            # COM position\n            com_pos = initial_com + p * (final_com - initial_com)\n\n            # COM velocity (derivative of position)\n            com_vel = dp * (final_com - initial_com)\n\n            # COM acceleration (second derivative)\n            com_acc = ddp * (final_com - initial_com)\n\n            # Calculate ZMP from COM dynamics\n            zmp = self.calculate_zmp_from_com(com_pos, com_vel, com_acc)\n\n            com_trajectory.append({\n                'time': time,\n                'position': com_pos,\n                'velocity': com_vel,\n                'acceleration': com_acc,\n                'zmp': zmp\n            })\n\n        return com_trajectory\n\n    def calculate_zmp_from_com(self, com_pos, com_vel, com_acc):\n        # Calculate ZMP from COM dynamics\n        g = 9.81\n        com_height = com_pos[2]\n\n        # ZMP = COM - (COM_height * COM_acceleration) / g\n        zmp = com_pos[:2] - (com_height * com_acc[:2]) / g\n        zmp = np.append(zmp, [0])  # ZMP on ground plane\n\n        return zmp\n"})}),"\n",(0,i.jsx)(e.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,i.jsx)(e.p,{children:"By the end of this week, students should be able to:"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Solve forward and inverse kinematics"})," - Compute end-effector positions from joint angles and vice versa using both analytical and numerical methods."]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Derive and compute dynamic equations"})," - Apply Lagrangian mechanics and the recursive Newton-Euler algorithm to compute robot dynamics."]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Analyze center of mass and stability"})," - Calculate center of mass position, compute stability margins, and understand balance control principles."]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Generate walking patterns"})," - Create stable walking trajectories using inverted pendulum models and capture point control."]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Implement kinematic and dynamic control"})," - Apply kinematic and dynamic principles to control humanoid robot movements and maintain balance."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(e.hr,{}),"\n",(0,i.jsx)(e.h1,{id:"week-12-bipedal-locomotion-manipulation-and-human-robot-interaction",children:"Week 12: Bipedal Locomotion, Manipulation, and Human-Robot Interaction"}),"\n",(0,i.jsx)(e.h2,{id:"introduction-1",children:"Introduction"}),"\n",(0,i.jsx)(e.p,{children:"This final week of Module 3 brings together all the concepts learned to address the most challenging aspects of humanoid robotics: stable bipedal locomotion, dexterous manipulation, and natural human-robot interaction. These capabilities represent the pinnacle of humanoid robot development, requiring sophisticated integration of perception, planning, control, and learning systems to achieve human-like mobility and interaction capabilities."}),"\n",(0,i.jsx)(e.h2,{id:"bipedal-locomotion-control",children:"Bipedal Locomotion Control"}),"\n",(0,i.jsx)(e.p,{children:"Bipedal locomotion is one of the most complex challenges in humanoid robotics, requiring precise balance control, coordinated multi-joint movements, and adaptive responses to environmental disturbances."}),"\n",(0,i.jsx)(e.h3,{id:"balance-control-systems",children:"Balance Control Systems"}),"\n",(0,i.jsx)(e.p,{children:"Maintaining balance during locomotion requires sophisticated control strategies that can handle the underactuated nature of bipedal systems:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"Pseudocode: Balance Control System\nclass BalanceController:\n    def __init__(self, robot_model):\n        self.robot_model = robot_model\n        self.com_controller = COMController()\n        self.ankle_strategy = AnkleStrategyController()\n        self.hip_strategy = HipStrategyController()\n        self.waist_strategy = WaistStrategyController()\n        self.arm_swing_controller = ArmSwingController()\n\n    def compute_balance_control(self, current_state, desired_state, dt):\n        # Extract relevant state information\n        com_pos = current_state['com_position']\n        com_vel = current_state['com_velocity']\n        com_acc = current_state['com_acceleration']\n\n        base_orientation = current_state['base_orientation']\n        base_angular_vel = current_state['base_angular_velocity']\n\n        foot_positions = current_state['foot_positions']\n        zmp = current_state['zmp']\n\n        # Determine support polygon\n        support_polygon = self.compute_support_polygon(foot_positions)\n\n        # Calculate balance error\n        balance_error = self.calculate_balance_error(com_pos, support_polygon)\n\n        # Select balance strategy based on situation\n        if abs(balance_error) < 0.02:  # Small perturbation\n            # Ankle strategy - use ankle torques for small corrections\n            balance_torques = self.ankle_strategy.compute_control(\n                base_orientation, base_angular_vel, balance_error\n            )\n        elif abs(balance_error) < 0.05:  # Medium perturbation\n            # Hip strategy - use hip and ankle coordination\n            balance_torques = self.hip_strategy.compute_control(\n                com_pos, com_vel, balance_error\n            )\n        else:  # Large perturbation\n            # Full body strategy - use waist, arms, and stepping\n            balance_torques = self.waist_strategy.compute_control(\n                com_pos, com_vel, base_orientation\n            )\n\n            # Add arm swing for additional balance\n            arm_torques = self.arm_swing_controller.compute_control(balance_error)\n            balance_torques += arm_torques\n\n            # Consider stepping strategy if needed\n            if self.should_take_step(balance_error, current_state):\n                step_plan = self.plan_recovery_step(balance_error, current_state)\n                return balance_torques, step_plan\n\n        return balance_torques, None\n\n    def calculate_balance_error(self, com_pos, support_polygon):\n        # Calculate distance from COM projection to support polygon\n        com_xy = com_pos[:2]\n\n        if self.point_in_polygon(com_xy, support_polygon):\n            # COM is inside support polygon - positive margin\n            min_distance = min([\n                self.point_to_edge_distance(com_xy, edge)\n                for edge in self.get_polygon_edges(support_polygon)\n            ])\n            return min_distance\n        else:\n            # COM is outside support polygon - negative margin (unstable)\n            min_distance = min([\n                np.linalg.norm(com_xy - vertex[:2])\n                for vertex in support_polygon\n            ])\n            return -min_distance\n\n    def should_take_step(self, balance_error, state):\n        # Determine if stepping is needed for balance recovery\n        if balance_error < -0.05:  # COM too far outside support\n            return True\n\n        # Check angular momentum\n        angular_momentum = state['base_angular_velocity'] * state['com_height']\n        if abs(angular_momentum) > 0.5:  # Too much angular momentum to recover with stance\n            return True\n\n        # Check if recovery step is feasible\n        current_support_foot = state['current_support_foot']\n        if self.is_step_feasible(current_support_foot, balance_error):\n            return True\n\n        return False\n\n    def plan_recovery_step(self, balance_error, state):\n        # Plan optimal recovery step location\n        current_com = state['com_position'][:2]\n        current_support = state['current_support_foot']\n\n        # Calculate capture point (where COM will fall if no control applied)\n        com_vel = state['com_velocity'][:2]\n        com_height = state['com_position'][2]\n        capture_point = current_com + com_vel * np.sqrt(com_height / 9.81)\n\n        # Plan step to capture point or slightly beyond for stability\n        step_target = capture_point + 0.1 * (capture_point - current_com) / np.linalg.norm(capture_point - current_com)\n\n        # Ensure step is within physical limits\n        step_target = self.constrain_step_target(step_target, current_support)\n\n        # Calculate step timing\n        step_duration = self.calculate_step_duration(state['walking_speed'])\n\n        return {\n            'target_position': step_target,\n            'timing': step_duration,\n            'foot': 'left' if state['current_support_foot'] == 'right' else 'right'\n        }\n"})}),"\n",(0,i.jsx)(e.h3,{id:"walking-pattern-generation-and-execution",children:"Walking Pattern Generation and Execution"}),"\n",(0,i.jsx)(e.p,{children:"Creating stable walking patterns that can adapt to different terrains and conditions:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"Pseudocode: Adaptive Walking Controller\nclass AdaptiveWalkingController:\n    def __init__(self, robot_model):\n        self.robot_model = robot_model\n        self.footstep_planner = FootstepPlanner()\n        self.trajectory_generator = TrajectoryGenerator()\n        self.terrain_analyzer = TerrainAnalyzer()\n        self.gait_adaptation = GaitAdaptationSystem()\n\n    def generate_adaptive_walk(self, terrain_data, desired_speed, step_adjustments):\n        # Analyze terrain properties\n        terrain_analysis = self.terrain_analyzer.analyze(terrain_data)\n\n        # Adjust gait parameters based on terrain\n        gait_params = self.gait_adaptation.adjust_parameters(\n            terrain_analysis, desired_speed, step_adjustments\n        )\n\n        # Plan footstep sequence\n        footstep_sequence = self.footstep_planner.plan_sequence(\n            terrain_analysis, gait_params\n        )\n\n        # Generate walking trajectories\n        walking_trajectories = []\n        for step in footstep_sequence:\n            trajectory = self.trajectory_generator.generate_step_trajectory(\n                step, gait_params, terrain_analysis\n            )\n            walking_trajectories.append(trajectory)\n\n        return walking_trajectories\n\n    def execute_walking_step(self, current_state, next_step, gait_params):\n        # Execute single walking step with real-time adaptation\n        step_trajectory = next_step['trajectory']\n\n        # Track trajectory with feedback control\n        desired_com = step_trajectory['com_position']\n        desired_foot = step_trajectory['foot_position']\n\n        # Compute tracking errors\n        com_error = current_state['com_position'] - desired_com\n        foot_error = current_state['foot_position'] - desired_foot\n\n        # Apply feedback control\n        com_control = self.compute_com_feedback(com_error, current_state)\n        foot_control = self.compute_foot_feedback(foot_error, current_state)\n\n        # Combine with feedforward commands\n        total_control = self.combine_feedforward_feedback(\n            step_trajectory['feedforward_torques'],\n            com_control,\n            foot_control\n        )\n\n        # Apply terrain adaptation\n        if self.detect_terrain_change(current_state):\n            adapted_control = self.adapt_to_terrain(total_control, current_state)\n            return adapted_control\n\n        return total_control\n\n    def detect_terrain_change(self, state):\n        # Detect terrain changes using sensor fusion\n        contact_force_change = self.detect_contact_force_change(state)\n        imu_angular_velocity_change = self.detect_angular_velocity_change(state)\n        foot_slip_detection = self.detect_foot_slip(state)\n\n        # Combine multiple indicators\n        terrain_change_score = (\n            0.4 * contact_force_change +\n            0.3 * imu_angular_velocity_change +\n            0.3 * foot_slip_detection\n        )\n\n        return terrain_change_score > 0.5  # Threshold for terrain change detection\n\n    def adapt_gait_to_terrain(self, current_gait, terrain_type):\n        # Adapt gait parameters based on terrain type\n        adapted_gait = current_gait.copy()\n\n        if terrain_type == 'rough':\n            # Increase step height, reduce step length, increase double support\n            adapted_gait['step_height'] *= 1.5\n            adapted_gait['step_length'] *= 0.8\n            adapted_gait['double_support_ratio'] = 0.3\n            adapted_gait['ankle_stiffness'] *= 1.2\n        elif terrain_type == 'slippery':\n            # Reduce step length, increase contact time, adjust foot angle\n            adapted_gait['step_length'] *= 0.6\n            adapted_gait['step_width'] *= 1.2  # Wider stance\n            adapted_gait['contact_angle'] = -5  # More conservative foot placement\n        elif terrain_type == 'stairs':\n            # Adjust for step climbing with appropriate lifting\n            adapted_gait['step_height'] = 0.15  # Typical stair height\n            adapted_gait['step_length'] *= 0.7\n            adapted_gait['hip_lift_compensation'] = 0.05\n\n        return adapted_gait\n"})}),"\n",(0,i.jsx)(e.h3,{id:"walking-stability-and-disturbance-rejection",children:"Walking Stability and Disturbance Rejection"}),"\n",(0,i.jsx)(e.p,{children:"Handling external disturbances and maintaining stable locomotion:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"Pseudocode: Disturbance Rejection System\nclass DisturbanceRejection:\n    def __init__(self):\n        self.disturbance_observer = DisturbanceObserver()\n        self.rejection_controller = DisturbanceRejectionController()\n        self.adaptive_filter = AdaptiveFilter()\n\n    def handle_external_disturbance(self, state, measured_torques, expected_torques):\n        # Estimate external disturbances\n        disturbance_estimate = self.disturbance_observer.estimate(\n            measured_torques, expected_torques\n        )\n\n        # Classify disturbance type and magnitude\n        disturbance_type = self.classify_disturbance(disturbance_estimate)\n        disturbance_magnitude = np.linalg.norm(disturbance_estimate)\n\n        # Apply appropriate rejection strategy\n        if disturbance_magnitude < 5.0:  # Small disturbance\n            # Use feedback control to reject\n            rejection_torques = self.rejection_controller.small_disturbance(\n                state, disturbance_estimate\n            )\n        elif disturbance_magnitude < 20.0:  # Medium disturbance\n            # Use feedforward + feedback control\n            rejection_torques = self.rejection_controller.medium_disturbance(\n                state, disturbance_estimate\n            )\n        else:  # Large disturbance\n            # Emergency response - prepare for potential fall\n            rejection_torques = self.rejection_controller.large_disturbance(\n                state, disturbance_estimate\n            )\n            # Consider protective behaviors\n            protective_action = self.plan_protective_action(state, disturbance_estimate)\n\n        return rejection_torques\n\n    def classify_disturbance(self, disturbance):\n        # Classify disturbance based on pattern recognition\n        if len(disturbance) > 50:  # Time series analysis\n            # Analyze frequency content and temporal patterns\n            freq_content = np.fft.fft(disturbance)\n            dominant_freq = np.argmax(np.abs(freq_content))\n\n            if dominant_freq < 5:  # Low frequency - sustained force\n                return 'sustained_push'\n            elif 5 <= dominant_freq < 50:  # Medium frequency - impact\n                return 'impact'\n            else:  # High frequency - vibration/shaking\n                return 'vibration'\n        else:\n            # Single measurement classification\n            magnitude = np.linalg.norm(disturbance)\n            direction = disturbance / magnitude if magnitude > 0 else np.zeros_like(disturbance)\n\n            # Determine direction relative to robot orientation\n            if abs(direction[0]) > 0.7:  # Forward/backward\n                return 'forward_backward_push'\n            elif abs(direction[1]) > 0.7:  # Lateral\n                return 'lateral_push'\n            else:  # Vertical or diagonal\n                return 'vertical_or_diagonal'\n\n    def plan_protective_action(self, state, disturbance):\n        # Plan protective action for large disturbances\n        if self.is_fall_imminent(state, disturbance):\n            # Prepare for controlled fall\n            return self.prepare_controlled_fall(state)\n        else:\n            # Attempt recovery\n            return self.attempt_recovery(state, disturbance)\n\n    def is_fall_imminent(self, state, disturbance):\n        # Predict if fall is unavoidable\n        # Simulate forward dynamics with disturbance\n        predicted_state = self.simulate_disturbance_response(state, disturbance)\n\n        # Check if COM will exit capture region\n        if self.will_com_escape_capture_region(predicted_state):\n            return True\n\n        # Check angular momentum limits\n        if self.exceeds_angular_momentum_limits(predicted_state):\n            return True\n\n        return False\n"})}),"\n",(0,i.jsx)(e.h2,{id:"dexterous-manipulation",children:"Dexterous Manipulation"}),"\n",(0,i.jsx)(e.p,{children:"Humanoid robots must achieve human-like manipulation capabilities for effective interaction with the environment and objects."}),"\n",(0,i.jsx)(e.h3,{id:"grasp-planning-and-execution",children:"Grasp Planning and Execution"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"Pseudocode: Grasp Planning System\nclass GraspPlanner:\n    def __init__(self, robot_model):\n        self.robot_model = robot_model\n        self.hand_model = robot_model.hand_model\n        self.object_analyzer = ObjectAnalyzer()\n        self.grasp_synthesizer = GraspSynthesizer()\n        self.stability_evaluator = GraspStabilityEvaluator()\n\n    def plan_grasp(self, object_pose, object_properties):\n        # Analyze object properties\n        object_analysis = self.object_analyzer.analyze(\n            object_pose, object_properties\n        )\n\n        # Generate candidate grasps\n        candidate_grasps = self.grasp_synthesizer.generate_candidates(\n            object_analysis\n        )\n\n        # Evaluate grasp stability\n        stable_grasps = []\n        for grasp in candidate_grasps:\n            stability_score = self.stability_evaluator.evaluate(\n                grasp, object_analysis\n            )\n\n            if stability_score > 0.7:  # Threshold for stable grasp\n                grasp['stability_score'] = stability_score\n                stable_grasps.append(grasp)\n\n        # Sort by stability and other criteria\n        stable_grasps.sort(key=lambda g: (\n            g['stability_score'],\n            g['ease_of_implementation'],\n            g['object_alignment']\n        ), reverse=True)\n\n        return stable_grasps[0] if stable_grasps else None\n\n    def execute_grasp_sequence(self, grasp_plan, object_pose):\n        # Generate approach trajectory\n        approach_trajectory = self.generate_approach_trajectory(\n            grasp_plan, object_pose\n        )\n\n        # Execute approach motion\n        for waypoint in approach_trajectory:\n            self.move_to_waypoint(waypoint)\n\n            # Monitor for contact\n            if self.detect_contact():\n                break\n\n        # Execute grasp closure\n        self.execute_grasp_closure(grasp_plan['grasp_type'])\n\n        # Verify grasp success\n        if self.verify_grasp_success():\n            # Lift object carefully\n            lift_trajectory = self.generate_lift_trajectory(object_pose)\n            for waypoint in lift_trajectory:\n                self.move_to_waypoint(waypoint)\n\n            return True\n        else:\n            # Grasp failed, try alternative\n            return False\n\n    def generate_approach_trajectory(self, grasp_plan, object_pose):\n        # Generate collision-free approach trajectory\n        start_pose = self.get_current_hand_pose()\n        grasp_pose = grasp_plan['grasp_pose']\n\n        # Offset for approach (e.g., 10cm from grasp point)\n        approach_offset = grasp_plan['approach_direction'] * 0.1\n        approach_pose = grasp_pose.copy()\n        approach_pose[:3, 3] += approach_offset\n\n        # Plan path using RRT or other motion planning algorithm\n        trajectory = self.plan_collision_free_path(\n            start_pose, approach_pose, object_pose\n        )\n\n        # Add final approach to grasp\n        trajectory.append(grasp_pose)\n\n        return trajectory\n\n    def execute_grasp_closure(self, grasp_type):\n        # Execute appropriate grasp closure pattern\n        if grasp_type == 'precision_pinch':\n            # Move thumb and index finger together\n            self.move_finger_to_position('thumb', 0.02)\n            self.move_finger_to_position('index', 0.02)\n            self.apply_force_control(20)  # 20N grasp force\n        elif grasp_type == 'power_grasp':\n            # Close all fingers\n            for finger in ['thumb', 'index', 'middle', 'ring', 'pinky']:\n                self.move_finger_to_position(finger, 0.05)\n            self.apply_force_control(50)  # Higher force for power grasp\n        elif grasp_type == 'cylindrical':\n            # Wrap fingers around cylindrical object\n            self.move_finger_to_position('thumb', 0.03)\n            self.move_finger_to_position('index', 0.04)\n            self.move_finger_to_position('middle', 0.04)\n            self.apply_force_control(30)\n"})}),"\n",(0,i.jsx)(e.h3,{id:"multi-modal-manipulation",children:"Multi-Modal Manipulation"}),"\n",(0,i.jsx)(e.p,{children:"Integrating vision, touch, and force feedback for robust manipulation:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"Pseudocode: Multi-Modal Manipulation Controller\nclass MultiModalManipulation:\n    def __init__(self, robot_model):\n        self.robot_model = robot_model\n        self.vision_system = VisionSystem()\n        self.tactile_sensors = TactileSensorArray()\n        self.force_control = ForceController()\n        self.manipulation_planner = ManipulationPlanner()\n\n    def execute_vision_guided_manipulation(self, target_object, task):\n        # Get visual feedback\n        object_pose = self.vision_system.get_object_pose(target_object)\n\n        # Plan manipulation sequence\n        manipulation_sequence = self.manipulation_planner.plan_sequence(\n            object_pose, task\n        )\n\n        # Execute with multi-modal feedback\n        for action in manipulation_sequence:\n            if action['type'] == 'reach':\n                self.execute_reach_with_vision_feedback(action)\n            elif action['type'] == 'grasp':\n                self.execute_grasp_with_tactile_feedback(action)\n            elif action['type'] == 'manipulate':\n                self.execute_manipulation_with_force_feedback(action)\n\n    def execute_reach_with_vision_feedback(self, reach_action):\n        # Execute reaching motion with visual servoing\n        target_position = reach_action['target_position']\n        current_position = self.get_end_effector_position()\n\n        while np.linalg.norm(target_position - current_position) > 0.01:  # 1cm threshold\n            # Get updated visual feedback\n            updated_target = self.vision_system.get_object_pose(\n                reach_action['target_object']\n            )\n\n            # Adjust trajectory based on updated target\n            adjusted_target = self.adjust_trajectory_for_vision_error(\n                target_position, updated_target\n            )\n\n            # Compute control command\n            control_command = self.compute_visual_servoing_command(\n                current_position, adjusted_target\n            )\n\n            # Apply control\n            self.apply_joint_velocities(control_command)\n\n            # Update current position\n            current_position = self.get_end_effector_position()\n\n    def execute_grasp_with_tactile_feedback(self, grasp_action):\n        # Execute grasp with tactile feedback for adjustment\n        initial_grasp_force = grasp_action['initial_force']\n\n        # Begin grasp execution\n        self.apply_grasp_command(grasp_action['grasp_pattern'])\n\n        # Monitor tactile sensors during grasp\n        while not self.is_grasp_stable():\n            tactile_data = self.tactile_sensors.get_data()\n\n            # Analyze tactile feedback\n            contact_points = self.extract_contact_points(tactile_data)\n            pressure_distribution = self.analyze_pressure_distribution(tactile_data)\n\n            # Adjust grasp based on tactile feedback\n            if not all_contacts_stable(contact_points):\n                # Adjust finger positions\n                self.adjust_finger_positions(contact_points)\n\n            if pressure_distribution_skewed(pressure_distribution):\n                # Adjust grasp force distribution\n                self.adjust_force_distribution(pressure_distribution)\n\n            # Continue grasping\n            self.continue_grasp_execution()\n\n    def execute_manipulation_with_force_feedback(self, manipulation_action):\n        # Execute manipulation with force control\n        task_frame = manipulation_action['task_frame']\n        desired_wrench = manipulation_action['desired_wrench']\n\n        # Transform desired wrench to end-effector frame\n        ee_wrench = self.transform_wrench_to_ee_frame(\n            desired_wrench, task_frame\n        )\n\n        # Execute force-controlled manipulation\n        while not task_complete(manipulation_action):\n            # Measure current wrench\n            current_wrench = self.force_control.get_measured_wrench()\n\n            # Compute wrench error\n            wrench_error = ee_wrench - current_wrench\n\n            # Apply force control\n            force_control_command = self.compute_force_control_command(\n                wrench_error, manipulation_action\n            )\n\n            # Apply position control for unconstrained directions\n            position_command = self.compute_position_command(\n                manipulation_action\n            )\n\n            # Combine force and position control\n            combined_command = self.combine_force_position_control(\n                force_control_command, position_command\n            )\n\n            self.apply_manipulation_command(combined_command)\n"})}),"\n",(0,i.jsx)(e.h2,{id:"human-robot-interaction",children:"Human-Robot Interaction"}),"\n",(0,i.jsx)(e.p,{children:"Natural and intuitive interaction between humans and humanoid robots is essential for practical applications."}),"\n",(0,i.jsx)(e.h3,{id:"social-interaction-framework",children:"Social Interaction Framework"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"Pseudocode: Social Interaction System\nclass SocialInteractionSystem:\n    def __init__(self, robot_model):\n        self.robot_model = robot_model\n        self.speech_recognition = SpeechRecognitionSystem()\n        self.natural_language_processing = NaturalLanguageProcessor()\n        self.social_behavior_engine = SocialBehaviorEngine()\n        self.emotion_recognition = EmotionRecognitionSystem()\n        self.gesture_recognition = GestureRecognitionSystem()\n\n    def handle_human_interaction(self, human_input):\n        # Process different types of human input\n        interaction_elements = self.analyze_human_input(human_input)\n\n        # Extract speech content\n        if 'speech' in interaction_elements:\n            speech_content = self.speech_recognition.process(\n                interaction_elements['speech']\n            )\n            intent = self.natural_language_processing.extract_intent(speech_content)\n\n        # Recognize emotions\n        if 'facial_expression' in interaction_elements:\n            emotions = self.emotion_recognition.analyze(\n                interaction_elements['facial_expression']\n            )\n\n        # Recognize gestures\n        if 'hand_gesture' in interaction_elements:\n            gestures = self.gesture_recognition.analyze(\n                interaction_elements['hand_gesture']\n            )\n\n        # Generate appropriate response\n        response = self.social_behavior_engine.generate_response(\n            intent, emotions, gestures\n        )\n\n        # Execute response with appropriate modalities\n        self.execute_social_response(response)\n\n    def analyze_human_input(self, input_data):\n        # Multi-modal input analysis\n        elements = {}\n\n        if input_data.get('audio'):\n            elements['speech'] = input_data['audio']\n\n        if input_data.get('video'):\n            face_data = self.extract_face_features(input_data['video'])\n            gesture_data = self.extract_gesture_features(input_data['video'])\n\n            if face_data:\n                elements['facial_expression'] = face_data\n            if gesture_data:\n                elements['hand_gesture'] = gesture_data\n\n        if input_data.get('proximity'):\n            elements['proximity'] = input_data['proximity']\n\n        return elements\n\n    def generate_context_aware_response(self, context, intent, emotions):\n        # Generate response based on context and social cues\n        response = {\n            'speech': '',\n            'gesture': '',\n            'facial_expression': '',\n            'action': ''\n        }\n\n        # Adjust response based on detected emotions\n        if 'happy' in emotions:\n            response['speech'] = f\"That sounds wonderful! {intent.response}\"\n            response['facial_expression'] = 'smile'\n        elif 'sad' in emotions:\n            response['speech'] = f\"I'm sorry to hear that. {intent.response}\"\n            response['facial_expression'] = 'concerned'\n        elif 'angry' in emotions:\n            response['speech'] = f\"I understand your concern. Let me help with {intent.request}\"\n            response['facial_expression'] = 'attentive'\n        else:\n            response['speech'] = intent.response\n\n        # Add appropriate gestures\n        if intent.type == 'greeting':\n            response['gesture'] = 'wave'\n        elif intent.type == 'question':\n            response['gesture'] = 'point_to_self'  # Indicate listening\n        elif intent.type == 'direction':\n            response['gesture'] = 'point_to_location'\n\n        # Adjust based on social context\n        if context.get('formal_setting'):\n            response['speech'] = self.make_response_formal(response['speech'])\n        elif context.get('child_interaction'):\n            response['speech'] = self.make_response_child_friendly(response['speech'])\n\n        return response\n\n    def execute_social_response(self, response):\n        # Execute response using multiple modalities\n        if response.get('speech'):\n            self.speak(response['speech'])\n\n        if response.get('gesture'):\n            self.perform_gesture(response['gesture'])\n\n        if response.get('facial_expression'):\n            self.display_facial_expression(response['facial_expression'])\n\n        if response.get('action'):\n            self.perform_action(response['action'])\n\n    def manage_interaction_flow(self, conversation_history):\n        # Manage turn-taking and conversation flow\n        if self.should_respond(conversation_history):\n            # Generate and execute response\n            response = self.generate_context_aware_response(\n                self.get_current_context(),\n                self.get_current_intent(),\n                self.get_current_emotions()\n            )\n            self.execute_social_response(response)\n        elif self.should_initiate_topic(conversation_history):\n            # Initiate new topic based on context\n            topic = self.select_appropriate_topic(conversation_history)\n            self.initiate_topic(topic)\n        elif self.should_maintain_attention(conversation_history):\n            # Use attention-maintaining behaviors\n            self.perform_attention_behavior()\n"})}),"\n",(0,i.jsx)(e.h3,{id:"collaborative-task-execution",children:"Collaborative Task Execution"}),"\n",(0,i.jsx)(e.p,{children:"Enabling robots to work effectively alongside humans:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{children:"Pseudocode: Collaborative Task System\nclass CollaborativeTaskSystem:\n    def __init__(self, robot_model):\n        self.robot_model = robot_model\n        self.task_planner = CollaborativeTaskPlanner()\n        self.human_activity_recognizer = HumanActivityRecognizer()\n        self.intent_predictor = IntentPredictor()\n        self.safety_monitor = SafetyMonitor()\n\n    def execute_collaborative_task(self, task_specification, human_partner):\n        # Plan collaborative task\n        collaborative_plan = self.task_planner.create_plan(\n            task_specification, human_partner.capabilities\n        )\n\n        # Monitor human activities\n        while not task_complete(collaborative_plan):\n            human_state = self.human_activity_recognizer.get_current_state()\n            human_intent = self.intent_predictor.predict(human_state)\n\n            # Predict human actions and adjust robot behavior\n            if human_intent.action == 'reach_for_object':\n                robot_action = self.yield_object(human_intent.target_object)\n            elif human_intent.action == 'make_space':\n                robot_action = self.move_out_of_way()\n            elif human_intent.action == 'need_help':\n                robot_action = self.provide_assistance(human_intent.task)\n            else:\n                robot_action = self.continue_plan_execution(collaborative_plan)\n\n            # Execute coordinated action\n            self.execute_action_with_coordination(robot_action, human_state)\n\n            # Monitor safety\n            if not self.safety_monitor.is_safe(human_state, robot_action):\n                self.initiate_safety_protocol()\n\n    def predict_human_intentions(self, human_state):\n        # Predict human intentions based on observed behavior\n        features = self.extract_behavioral_features(human_state)\n\n        # Use machine learning model to predict intentions\n        intent_probabilities = self.ml_model.predict(features)\n\n        # Select most likely intentions\n        likely_intents = [\n            intent for intent, prob in intent_probabilities.items()\n            if prob > 0.3  # Threshold for consideration\n        ]\n\n        # Rank intentions by probability and context\n        ranked_intents = sorted(\n            likely_intents,\n            key=lambda i: intent_probabilities[i],\n            reverse=True\n        )\n\n        return ranked_intents\n\n    def coordinate_with_human(self, human_action, robot_action):\n        # Coordinate robot action with human action to avoid conflicts\n        if self.actions_conflict(human_action, robot_action):\n            # Resolve conflict using priority rules\n            if self.robot_action_has_priority(robot_action, human_action):\n                # Proceed with robot action, inform human\n                self.inform_human_of_robot_action(robot_action)\n            else:\n                # Wait for human to complete action\n                self.wait_for_human_completion(human_action)\n                # Adjust robot plan accordingly\n                self.adjust_robot_plan(human_action)\n        else:\n            # Execute actions in parallel if safe\n            self.execute_parallel_actions(human_action, robot_action)\n\n    def ensure_safe_collaboration(self, human_position, robot_motion):\n        # Ensure robot motion doesn't interfere with human safety\n        human_workspace = self.calculate_human_workspace(human_position)\n        robot_trajectory = self.discretize_robot_trajectory(robot_motion)\n\n        for waypoint in robot_trajectory:\n            if self.would_interfere_with_human(waypoint, human_workspace):\n                # Adjust trajectory to maintain safety margin\n                safe_waypoint = self.find_safe_alternative(waypoint, human_workspace)\n                robot_motion = self.adjust_trajectory(robot_motion, waypoint, safe_waypoint)\n\n        return robot_motion\n"})}),"\n",(0,i.jsx)(e.h2,{id:"learning-outcomes-1",children:"Learning Outcomes"}),"\n",(0,i.jsx)(e.p,{children:"By the end of this week, students should be able to:"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Implement balance control systems"})," - Design and implement sophisticated balance control algorithms that can handle various perturbations and maintain stable bipedal locomotion."]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Generate adaptive walking patterns"})," - Create walking controllers that can adapt to different terrains, speeds, and environmental conditions while maintaining stability."]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Execute dexterous manipulation"})," - Plan and execute complex manipulation tasks using multi-modal feedback (vision, touch, force) for robust object interaction."]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Design human-robot interaction systems"})," - Create social interaction frameworks that enable natural communication and collaboration between humans and humanoid robots."]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Integrate locomotion, manipulation, and interaction"})," - Combine all humanoid capabilities into cohesive systems that demonstrate human-like mobility, dexterity, and social behavior."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(e.hr,{})]})}function p(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,i.jsx)(e,{...n,children:(0,i.jsx)(_,{...n})}):_(n)}},8453:(n,e,t)=>{t.d(e,{R:()=>r,x:()=>s});var o=t(6540);const i={},a=o.createContext(i);function r(n){const e=o.useContext(a);return o.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(i):n.components||i:r(n.components),o.createElement(a.Provider,{value:e},n.children)}}}]);