"use strict";(globalThis.webpackChunkphysical_ai_robotics_book=globalThis.webpackChunkphysical_ai_robotics_book||[]).push([[710],{8453:(n,e,i)=>{i.d(e,{R:()=>a,x:()=>s});var t=i(6540);const o={},r=t.createContext(o);function a(n){const e=t.useContext(r);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:a(n.components),t.createElement(r.Provider,{value:e},n.children)}},9985:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>c,contentTitle:()=>s,default:()=>h,frontMatter:()=>a,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"module-4-vla/diagrams/voice-to-action-pipeline","title":"Voice-to-Action Pipeline Diagram","description":"Diagram Information","source":"@site/docs/module-4-vla/diagrams/voice-to-action-pipeline.md","sourceDirName":"module-4-vla/diagrams","slug":"/module-4-vla/diagrams/voice-to-action-pipeline","permalink":"/Textbook-for-Teaching-Physical-AI-Humanoid-Robotics-Course/docs/module-4-vla/diagrams/voice-to-action-pipeline","draft":false,"unlisted":false,"editUrl":"https://github.com/AqsaIftikhar15/Textbook-for-Teaching-Physical-AI-Humanoid-Robotics-Course/docs/module-4-vla/diagrams/voice-to-action-pipeline.md","tags":[],"version":"current","frontMatter":{}}');var o=i(4848),r=i(8453);const a={},s="Voice-to-Action Pipeline Diagram",c={},l=[{value:"Diagram Information",id:"diagram-information",level:2},{value:"Diagram Content",id:"diagram-content",level:2},{value:"Mathematical Explanation",id:"mathematical-explanation",level:2},{value:"Figure Notes",id:"figure-notes",level:2},{value:"APA Citation for Source",id:"apa-citation-for-source",level:2}];function d(n){const e={code:"code",em:"em",h1:"h1",h2:"h2",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.header,{children:(0,o.jsx)(e.h1,{id:"voice-to-action-pipeline-diagram",children:"Voice-to-Action Pipeline Diagram"})}),"\n",(0,o.jsx)(e.h2,{id:"diagram-information",children:"Diagram Information"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Title"}),": Voice-to-Action Pipeline: From Speech Input to Robot Action Execution"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Type"}),": process-flow"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Description"}),": This diagram illustrates the complete pipeline for processing voice commands and translating them into robot actions, showing the flow from speech input through cognitive processing to motor execution."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Concepts Illustrated"}),": language-understanding, action-planning, perception-cognition-action-loop, llm-robot-integration, human-robot-interaction-vla"]}),"\n",(0,o.jsx)(e.h2,{id:"diagram-content",children:"Diagram Content"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-mermaid",children:'graph TD\n    subgraph "Human User"\n        SPEECH["\ud83d\udde3\ufe0f Speech Command<br/>- Natural language input<br/>- \'Please bring me the red cup\'"]\n    end\n\n    subgraph "Voice-to-Action Pipeline"\n        subgraph "Perception Layer: Speech Processing"\n            ASR["\ud83c\udfa4 Automatic Speech Recognition<br/>- Audio to text conversion<br/>- Speech-to-text models<br/>- Noise filtering"]\n            TOKENIZATION["\ud83d\udcdd Tokenization<br/>- Text to tokens<br/>- Language model input<br/>- Context preparation"]\n        end\n\n        subgraph "Cognition Layer: Intent Processing"\n            EMBEDDING["\ud83e\udde0 Embedding Generation<br/>- Semantic representations<br/>- Contextual vectors<br/>- Meaning encoding"]\n            INTENT_RECOGNITION["\ud83c\udfaf Intent Recognition<br/>- Command classification<br/>- Object identification<br/>- Action determination"]\n            PLANNING["\u2699\ufe0f Action Planning<br/>- Task decomposition<br/>- Sequence generation<br/>- Constraint checking"]\n        end\n\n        subgraph "Action Layer: Execution"\n            MOTOR_COMMANDS["\ud83e\udd16 Motor Commands<br/>- Navigation planning<br/>- Manipulation sequences<br/>- Safety checks"]\n            EXECUTION["\u26a1 Action Execution<br/>- Physical movement<br/>- Environmental interaction<br/>- Real-time adjustments"]\n        end\n\n        FEEDBACK["\ud83d\udd04 Feedback Loop<br/>- Execution status<br/>- Success confirmation<br/>- Error handling"]\n    end\n\n    SPEECH --\x3e ASR\n    ASR --\x3e TOKENIZATION\n    TOKENIZATION --\x3e EMBEDDING\n    EMBEDDING --\x3e INTENT_RECOGNITION\n    INTENT_RECOGNITION --\x3e PLANNING\n    PLANNING --\x3e MOTOR_COMMANDS\n    MOTOR_COMMANDS --\x3e EXECUTION\n    EXECUTION --\x3e FEEDBACK\n    FEEDBACK --\x3e INTENT_RECOGNITION\n    FEEDBACK --\x3e PLANNING\n    FEEDBACK --\x3e MOTOR_COMMANDS\n\n    subgraph "External Context"\n        ENVIRONMENT["\ud83c\udfe0 Environment<br/>- Object locations<br/>- Obstacle maps<br/>- Task context"]\n    end\n\n    ENVIRONMENT -.-> EMBEDDING\n    ENVIRONMENT -.-> INTENT_RECOGNITION\n    ENVIRONMENT -.-> PLANNING\n    ENVIRONMENT -.-> MOTOR_COMMANDS\n'})}),"\n",(0,o.jsx)(e.h2,{id:"mathematical-explanation",children:"Mathematical Explanation"}),"\n",(0,o.jsx)(e.p,{children:"The voice-to-action process can be represented as a sequence of transformations:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{children:"S \u2192 T \u2192 E \u2192 I \u2192 A \u2192 M\n"})}),"\n",(0,o.jsx)(e.p,{children:"Where:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"S: Speech signal s(t) \u2208 R^T (audio waveform over time)"}),"\n",(0,o.jsx)(e.li,{children:"T: Text output T = ASR(S) (automatic speech recognition)"}),"\n",(0,o.jsx)(e.li,{children:"E: Embedding vector E = f_embed(T) \u2208 R^d (semantic representation)"}),"\n",(0,o.jsx)(e.li,{children:"I: Intent vector I = f_intent(E, C) \u2208 R^n (intent classification with context)"}),"\n",(0,o.jsx)(e.li,{children:"A: Action sequence A = Plan(I, Env) (action planning with environment)"}),"\n",(0,o.jsx)(e.li,{children:"M: Motor commands M = Execute(A) (motor execution)"}),"\n"]}),"\n",(0,o.jsx)(e.p,{children:"The probability distribution over possible intents given the input can be expressed as:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{children:"P(intent | speech, context) = softmax(W_o * Attention(Q, K, V))\n"})}),"\n",(0,o.jsx)(e.p,{children:"Where Q, K, V are computed from the speech embedding and contextual information."}),"\n",(0,o.jsx)(e.h2,{id:"figure-notes",children:"Figure Notes"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Educational Purpose"}),": This diagram helps students visualize the complete pipeline from voice command to robot action, showing how different system components work together."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Key Elements"}),":"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"The three main layers (Perception, Cognition, Action) showing the flow from speech to action"}),"\n",(0,o.jsx)(e.li,{children:"The feedback loop showing how execution results influence future processing"}),"\n",(0,o.jsx)(e.li,{children:"The external context providing environmental information"}),"\n"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Common Misconceptions"}),":"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Students might think speech recognition is 100% accurate; the diagram shows error handling through feedback"}),"\n",(0,o.jsx)(e.li,{children:"The process involves complex context integration, not just simple keyword matching"}),"\n"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Related Content"}),":"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"This pipeline connects to the broader perception-cognition-action loop"}),"\n",(0,o.jsx)(e.li,{children:"Embedding and attention mechanisms are detailed in the mathematical explanation"}),"\n",(0,o.jsx)(e.li,{children:"Pseudo-code examples demonstrate the workflow in T026"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"apa-citation-for-source",children:"APA Citation for Source"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Thomason, J., Bisk, Y., & Khosla, A. (2019). Shifting towards task completion with touch in multimodal conversational interfaces. arXiv preprint arXiv:1909.05288."}),"\n",(0,o.jsx)(e.li,{children:"OpenAI. (2023). GPT-4 Technical Report. OpenAI."}),"\n"]}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.p,{children:(0,o.jsx)(e.em,{children:"Note: This diagram follows ADR-002 requirements by providing both visual and mathematical explanations for conceptual understanding."})})]})}function h(n={}){const{wrapper:e}={...(0,r.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(d,{...n})}):d(n)}}}]);