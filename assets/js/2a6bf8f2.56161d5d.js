"use strict";(globalThis.webpackChunkphysical_ai_robotics_book=globalThis.webpackChunkphysical_ai_robotics_book||[]).push([[230],{5561:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>h,frontMatter:()=>s,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"module-4-vla/diagrams/vla-system-architecture","title":"VLA System Architecture Diagram","description":"Diagram Information","source":"@site/docs/module-4-vla/diagrams/vla-system-architecture.md","sourceDirName":"module-4-vla/diagrams","slug":"/module-4-vla/diagrams/vla-system-architecture","permalink":"/Textbook-for-Teaching-Physical-AI-Humanoid-Robotics-Course/docs/module-4-vla/diagrams/vla-system-architecture","draft":false,"unlisted":false,"editUrl":"https://github.com/AqsaIftikhar15/Textbook-for-Teaching-Physical-AI-Humanoid-Robotics-Course/docs/module-4-vla/diagrams/vla-system-architecture.md","tags":[],"version":"current","frontMatter":{}}');var a=t(4848),r=t(8453);const s={},o="VLA System Architecture Diagram",c={},l=[{value:"Diagram Information",id:"diagram-information",level:2},{value:"Diagram Content",id:"diagram-content",level:2},{value:"Mathematical Explanation",id:"mathematical-explanation",level:2},{value:"Figure Notes",id:"figure-notes",level:2},{value:"APA Citation for Source",id:"apa-citation-for-source",level:2}];function d(e){const n={code:"code",em:"em",h1:"h1",h2:"h2",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"vla-system-architecture-diagram",children:"VLA System Architecture Diagram"})}),"\n",(0,a.jsx)(n.h2,{id:"diagram-information",children:"Diagram Information"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Title"}),": VLA System Architecture: Perception-Cognition-Action Flow"]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Type"}),": system-diagram"]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Description"}),": This diagram illustrates the architecture of Vision-Language-Action (VLA) systems, showing how visual perception, language understanding, and motor action components interact in a humanoid robot system."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Concepts Illustrated"}),": vla-system, perception-cognition-action-loop, multimodal-integration, cross-modal-attention"]}),"\n",(0,a.jsx)(n.h2,{id:"diagram-content",children:"Diagram Content"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-mermaid",children:'graph TB\n    subgraph "Human User"\n        HUMAN["\ud83d\udde3\ufe0f Natural Language Command"]\n    end\n\n    subgraph "VLA System Architecture"\n        subgraph "Perception Layer"\n            VISION["\ud83d\udc41\ufe0f Vision Processing<br/>- Camera sensors<br/>- Feature extraction<br/>- Object detection"]\n            LANGUAGE_PERC["\ud83d\udde3\ufe0f Language Processing<br/>- Speech recognition<br/>- NLP parsing<br/>- Intent extraction"]\n        end\n\n        subgraph "Cognition Layer"\n            CROSS_ATTENTION["\ud83d\udd04 Cross-Modal Attention<br/>- Visual-language fusion<br/>- Attention mechanisms<br/>- Context integration"]\n            REASONING["\ud83e\udde0 Reasoning Engine<br/>- Task planning<br/>- Context awareness<br/>- Decision making"]\n        end\n\n        subgraph "Action Layer"\n            MOTOR_PLANNING["\u2699\ufe0f Motor Planning<br/>- Path planning<br/>- Motion planning<br/>- Action sequencing"]\n            EXECUTION["\ud83e\udd16 Action Execution<br/>- Motor control<br/>- Navigation<br/>- Manipulation"]\n        end\n\n        FEEDBACK["\ud83d\udd04 Feedback Loop<br/>- Sensor data<br/>- Execution results<br/>- Environmental changes"]\n    end\n\n    HUMAN --\x3e LANGUAGE_PERC\n    VISION --\x3e CROSS_ATTENTION\n    LANGUAGE_PERC --\x3e CROSS_ATTENTION\n    CROSS_ATTENTION --\x3e REASONING\n    REASONING --\x3e MOTOR_PLANNING\n    MOTOR_PLANNING --\x3e EXECUTION\n    EXECUTION --\x3e FEEDBACK\n    FEEDBACK --\x3e VISION\n    FEEDBACK --\x3e CROSS_ATTENTION\n    FEEDBACK --\x3e REASONING\n    FEEDBACK --\x3e MOTOR_PLANNING\n'})}),"\n",(0,a.jsx)(n.h2,{id:"mathematical-explanation",children:"Mathematical Explanation"}),"\n",(0,a.jsx)(n.p,{children:"The VLA system can be represented as:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"S(t) = f(V(t), L(t), A(t-1), H)\n"})}),"\n",(0,a.jsx)(n.p,{children:"Where:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"S(t) is the system state at time t"}),"\n",(0,a.jsx)(n.li,{children:"V(t) represents visual input at time t"}),"\n",(0,a.jsx)(n.li,{children:"L(t) represents language input at time t"}),"\n",(0,a.jsx)(n.li,{children:"A(t-1) represents previous actions"}),"\n",(0,a.jsx)(n.li,{children:"H represents historical context"}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"The cross-modal attention mechanism can be expressed as:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"Attention(Q, K, V) = softmax((QK^T)/\u221ad_k)V\n"})}),"\n",(0,a.jsx)(n.p,{children:"Where Q (queries) come from one modality, K (keys) and V (values) from another, enabling information flow between vision and language components."}),"\n",(0,a.jsx)(n.h2,{id:"figure-notes",children:"Figure Notes"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Educational Purpose"}),": This diagram helps students visualize how different components of a VLA system work together to process multimodal inputs and generate appropriate actions."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Key Elements"}),":"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"The three main layers (Perception, Cognition, Action) showing the flow from input to output"}),"\n",(0,a.jsx)(n.li,{children:"The feedback loop showing how the system adapts based on execution results"}),"\n",(0,a.jsx)(n.li,{children:"The cross-modal attention component that integrates vision and language"}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Common Misconceptions"}),":"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Students might think these components operate independently; the diagram shows their interconnections"}),"\n",(0,a.jsx)(n.li,{children:"The feedback loop is essential for adaptive behavior, not just a simple feedforward process"}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Related Content"}),":"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"This architecture connects to the perception-cognition-action loop concept"}),"\n",(0,a.jsx)(n.li,{children:"Cross-modal attention mechanisms are detailed in the mathematical explanation"}),"\n",(0,a.jsx)(n.li,{children:"Pseudo-code examples demonstrate the workflow in T018"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"apa-citation-for-source",children:"APA Citation for Source"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Ahn, H., Du, Y., Kolve, E., Gupta, A., & Gupta, S. (2022). Do as i can, not as i say: Grounding embodied agents with human demonstrations. arXiv preprint arXiv:2206.10558."}),"\n",(0,a.jsx)(n.li,{children:"Reed, K., Vu, T. T., Paine, T. L., Brohan, A., Joshi, S., Valenzuela-Esc\xe1rcega, M. A., ... & Le, Q. V. (2022). A generalist agent. Transactions on Machine Learning Research."}),"\n"]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.em,{children:"Note: This diagram follows ADR-002 requirements by providing both visual and mathematical explanations for conceptual understanding."})})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>o});var i=t(6540);const a={},r=i.createContext(a);function s(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:s(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);