"use strict";(globalThis.webpackChunkphysical_ai_robotics_book=globalThis.webpackChunkphysical_ai_robotics_book||[]).push([[7726],{1908:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>h,frontMatter:()=>r,metadata:()=>t,toc:()=>a});const t=JSON.parse('{"id":"module-4-vla/hri-speech-recognition","title":"Speech Recognition in Multimodal Interaction Systems","description":"Understanding speech processing within multimodal human-robot interaction","source":"@site/docs/module-4-vla/hri-speech-recognition.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/hri-speech-recognition","permalink":"/Textbook-for-Teaching-Physical-AI-Humanoid-Robotics-Course/module-4-vla/hri-speech-recognition","draft":false,"unlisted":false,"editUrl":"https://github.com/AqsaIftikhar15/Textbook-for-Teaching-Physical-AI-Humanoid-Robotics-Course/docs/module-4-vla/hri-speech-recognition.md","tags":[],"version":"current","sidebarPosition":10,"frontMatter":{"title":"Speech Recognition in Multimodal Interaction Systems","sidebar_position":10,"description":"Understanding speech processing within multimodal human-robot interaction"},"sidebar":"tutorialSidebar","previous":{"title":"Introduction to Human-Robot Interaction Design Principles","permalink":"/Textbook-for-Teaching-Physical-AI-Humanoid-Robotics-Course/module-4-vla/hri-design-principles-intro"},"next":{"title":"Gesture and Vision Integration in Human-Robot Interaction","permalink":"/Textbook-for-Teaching-Physical-AI-Humanoid-Robotics-Course/module-4-vla/gesture-vision-integration"}}');var s=i(4848),o=i(8453);const r={title:"Speech Recognition in Multimodal Interaction Systems",sidebar_position:10,description:"Understanding speech processing within multimodal human-robot interaction"},l="Speech Recognition in Multimodal Interaction Systems",c={},a=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Overview",id:"overview",level:2},{value:"Speech Recognition Architecture in HRI",id:"speech-recognition-architecture-in-hri",level:2},{value:"Acoustic Processing",id:"acoustic-processing",level:3},{value:"Language Processing",id:"language-processing",level:3},{value:"Multimodal Integration",id:"multimodal-integration",level:3},{value:"Challenges in HRI Speech Recognition",id:"challenges-in-hri-speech-recognition",level:2},{value:"Environmental Challenges",id:"environmental-challenges",level:3},{value:"Linguistic Challenges",id:"linguistic-challenges",level:3},{value:"Multimodal Integration Challenges",id:"multimodal-integration-challenges",level:3},{value:"Integration with Other Modalities",id:"integration-with-other-modalities",level:2},{value:"Speech-Gesture Integration",id:"speech-gesture-integration",level:3},{value:"Visual Context Integration",id:"visual-context-integration",level:3},{value:"Social Context Integration",id:"social-context-integration",level:3},{value:"Techniques for Robust Speech Recognition",id:"techniques-for-robust-speech-recognition",level:2},{value:"Noise-Robust Processing",id:"noise-robust-processing",level:3},{value:"Context-Aware Recognition",id:"context-aware-recognition",level:3},{value:"Multimodal Fusion Techniques",id:"multimodal-fusion-techniques",level:3},{value:"Mathematical Framework",id:"mathematical-framework",level:2},{value:"Acoustic Model",id:"acoustic-model",level:3},{value:"Language Model",id:"language-model",level:3},{value:"Multimodal Fusion",id:"multimodal-fusion",level:3},{value:"Applications in HRI",id:"applications-in-hri",level:2},{value:"Command and Control",id:"command-and-control",level:3},{value:"Conversational Interaction",id:"conversational-interaction",level:3},{value:"Collaborative Tasks",id:"collaborative-tasks",level:3},{value:"Evaluation Metrics",id:"evaluation-metrics",level:2},{value:"Recognition Accuracy",id:"recognition-accuracy",level:3},{value:"Multimodal Integration",id:"multimodal-integration-1",level:3},{value:"Future Directions",id:"future-directions",level:2},{value:"End-to-End Learning",id:"end-to-end-learning",level:3},{value:"Personalization",id:"personalization",level:3},{value:"Lifelong Learning",id:"lifelong-learning",level:3},{value:"References",id:"references",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"speech-recognition-in-multimodal-interaction-systems",children:"Speech Recognition in Multimodal Interaction Systems"})}),"\n",(0,s.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Understand the role of speech recognition in multimodal human-robot interaction"}),"\n",(0,s.jsx)(n.li,{children:"Explain how speech processing integrates with other modalities"}),"\n",(0,s.jsx)(n.li,{children:"Analyze the challenges of speech recognition in real-world environments"}),"\n",(0,s.jsx)(n.li,{children:"Evaluate techniques for improving speech recognition accuracy in HRI contexts"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(n.p,{children:"Speech recognition in multimodal interaction systems goes far beyond simple voice command processing. In human-robot interaction contexts, speech recognition must handle the natural variability of human speech, integrate with other modalities for disambiguation, and operate effectively in real-world environments with noise and other challenges. The goal is to enable natural, conversational interaction where robots can understand and respond appropriately to human speech in context."}),"\n",(0,s.jsx)(n.p,{children:"Unlike traditional speech recognition systems that operate in isolation, multimodal systems use speech as one component of a rich communication channel that includes gesture, gaze, and visual context. This integration allows for more robust understanding and natural interaction patterns."}),"\n",(0,s.jsx)(n.h2,{id:"speech-recognition-architecture-in-hri",children:"Speech Recognition Architecture in HRI"}),"\n",(0,s.jsx)(n.h3,{id:"acoustic-processing",children:"Acoustic Processing"}),"\n",(0,s.jsx)(n.p,{children:"The first stage of speech recognition involves converting acoustic signals to digital representations:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Signal Preprocessing"}),": Noise reduction, echo cancellation, and audio enhancement"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Feature Extraction"}),": Conversion to spectral features that capture phonetic information"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Acoustic Modeling"}),": Mapping acoustic features to phonetic units using neural networks"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"language-processing",children:"Language Processing"}),"\n",(0,s.jsx)(n.p,{children:"Once acoustic features are extracted, language models interpret the meaning:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Lexical Processing"}),": Recognition of words from phonetic sequences"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Syntactic Analysis"}),": Parsing of grammatical structure"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Semantic Interpretation"}),": Extraction of meaning from linguistic structures"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"multimodal-integration",children:"Multimodal Integration"}),"\n",(0,s.jsx)(n.p,{children:"In HRI contexts, speech processing is enhanced by integration with other modalities:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Visual Context"}),": Object and scene information to resolve linguistic ambiguity"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Gestural Cues"}),": Hand movements and pointing to disambiguate references"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Social Context"}),": Previous interaction history and social relationships"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"challenges-in-hri-speech-recognition",children:"Challenges in HRI Speech Recognition"}),"\n",(0,s.jsx)(n.h3,{id:"environmental-challenges",children:"Environmental Challenges"}),"\n",(0,s.jsx)(n.p,{children:"Real-world environments present significant challenges for speech recognition:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Background Noise"}),": Ambient sounds that interfere with speech signals"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Reverberation"}),": Echo effects in rooms that distort speech"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Multiple Speakers"}),": Overlapping speech from multiple people"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Distance Effects"}),": Degradation of signal quality with distance"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"linguistic-challenges",children:"Linguistic Challenges"}),"\n",(0,s.jsx)(n.p,{children:"Human speech contains many complexities that challenge recognition systems:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Disfluencies"}),": Filler words, false starts, and self-corrections"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Ambiguity"}),": Words with multiple meanings that require context for disambiguation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Variability"}),": Different accents, speaking rates, and vocal characteristics"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Spontaneous Speech"}),": Less structured than prepared speech"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"multimodal-integration-challenges",children:"Multimodal Integration Challenges"}),"\n",(0,s.jsx)(n.p,{children:"Combining speech with other modalities introduces additional complexities:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Temporal Synchronization"}),": Aligning speech with gestures and visual events"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Cross-Modal Ambiguity"}),": Resolving conflicts between different modalities"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Computational Load"}),": Processing multiple modalities in real-time"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Fusion Strategies"}),": Effectively combining information from different sources"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"integration-with-other-modalities",children:"Integration with Other Modalities"}),"\n",(0,s.jsx)(n.h3,{id:"speech-gesture-integration",children:"Speech-Gesture Integration"}),"\n",(0,s.jsx)(n.p,{children:"Humans naturally combine speech with gestures, and effective HRI systems must process these together:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Co-verbal Gestures"}),": Hand movements that accompany speech and provide additional meaning"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Deictic Gestures"}),": Pointing gestures that reference objects or locations"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Iconic Gestures"}),": Movements that represent actions or objects"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:'The system must understand that "that one" when accompanied by pointing refers to the pointed object, not just the linguistic content.'}),"\n",(0,s.jsx)(n.h3,{id:"visual-context-integration",children:"Visual Context Integration"}),"\n",(0,s.jsx)(n.p,{children:"Visual information provides crucial context for speech understanding:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Object Grounding"}),": Connecting linguistic references to visible objects"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Scene Context"}),": Understanding commands based on environmental context"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Attention Modeling"}),": Recognizing what the human is attending to"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"social-context-integration",children:"Social Context Integration"}),"\n",(0,s.jsx)(n.p,{children:"Speech interpretation benefits from social context understanding:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Previous Interaction"}),": Using conversation history for reference resolution"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"User Modeling"}),": Adapting to individual communication styles and preferences"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Social Cues"}),": Recognizing politeness, urgency, and other social factors"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"techniques-for-robust-speech-recognition",children:"Techniques for Robust Speech Recognition"}),"\n",(0,s.jsx)(n.h3,{id:"noise-robust-processing",children:"Noise-Robust Processing"}),"\n",(0,s.jsx)(n.p,{children:"Several techniques improve speech recognition in noisy environments:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Beamforming"}),": Using microphone arrays to focus on the speaker's voice"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Denoising"}),": Neural networks trained to remove background noise"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Robust Features"}),": Acoustic features designed to be insensitive to noise"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"context-aware-recognition",children:"Context-Aware Recognition"}),"\n",(0,s.jsx)(n.p,{children:"Context information can improve recognition accuracy:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Language Models"}),": Using environmental context to bias language models"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Acoustic Models"}),": Adapting to specific acoustic conditions"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Pronunciation Models"}),": Adapting to individual speakers"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"multimodal-fusion-techniques",children:"Multimodal Fusion Techniques"}),"\n",(0,s.jsx)(n.p,{children:"Different approaches to combining modalities:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Early Fusion"}),": Combining features from different modalities early in processing"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Late Fusion"}),": Combining decisions from different modalities"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Intermediate Fusion"}),": Combining at intermediate processing stages"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"mathematical-framework",children:"Mathematical Framework"}),"\n",(0,s.jsx)(n.h3,{id:"acoustic-model",children:"Acoustic Model"}),"\n",(0,s.jsx)(n.p,{children:"The acoustic model estimates the probability of observing acoustic features given phonetic units:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"P(O|q) = \u2211_s P(O|s) \xd7 P(s|q)\n"})}),"\n",(0,s.jsx)(n.p,{children:"Where:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"O is the observed acoustic feature sequence"}),"\n",(0,s.jsx)(n.li,{children:"q is the phonetic state sequence"}),"\n",(0,s.jsx)(n.li,{children:"s represents hidden states in the acoustic model"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"language-model",children:"Language Model"}),"\n",(0,s.jsx)(n.p,{children:"The language model estimates the probability of word sequences:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"P(w\u2081, w\u2082, ..., w\u2099) = \u220f\u1d62 P(w\u1d62 | w\u2081, ..., w\u1d62\u208b\u2081)\n"})}),"\n",(0,s.jsx)(n.p,{children:"For multimodal integration, this becomes:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"P(w\u2081, w\u2082, ..., w\u2099 | context) = \u220f\u1d62 P(w\u1d62 | w\u2081, ..., w\u1d62\u208b\u2081, visual_context, gestural_context)\n"})}),"\n",(0,s.jsx)(n.h3,{id:"multimodal-fusion",children:"Multimodal Fusion"}),"\n",(0,s.jsx)(n.p,{children:"The combined multimodal probability:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"P(interpretation | speech, vision, gesture) \u221d\nP(speech | interpretation) \xd7 P(vision | interpretation) \xd7 P(gesture | interpretation) \xd7 P(interpretation)\n"})}),"\n",(0,s.jsx)(n.h2,{id:"applications-in-hri",children:"Applications in HRI"}),"\n",(0,s.jsx)(n.h3,{id:"command-and-control",children:"Command and Control"}),"\n",(0,s.jsx)(n.p,{children:"Speech recognition enables natural command interfaces:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Navigation Commands"}),': "Go to the kitchen"']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Manipulation Commands"}),': "Pick up the red cup"']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Complex Tasks"}),': "Bring me the book from the table"']}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"conversational-interaction",children:"Conversational Interaction"}),"\n",(0,s.jsx)(n.p,{children:"More natural conversation with contextual understanding:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Reference Resolution"}),': Understanding "it" and "that one" in context']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Clarification Requests"}),": Asking for clarification when uncertain"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Social Interaction"}),": Politeness, small talk, and social responses"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"collaborative-tasks",children:"Collaborative Tasks"}),"\n",(0,s.jsx)(n.p,{children:"Supporting human-robot collaboration:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Task Coordination"}),': "I\'ll take the left side, you take the right"']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Status Updates"}),': "I\'ve finished the first step"']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Help Requests"}),': "I need assistance with this"']}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"evaluation-metrics",children:"Evaluation Metrics"}),"\n",(0,s.jsx)(n.h3,{id:"recognition-accuracy",children:"Recognition Accuracy"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Word Error Rate (WER)"}),": Percentage of incorrectly recognized words"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Sentence Error Rate (SER)"}),": Percentage of sentences with at least one error"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Intent Recognition Rate"}),": Percentage of correctly interpreted user intents"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"multimodal-integration-1",children:"Multimodal Integration"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Cross-Modal Accuracy"}),": How well modalities support each other"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Disambiguation Success"}),": Success rate in resolving ambiguous references"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Context Utilization"}),": Effectiveness of contextual information use"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"future-directions",children:"Future Directions"}),"\n",(0,s.jsx)(n.h3,{id:"end-to-end-learning",children:"End-to-End Learning"}),"\n",(0,s.jsx)(n.p,{children:"Modern approaches use end-to-end neural networks that learn to map acoustic signals directly to meanings, potentially including multimodal context."}),"\n",(0,s.jsx)(n.h3,{id:"personalization",children:"Personalization"}),"\n",(0,s.jsx)(n.p,{children:"Systems that adapt to individual users' speech patterns, vocabulary, and communication styles."}),"\n",(0,s.jsx)(n.h3,{id:"lifelong-learning",children:"Lifelong Learning"}),"\n",(0,s.jsx)(n.p,{children:"Systems that continuously improve through interaction with users in real-world settings."}),"\n",(0,s.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Hain, T., & Bourlard, H. (2005). Speech and audio processing in adverse conditions. EURASIP Journal on Applied Signal Processing, 2005, 584-585."}),"\n",(0,s.jsx)(n.li,{children:"Cassell, J., & Vilhj\xe1lmsson, H. H. (2003). Fully automatic auditory gesture generation. International Conference on Intelligent Virtual Agents, 32-45."}),"\n",(0,s.jsx)(n.li,{children:"Thomason, J., Bisk, Y., & Khosla, A. (2019). Shifting towards task completion with touch in multimodal conversational interfaces. arXiv preprint arXiv:1909.05288."}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>l});var t=i(6540);const s={},o=t.createContext(s);function r(e){const n=t.useContext(o);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),t.createElement(o.Provider,{value:n},e.children)}}}]);